<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Paper Sorted</title>
    <url>/posts/46623364.html</url>
    <content><![CDATA[<h1 id="Awesome-paper-list"><a href="#Awesome-paper-list" class="headerlink" title="Awesome paper list"></a>Awesome paper list</h1><p>A curated list of repositories in which many NLP/CV/ML papers and related area resources are collected.</p>
<h2 id="Tabel-of-Contents"><a href="#Tabel-of-Contents" class="headerlink" title="Tabel of Contents"></a>Tabel of Contents</h2><ul>
<li><a href="#awesome-paper-list">Awesome paper list</a><ul>
<li><a href="#tabel-of-contents">Tabel of Contents</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#natural-language-processing">Natural Language Processing</a></li>
<li><a href="#computer-vision">Computer Vision</a></li>
<li><a href="#graphs">Graphs</a></li>
<li><a href="#knowledge-graph">Knowledge Graph</a></li>
<li><a href="#multimodality">MultiModality</a></li>
<li><a href="#others">Others</a></li>
<li><a href="#commensense">Commensense</a></li>
<li><a href="#time-series">Time Series</a></li>
<li><a href="#speech">Speech</a></li>
<li><a href="#causality">Causality</a></li>
<li><a href="#anomaly-detection">Anomaly Detection</a></li>
<li><a href="#thanks">Thanks</a></li>
</ul>
</li>
</ul>
<h2 id="Contributing"><a href="#Contributing" class="headerlink" title="Contributing"></a>Contributing</h2><p>The repository is under construction. Please feel free to send me <a href="https://github.com/Doragd/Awesome-Paper-List/pulls" target="_blank" rel="noopener">pull requests</a> or email (guodun.li#gmail.com) to add links. Markdown format:<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">- </span>[<span class="string">Repository Name</span>](<span class="link">link</span>) <span class="xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span></span><br><span class="line">![](https://img.shields.io/badge/author-:user-be8abf)</span><br><span class="line">![](https://img.shields.io/github/stars/:user/:repo)</span><br></pre></td></tr></table></figure></p>
<h2 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h2><ul>
<li><a href="https://github.com/maidis/awesome-machine-translation" target="_blank" rel="noopener">Machine Translation</a> <br> <img src="https://img.shields.io/badge/author-maidis-be8abf" alt> <img src="https://img.shields.io/github/stars/maidis/awesome-machine-translation" alt></li>
<li><a href="https://github.com/fendouai/Awesome-Text-Classification" target="_blank" rel="noopener">Text Classification</a> <br> <img src="https://img.shields.io/badge/author-fendouai-be8abf" alt> <img src="https://img.shields.io/github/stars/fendouai/Awesome-Text-Classification" alt></li>
<li><a href="https://github.com/xiamx/awesome-sentiment-analysis" target="_blank" rel="noopener">Sentiment Analysis</a> <br> <img src="https://img.shields.io/badge/author-xiamx-be8abf" alt> <img src="https://img.shields.io/github/stars/xiamx/awesome-sentiment-analysis" alt></li>
<li><a href="https://github.com/laugustyniak/awesome-sentiment-analysis" target="_blank" rel="noopener">Sentiment Analysis</a> <br> <img src="https://img.shields.io/badge/author-laugustyniak-be8abf" alt> <img src="https://img.shields.io/github/stars/laugustyniak/awesome-sentiment-analysis" alt></li>
<li><a href="https://github.com/haiker2011/awesome-nlp-sentiment-analysis" target="_blank" rel="noopener">Sentiment Analysis</a> <br> <img src="https://img.shields.io/badge/author-haiker2011-be8abf" alt> <img src="https://img.shields.io/github/stars/haiker2011/awesome-nlp-sentiment-analysis" alt></li>
<li><a href="https://github.com/ZhengZixiang/ABSAPapers" target="_blank" rel="noopener">Aspect-Based Sentiment Analysis</a> <br> <img src="https://img.shields.io/badge/author-ZhengZixiang-be8abf" alt> <img src="https://img.shields.io/github/stars/ZhengZixiang/ABSAPapers" alt></li>
<li><a href="https://github.com/jiangqn/Aspect-Based-Sentiment-Analysis" target="_blank" rel="noopener">Aspect-Based Sentiment Analysis</a> <br> <img src="https://img.shields.io/badge/author-jiangqn-be8abf" alt> <img src="https://img.shields.io/github/stars/jiangqn/Aspect-Based-Sentiment-Analysis" alt></li>
<li><a href="https://github.com/thunlp/RCPapers" target="_blank" rel="noopener">Machine Reading Comprehension</a> <br> <img src="https://img.shields.io/badge/author-thunlp-be8abf" alt> <img src="https://img.shields.io/github/stars/thunlp/RCPapers" alt></li>
<li><a href="https://github.com/xanhho/Reading-Comprehension-Question-Answering-Papers" target="_blank" rel="noopener">Machine Reading Comprehension</a> <br> <img src="https://img.shields.io/badge/author-xanhho-be8abf" alt> <img src="https://img.shields.io/github/stars/xanhho/Reading-Comprehension-Question-Answering-Papers" alt></li>
<li><a href="https://github.com/thunlp/NREPapers" target="_blank" rel="noopener">Relation Extraction</a> <br> <img src="https://img.shields.io/badge/author-thunlp-be8abf" alt> <img src="https://img.shields.io/github/stars/thunlp/NREPapers" alt></li>
<li><a href="https://github.com/roomylee/awesome-relation-extraction" target="_blank" rel="noopener">Relation Extraction</a> <br> <img src="https://img.shields.io/badge/author-roomylee-be8abf" alt> <img src="https://img.shields.io/github/stars/roomylee/awesome-relation-extraction" alt></li>
<li><a href="https://github.com/cedrickchee/awesome-bert-nlp" target="_blank" rel="noopener">BERT-based Research</a> <br> <img src="https://img.shields.io/badge/author-cedrickchee-be8abf" alt> <img src="https://img.shields.io/github/stars/cedrickchee/awesome-bert-nlp" alt></li>
<li><a href="https://github.com/thunlp/PLMpapers" target="_blank" rel="noopener">Pre-trained Language Model</a> <br> <img src="https://img.shields.io/badge/author-thunlp-be8abf" alt> <img src="https://img.shields.io/github/stars/thunlp/PLMpapers" alt></li>
<li><a href="https://github.com/thunlp/SCPapers" target="_blank" rel="noopener">Sememe Computation</a> <br> <img src="https://img.shields.io/badge/author-thunlp-be8abf" alt> <img src="https://img.shields.io/github/stars/thunlp/SCPapers" alt></li>
<li><a href="https://github.com/THUNLP-MT/TG-Reading-List" target="_blank" rel="noopener">Text Generation</a><br> <img src="https://img.shields.io/badge/author-THUNLP_MT-be8abf" alt> <img src="https://img.shields.io/github/stars/THUNLP-MT/TG-Reading-List" alt></li>
<li><a href="https://github.com/ChenChengKuan/awesome-text-generation" target="_blank" rel="noopener">Text Generation</a> <br> <img src="https://img.shields.io/badge/author-ChenChengKuan-be8abf" alt> <img src="https://img.shields.io/github/stars/ChenChengKuan/awesome-text-generation" alt></li>
<li><a href="https://github.com/tokenmill/awesome-nlg" target="_blank" rel="noopener">Text Generation</a><br> <img src="https://img.shields.io/badge/author-tokenmill-be8abf" alt> <img src="https://img.shields.io/github/stars/tokenmill/awesome-nlg" alt></li>
<li><a href="https://github.com/franxyao/Deep-Generative-Models-for-Natural-Language-Processing" target="_blank" rel="noopener">Text Generation</a><br> <img src="https://img.shields.io/badge/author-franxyao-be8abf" alt> <img src="https://img.shields.io/github/stars/franxyao/Deep-Generative-Models-for-Natural-Language-Processing" alt></li>
<li><a href="https://github.com/FranxYao/Language-Model-Pretraining-for-Text-Generation" target="_blank" rel="noopener">Pre-trained LM for Text Generation</a><br> <img src="https://img.shields.io/badge/author-franxyao-be8abf" alt> <img src="https://img.shields.io/github/stars/FranxYao/Language-Model-Pretraining-for-Text-Generation" alt></li>
<li><a href="https://github.com/thunlp/TAADpapers" target="_blank" rel="noopener">Textual Adversarial Attack and Defense</a> <br> <img src="https://img.shields.io/badge/author-thunlp-be8abf" alt> <img src="https://img.shields.io/github/stars/thunlp/TAADpapers" alt></li>
<li><a href="https://github.com/jaromirsalamon/Awesome-Dialogue-System-Papers" target="_blank" rel="noopener">Dialogue System</a> <br> <img src="https://img.shields.io/badge/author-jaromirsalamon-be8abf" alt> <img src="https://img.shields.io/github/stars/jaromirsalamon/Awesome-Dialogue-System-Papers" alt></li>
<li><a href="https://github.com/ZhengZixiang/DSPapers" target="_blank" rel="noopener">Dialogue System</a> <br> <img src="https://img.shields.io/badge/author-ZhengZixiang-be8abf" alt> <img src="https://img.shields.io/github/stars/ZhengZixiang/DSPapers" alt></li>
<li><a href="https://github.com/tsenghungchen/dialog-generation-paper" target="_blank" rel="noopener">Dialogue Generation</a> <br> <img src="https://img.shields.io/badge/author-tsenghungchen-be8abf" alt> <img src="https://img.shields.io/github/stars/tsenghungchen/dialog-generation-paper" alt></li>
<li><a href="https://github.com/yajingsunno/dialogue-system-reading-paper-list" target="_blank" rel="noopener">Dialogue System</a> <br> <img src="https://img.shields.io/badge/author-yajingsunno-be8abf" alt> <img src="https://img.shields.io/github/stars/yajingsunno/dialogue-system-reading-paper-list" alt></li>
<li><a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue" target="_blank" rel="noopener">Dialogue State Tracking</a> <br> <img src="https://img.shields.io/badge/author-google-be8abf" alt> <img src="https://img.shields.io/github/stars/google-research-datasets/dstc8-schema-guided-dialogue" alt></li>
<li><a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey" target="_blank" rel="noopener">Task-Oriented Dialogue</a> <br> <img src="https://img.shields.io/badge/author-AtmaHou-be8abf" alt> <img src="https://img.shields.io/github/stars/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey" alt></li>
<li><a href="https://github.com/jianguoz/Conversational-AI" target="_blank" rel="noopener">Conversational AI</a> <br> <img src="https://img.shields.io/badge/author-jianguoz-be8abf" alt> <img src="https://img.shields.io/github/stars/jianguoz/Conversational-AI" alt></li>
<li><a href="https://github.com/Separius/awesome-sentence-embedding" target="_blank" rel="noopener">Sentence Embeddings</a> <br> <img src="https://img.shields.io/badge/author-Separius-be8abf" alt> <img src="https://img.shields.io/github/stars/Separius/awesome-sentence-embedding" alt></li>
<li><a href="https://github.com/dapurv5/awesome-question-answering" target="_blank" rel="noopener">Question Answering</a> <br> <img src="https://img.shields.io/badge/author-dapurv5-be8abf" alt> <img src="https://img.shields.io/github/stars/dapurv5/awesome-question-answering" alt></li>
<li><a href="https://github.com/BshoterJ/awesome-knowledge-graph-question-answering" target="_blank" rel="noopener">Knowledge Base Question Answering</a> <br> <img src="https://img.shields.io/badge/author-Bshoter-be8abf" alt> <img src="https://img.shields.io/github/stars/BshoterJ/awesome-knowledge-graph-question-answering" alt></li>
<li><a href="https://github.com/fuzhenxin/Style-Transfer-in-Text" target="_blank" rel="noopener">Text Style Transfer</a> <br> <img src="https://img.shields.io/badge/author-fuzhenxin-be8abf" alt> <img src="https://img.shields.io/github/stars/fuzhenxin/Style-Transfer-in-Text" alt></li>
<li><a href="https://github.com/yd1996/awesome-text-style-transfer" target="_blank" rel="noopener">Text Style Transfer</a> <br> <img src="https://img.shields.io/badge/author-yd1996-be8abf" alt> <img src="https://img.shields.io/github/stars/yd1996/awesome-text-style-transfer" alt></li>
<li><a href="https://github.com/NTMC-Community/awesome-neural-models-for-semantic-match" target="_blank" rel="noopener">Text Matching</a> <br> <img src="https://img.shields.io/badge/author-NTMC_Community-be8abf" alt> <img src="https://img.shields.io/github/stars/NTMC-Community/awesome-neural-models-for-semantic-match" alt></li>
<li><a href="https://github.com/luopeixiang/awesome-text-summarization" target="_blank" rel="noopener">Text Summarization</a> <br> <img src="https://img.shields.io/badge/author-NTMC_Community-be8abf" alt> <img src="https://img.shields.io/github/stars/NTMC-Community/awesome-neural-models-for-semantic-match" alt></li>
<li><a href="https://github.com/fuzhenxin/Personal-Emotional-Stylized-Dialog" target="_blank" rel="noopener">Personal Emotional Stylized Dialog</a> <br> <img src="https://img.shields.io/badge/author-fuzhenxin-be8abf" alt> <img src="https://img.shields.io/github/stars/fuzhenxin/Personal-Emotional-Stylized-Dialog" alt></li>
<li><a href="https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers" target="_blank" rel="noopener">Named Entity Recognition</a> <br> <img src="https://img.shields.io/badge/author-pfliu-be8abf" alt> <img src="https://img.shields.io/github/stars/pfliu-nlp/Named-Entity-Recognition-NER-Papers" alt></li>
<li><a href="https://github.com/lingluodlut/BioNER-Progress" target="_blank" rel="noopener">Biomedical NER</a> <br> <img src="https://img.shields.io/badge/author-lingluodlut-be8abf" alt> <img src="https://img.shields.io/github/stars/lingluodlut/BioNER-Progress" alt></li>
<li><a href="https://github.com/ZhengZixiang/NERPapers" target="_blank" rel="noopener">Named Entity Recognition</a> <br> <img src="https://img.shields.io/badge/author-ZhengZixiang-be8abf" alt> <img src="https://img.shields.io/github/stars/ZhengZixiang/NERPapers" alt></li>
<li><a href="https://github.com/ryanzhumich/awesome-clir" target="_blank" rel="noopener">Cross-lingual Information Retrieval</a> <br> <img src="https://img.shields.io/badge/author-ryanzhumich-be8abf" alt> <img src="https://img.shields.io/github/stars/ryanzhumich/awesome-clir" alt></li>
<li><a href="https://github.com/harpribot/awesome-information-retrieval" target="_blank" rel="noopener">Information Retrieval</a> <br> <img src="https://img.shields.io/badge/author-harpribot-be8abf" alt> <img src="https://img.shields.io/github/stars/harpribot/awesome-information-retrieval" alt></li>
<li><a href="https://github.com/umbrellabeach/awesome-Biomedical-EntityLinking-papers" target="_blank" rel="noopener">Biomedical Entity Linking</a> <br><br><img src="https://img.shields.io/badge/author-umbrellabeach-be8abf" alt><br><img src="https://img.shields.io/github/stars/umbrellabeach/awesome-Biomedical-EntityLinking-papers" alt></li>
<li><a href="https://github.com/NPCai/Open-IE-Papers" target="_blank" rel="noopener">Open Information Extraction</a> <br><br><img src="https://img.shields.io/badge/author-NPCai-be8abf" alt><br><img src="https://img.shields.io/github/stars/NPCai/Open-IE-Papers" alt></li>
<li><a href="https://github.com/caufieldjh/awesome-bioie" target="_blank" rel="noopener">Biomedical Information Extraction</a> <br><br><img src="https://img.shields.io/badge/author-caufieldjh-be8abf" alt><br><img src="https://img.shields.io/github/stars/caufieldjh/awesome-bioie" alt></li>
</ul>
<h2 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h2><ul>
<li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Semantic Segmentation</a> <br><br><img src="https://img.shields.io/badge/author-mrgloom-be8abf" alt><br><img src="https://img.shields.io/github/stars/mrgloom/awesome-semantic-segmentation" alt></li>
<li><a href="https://github.com/jinwchoi/awesome-action-recognition" target="_blank" rel="noopener">Action Recognition</a> <br><br><img src="https://img.shields.io/badge/author-jinwchoi-be8abf" alt><br><img src="https://img.shields.io/github/stars/jinwchoi/awesome-action-recognition" alt></li>
<li><a href="https://github.com/weiaicunzai/awesome-image-classification" target="_blank" rel="noopener">Image Classification</a> <br><br><img src="https://img.shields.io/badge/author-weiaicunzai-be8abf" alt><br><img src="https://img.shields.io/github/stars/weiaicunzai/awesome-image-classification" alt></li>
<li><a href="https://github.com/willard-yuan/awesome-cbir-papers" target="_blank" rel="noopener">Image Retrieval</a> <br><br><img src="https://img.shields.io/badge/author-willard_yuan-be8abf" alt><br><img src="https://img.shields.io/github/stars/willard-yuan/awesome-cbir-papers" alt></li>
<li><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">Object Detection</a> <br><br><img src="https://img.shields.io/badge/author-amusi-be8abf" alt><br><img src="https://img.shields.io/github/stars/amusi/awesome-object-detection" alt></li>
<li><a href="https://github.com/hoya012/deep_learning_object_detection" target="_blank" rel="noopener">Object Detection</a> <br><br><img src="https://img.shields.io/badge/author-hoya012-be8abf" alt><br><img src="https://img.shields.io/github/stars/hoya012/deep_learning_object_detection" alt></li>
<li><a href="https://github.com/xiaweihao/awesome-image-translation" target="_blank" rel="noopener">Image-to-image Translation</a> <br><br><img src="https://img.shields.io/badge/author-xiaweihao-be8abf" alt><br><img src="https://img.shields.io/github/stars/xiaweihao/awesome-image-translation" alt></li>
<li><a href="https://github.com/zhaoxin94/awesome-domain-adaptation" target="_blank" rel="noopener">Domain Adaptation</a> <br><br><img src="https://img.shields.io/badge/author-zhaoxin94-be8abf" alt><br><img src="https://img.shields.io/github/stars/zhaoxin94/awesome-domain-adaptation" alt></li>
<li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">vision-based SLAM / Visual Odometry</a> <br><br><img src="https://img.shields.io/badge/author-tzutalin-be8abf" alt><br><img src="https://img.shields.io/github/stars/tzutalin/awesome-visual-slam" alt></li>
<li><a href="https://github.com/ChanChiChoi/awesome-Face_Recognition" target="_blank" rel="noopener">Face-related</a> <br><br><img src="https://img.shields.io/badge/author-ChanChiChoi-be8abf" alt><br><img src="https://img.shields.io/github/stars/ChanChiChoi/awesome-Face_Recognition" alt></li>
<li><a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition" target="_blank" rel="noopener">Scene Text Recognition</a> <br><br><img src="https://img.shields.io/badge/author-chongyangtao-be8abf" alt><br><img src="https://img.shields.io/github/stars/chongyangtao/Awesome-Scene-Text-Recognition" alt></li>
<li><a href="https://github.com/hwalsuklee/awesome-deep-text-detection-recognition" target="_blank" rel="noopener">Text Detection &amp; Recognition</a> <br><br><img src="https://img.shields.io/badge/author-hwalsuklee-be8abf" alt><br><img src="https://img.shields.io/github/stars/hwalsuklee/awesome-deep-text-detection-recognition" alt></li>
</ul>
<h2 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h2><ul>
<li><a href="https://github.com/nnzhan/Awesome-Graph-Neural-Networks" target="_blank" rel="noopener">Graph Neural Networks</a> <br><br><img src="https://img.shields.io/badge/author-nnzhan-be8abf" alt><br><img src="https://img.shields.io/github/stars/nnzhan/Awesome-Graph-Neural-Networks" alt></li>
<li><a href="https://github.com/thunlp/GNNPapers" target="_blank" rel="noopener">Graph Neural Networks</a> <br><br><img src="https://img.shields.io/badge/author-thunlp-be8abf" alt><br><img src="https://img.shields.io/github/stars/thunlp/GNNPapers" alt></li>
<li><a href="https://github.com/Jiakui/awesome-gcn" target="_blank" rel="noopener">Graph Convolutional Networks</a> <br><br><img src="https://img.shields.io/badge/author-Jiakui-be8abf" alt><br><img src="https://img.shields.io/github/stars/Jiakui/awesome-gcn" alt></li>
<li><a href="https://github.com/benedekrozemberczki/awesome-graph-classification" target="_blank" rel="noopener">Graph Classification</a> <br><br><img src="https://img.shields.io/badge/author-benedekrozemberczki-be8abf" alt><br><img src="https://img.shields.io/github/stars/benedekrozemberczki/awesome-graph-classification" alt></li>
<li><a href="https://github.com/ky-zhang/awesome-graph-representation-learning" target="_blank" rel="noopener">Graph Representation Learning</a> <br><br><img src="https://img.shields.io/badge/author-ky_zhang-be8abf" alt><br><img src="https://img.shields.io/github/stars/ky-zhang/awesome-graph-representation-learning" alt></li>
<li><a href="https://github.com/thunlp/NRLPapers" target="_blank" rel="noopener">Graph Representation Learning</a> <br><br><img src="https://img.shields.io/badge/author-thunlp-be8abf" alt><br><img src="https://img.shields.io/github/stars/thunlp/NRLPapers" alt></li>
<li><a href="https://github.com/chihming/awesome-network-embedding" target="_blank" rel="noopener">Network Embeddings</a> <br><br><img src="https://img.shields.io/badge/author-chihming-be8abf" alt><br><img src="https://img.shields.io/github/stars/chihming/awesome-network-embedding" alt></li>
<li><a href="https://github.com/thunlp/KB2E" target="_blank" rel="noopener">Knowledge Graph Embeddings</a> <br><br><img src="https://img.shields.io/badge/author-thun-be8abf" alt><br><img src="https://img.shields.io/github/stars/thunlp/KB2E" alt></li>
<li><a href="https://github.com/benedekrozemberczki/awesome-community-detection" target="_blank" rel="noopener">Community Detection</a> <br><br><img src="https://img.shields.io/badge/author-benedekrozemberczki-be8abf" alt><br><img src="https://img.shields.io/github/stars/benedekrozemberczki/awesome-community-detection" alt></li>
<li><a href="https://github.com/FatemehTarashi/awesome-TDA" target="_blank" rel="noopener">Topological Data Analysis</a> <br><br><img src="https://img.shields.io/badge/author-FatemehTarashi-be8abf" alt><br><img src="https://img.shields.io/github/stars/FatemehTarashi/awesome-TDA" alt></li>
<li><a href="https://github.com/safe-graph/graph-adversarial-learning-literature" target="_blank" rel="noopener">Graph Adversarial Learning</a> <br><br><img src="https://img.shields.io/badge/author-SafeGraph-be8abf" alt><br><img src="https://img.shields.io/github/stars/safe-graph/graph-adversarial-learning-literature" alt></li>
<li><a href="https://github.com/jbmusso/awesome-graph" target="_blank" rel="noopener">Graph Computing</a> <br><br><img src="https://img.shields.io/badge/author-jbmusso-be8abf" alt><br><img src="https://img.shields.io/github/stars/jbmusso/awesome-graph" alt></li>
</ul>
<h2 id="Knowledge-Graph"><a href="#Knowledge-Graph" class="headerlink" title="Knowledge Graph"></a>Knowledge Graph</h2><ul>
<li><a href="https://github.com/songjiang0909/awesome-knowledge-graph-construction" target="_blank" rel="noopener">Knowledge Graph Construction</a> <br><br><img src="https://img.shields.io/badge/author-songjiang0909-be8abf" alt><br><img src="https://img.shields.io/github/stars/songjiang0909/awesome-knowledge-graph-construction" alt></li>
<li><a href="https://github.com/thunlp/KRLPapers" target="_blank" rel="noopener">Knowledge Embeddings</a> <br><br><img src="https://img.shields.io/badge/author-thunlp-be8abf" alt><br><img src="https://img.shields.io/github/stars/thunlp/KRLPapers" alt></li>
<li><a href="https://github.com/husthuke/awesome-knowledge-graph" target="_blank" rel="noopener">Knowledge Graph</a> <br><br><img src="https://img.shields.io/badge/author-husthuke-be8abf" alt><br><img src="https://img.shields.io/github/stars/husthuke/awesome-knowledge-graph" alt></li>
<li><a href="https://github.com/shaoxiongji/awesome-knowledge-graph" target="_blank" rel="noopener">Knowledge Graph</a> <br><br><img src="https://img.shields.io/badge/author-shaoxiongji-be8abf" alt><br><img src="https://img.shields.io/github/stars/shaoxiongji/awesome-knowledge-graph" alt></li>
<li><a href="https://github.com/BrambleXu/knowledge-graph-learning" target="_blank" rel="noopener">Knowledge Graph</a> <br><br><img src="https://img.shields.io/badge/author-BrambleXu-be8abf" alt><br><img src="https://img.shields.io/github/stars/BrambleXu/knowledge-graph-learning" alt></li>
<li><a href="https://github.com/totogo/awesome-knowledge-graph" target="_blank" rel="noopener">Knowledge Graph</a> <br><br><img src="https://img.shields.io/badge/author-totogo-be8abf" alt><br><img src="https://img.shields.io/github/stars/totogo/awesome-knowledge-graph" alt></li>
</ul>
<h2 id="MultiModality"><a href="#MultiModality" class="headerlink" title="MultiModality"></a>MultiModality</h2><ul>
<li><a href="https://github.com/zhjohnchan/awesome-image-captioning" target="_blank" rel="noopener">Image Captioning</a>  <br><br><img src="https://img.shields.io/badge/author-zhjohnchan-be8abf" alt><br><img src="https://img.shields.io/github/stars/zhjohnchan/awesome-image-captioning" alt></li>
<li><a href="https://github.com/forence/Awesome-Visual-Captioning" target="_blank" rel="noopener">Image Captioning</a>  <br><br><img src="https://img.shields.io/badge/author-forence-be8abf" alt><br><img src="https://img.shields.io/github/stars/forence/Awesome-Visual-Captioning" alt></li>
<li><a href="https://github.com/chingyaoc/awesome-vqa" target="_blank" rel="noopener">Visual Question Answering</a>  <br><br><img src="https://img.shields.io/badge/author-chingyaoc-be8abf" alt><br><img src="https://img.shields.io/github/stars/chingyaoc/awesome-vqa" alt></li>
<li><a href="https://github.com/jokieleung/awesome-visual-question-answering" target="_blank" rel="noopener">Visual Question Answering</a>  <br><br><img src="https://img.shields.io/badge/author-jokieleung-be8abf" alt><br><img src="https://img.shields.io/github/stars/jokieleung/awesome-visual-question-answering" alt></li>
<li><a href="https://github.com/DerekDLP/VQA-papers" target="_blank" rel="noopener">Visual Question Answering</a>  <br><br><img src="https://img.shields.io/badge/author-DerekDLP-be8abf" alt><br><img src="https://img.shields.io/github/stars/DerekDLP/VQA-papers" alt></li>
<li><a href="https://github.com/ZihengZZH/awesome-multimodal-machine-translation" target="_blank" rel="noopener">Multimodal Machine Translation</a>  <br><br><img src="https://img.shields.io/badge/author-ZihengZZH-be8abf" alt><br><img src="https://img.shields.io/github/stars/ZihengZZH/awesome-multimodal-machine-translation" alt></li>
<li><a href="https://github.com/TheShadow29/awesome-grounding" target="_blank" rel="noopener">Visual Grounding</a>  <br><br><img src="https://img.shields.io/badge/author-TheShadow29-be8abf" alt><br><img src="https://img.shields.io/github/stars/TheShadow29/awesome-grounding" alt></li>
</ul>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul>
<li><a href="https://github.com/matthewvowels1/Awesome-VAEs" target="_blank" rel="noopener">Awesome-VAEs</a> <br><br><img src="https://img.shields.io/badge/author-matthewvowels1-be8abf" alt><br><img src="https://img.shields.io/github/stars/matthewvowels1/Awesome-VAEs" alt></li>
<li><a href="https://github.com/LongLong-Jing/awesome-unsupervised-learning" target="_blank" rel="noopener">Unsupervised Learning</a> <br><br><img src="https://img.shields.io/badge/author-LongLong_Jing-be8abf" alt><br><img src="https://img.shields.io/github/stars/LongLong-Jing/awesome-unsupervised-learning" alt></li>
<li><a href="https://github.com/floodsung/Meta-Learning-Papers" target="_blank" rel="noopener">Meta Learning</a> <br><br><img src="https://img.shields.io/badge/author-floodsung-be8abf" alt><br><img src="https://img.shields.io/github/stars/floodsung/Meta-Learning-Papers" alt></li>
<li><a href="https://github.com/e-271/awesome-few-shot-learning" target="_blank" rel="noopener">Few Shot Learning</a> <br><br><img src="https://img.shields.io/badge/author-e_271-be8abf" alt><br><img src="https://img.shields.io/github/stars/e-271/awesome-few-shot-learning" alt></li>
<li><a href="https://github.com/Duan-JM/awesome-papers-fewshot" target="_blank" rel="noopener">Few Shot Learning</a> <br><br><img src="https://img.shields.io/badge/author-Duan_JM-be8abf" alt><br><img src="https://img.shields.io/github/stars/Duan-JM/awesome-papers-fewshot" alt></li>
<li><a href="https://github.com/sekwiatkowski/awesome-capsule-networks" target="_blank" rel="noopener">Capsule Networks</a> <br><br><img src="https://img.shields.io/badge/author-sekwiatkowski-be8abf" alt><br><img src="https://img.shields.io/github/stars/sekwiatkowski/awesome-capsule-networks" alt></li>
<li><a href="https://github.com/benedekrozemberczki/awesome-decision-tree-papers" target="_blank" rel="noopener">Decision Tree</a> <br><br><img src="https://img.shields.io/badge/author-benedekrozemberczki-be8abf" alt><br><img src="https://img.shields.io/github/stars/benedekrozemberczki/awesome-decision-tree-papers" alt></li>
<li><a href="https://github.com/blakeliu/awesome-cell-detection-segmentation" target="_blank" rel="noopener">Cell Detection &amp; Segmentation</a> <br><br><img src="https://img.shields.io/badge/author-blakeliu-be8abf" alt><br><img src="https://img.shields.io/github/stars/blakeliu/awesome-cell-detection-segmentation" alt></li>
<li><a href="https://github.com/benedekrozemberczki/awesome-fraud-detection-papers" target="_blank" rel="noopener">Fraud Detection</a> <br><br><img src="https://img.shields.io/badge/author-benedekrozemberczki-be8abf" alt><br><img src="https://img.shields.io/github/stars/benedekrozemberczki/awesome-fraud-detection-papers" alt></li>
<li><a href="https://github.com/safe-graph/graph-fraud-detection-papers" target="_blank" rel="noopener">Graph-based Fraud Detection</a> <br><br><img src="https://img.shields.io/badge/author-SafeGraph-be8abf" alt><br><img src="https://img.shields.io/github/stars/safe-graph/graph-fraud-detection-papers" alt></li>
<li><a href="https://github.com/thunlp/LegalPapers" target="_blank" rel="noopener">Legal Intelligence</a> <br><br><img src="https://img.shields.io/badge/author-thunlp-be8abf" alt><br><img src="https://img.shields.io/github/stars/thunlp/LegalPapers" alt></li>
<li><a href="https://github.com/CrazyVertigo/awesome-data-augmentation" target="_blank" rel="noopener">Data Augmentation</a> <br><br><img src="https://img.shields.io/badge/author-CrazyVertigo-be8abf" alt><br><img src="https://img.shields.io/github/stars/CrazyVertigo/awesome-data-augmentation" alt></li>
<li><a href="https://github.com/jiachenli94/Awesome-Decision-Making-Reinforcement-Learning" target="_blank" rel="noopener">Decision Making</a> <br><br><img src="https://img.shields.io/badge/author-jiachenli94-be8abf" alt><br><img src="https://img.shields.io/github/stars/jiachenli94/Awesome-Decision-Making-Reinforcement-Learning" alt></li>
</ul>
<h2 id="Commensense"><a href="#Commensense" class="headerlink" title="Commensense"></a>Commensense</h2><ul>
<li><a href="https://github.com/yhy1117/Commonsense_Reasoning_Papers" target="_blank" rel="noopener">Commonsense Reasoning</a> <br><br><img src="https://img.shields.io/badge/author-yhy1117-be8abf" alt><br><img src="https://img.shields.io/github/stars/yhy1117/Commonsense_Reasoning_Papers" alt></li>
<li><a href="https://github.com/wonderseen/Commonsense-Modeling" target="_blank" rel="noopener">Commonsense Modeling</a> <br><br><img src="https://img.shields.io/badge/author-wonderseen-be8abf" alt><br><img src="https://img.shields.io/github/stars/wonderseen/Commonsense-Modeling" alt></li>
</ul>
<h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul>
<li><a href="https://github.com/xephonhq/awesome-time-series-database" target="_blank" rel="noopener">Time Series</a> <br><br><img src="https://img.shields.io/badge/author-xephonhq-be8abf" alt><br><img src="https://img.shields.io/github/stars/xephonhq/awesome-time-series-database" alt></li>
<li><a href="https://github.com/bighuang624/Time-Series-Papers" target="_blank" rel="noopener">Time Series</a> <br><br><img src="https://img.shields.io/badge/author-bighuang624-be8abf" alt><br><img src="https://img.shields.io/github/stars/bighuang624/Time-Series-Papers" alt></li>
<li><a href="https://github.com/MaxBenChrist/awesome_time_series_in_python" target="_blank" rel="noopener">Time Series in Python</a> <br><br><img src="https://img.shields.io/badge/author-MaxBenChrist-be8abf" alt><br><img src="https://img.shields.io/github/stars/MaxBenChrist/awesome_time_series_in_python" alt></li>
<li><a href="https://github.com/youngdou/awesome-time-series-analysis" target="_blank" rel="noopener">Time Series Analysis</a> <br><br><img src="https://img.shields.io/badge/author-youngdou-be8abf" alt><br><img src="https://img.shields.io/github/stars/youngdou/awesome-time-series-analysis" alt></li>
</ul>
<h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><ul>
<li><p><a href="https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers" target="_blank" rel="noopener">Speech Recognition &amp; Synthesis</a> <br><br><img src="https://img.shields.io/badge/author-zzw922cn-be8abf" alt><br><img src="https://img.shields.io/github/stars/zzw922cn/awesome-speech-recognition-speech-synthesis-papers" alt></p>
</li>
<li><p><a href="https://github.com/charlesliucn/awesome-end2end-speech-recognition" target="_blank" rel="noopener">End2End Speech Recognition</a> <br><br><img src="https://img.shields.io/badge/author-charlesliucn-be8abf" alt><br><img src="https://img.shields.io/github/stars/charlesliucn/awesome-end2end-speech-recognition" alt></p>
</li>
<li><a href="https://github.com/cyrta/awesome-speech-enhancement" target="_blank" rel="noopener">Speech Enhancement</a> <br><br><img src="https://img.shields.io/badge/author-cyrta-be8abf" alt><br><img src="https://img.shields.io/github/stars/cyrta/awesome-speech-enhancement" alt></li>
</ul>
<h2 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h2><ul>
<li><a href="https://github.com/dragen1860/awesome-causal-reasoning" target="_blank" rel="noopener">Causal Reasoning</a> <br><br><img src="https://img.shields.io/badge/author-dragen1860-be8abf" alt><br><img src="https://img.shields.io/github/stars/dragen1860/awesome-causal-reasoning" alt></li>
<li><a href="https://github.com/imirzadeh/awesome-causal-inference" target="_blank" rel="noopener">Causal Inference</a> <br><br><img src="https://img.shields.io/badge/author-imirzadeh-be8abf" alt><br><img src="https://img.shields.io/github/stars/imirzadeh/awesome-causal-inference" alt></li>
<li><a href="https://github.com/rguo12/awesome-causality-algorithms" target="_blank" rel="noopener">Causality Algorithms</a> <br><br><img src="https://img.shields.io/badge/author-rguo12-be8abf" alt><br><img src="https://img.shields.io/github/stars/rguo12/awesome-causality-algorithms" alt></li>
<li><a href="https://github.com/napsternxg/awesome-causality" target="_blank" rel="noopener">Causality</a> <br><br><img src="https://img.shields.io/badge/author-napsternxg-be8abf" alt><br><img src="https://img.shields.io/github/stars/napsternxg/awesome-causality" alt></li>
<li><a href="https://github.com/huckiyang/awesome-deep-causal-learning" target="_blank" rel="noopener">Deep Causal Learning</a> <br><br><img src="https://img.shields.io/badge/author-huckiyang-be8abf" alt><br><img src="https://img.shields.io/github/stars/huckiyang/awesome-deep-causal-learning" alt></li>
</ul>
<h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><ul>
<li><p><a href="https://github.com/yzhao062/anomaly-detection-resources" target="_blank" rel="noopener">Anomaly Detection</a> <br><br><img src="https://img.shields.io/badge/author-yzhao062-be8abf" alt><br><img src="https://img.shields.io/github/stars/yzhao062/anomaly-detection-resources" alt></p>
</li>
<li><p><a href="https://github.com/hoya012/awesome-anomaly-detection" target="_blank" rel="noopener">Anomaly Detection</a> <br><br><img src="https://img.shields.io/badge/author-hoya012-be8abf" alt><br><img src="https://img.shields.io/github/stars/hoya012/awesome-anomaly-detection" alt></p>
</li>
<li><a href="https://github.com/rob-med/awesome-TS-anomaly-detection" target="_blank" rel="noopener">Anomaly Detection on Time-Series Data</a> <br><br><img src="https://img.shields.io/badge/author-rob_med-be8abf" alt><br><img src="https://img.shields.io/github/stars/rob-med/awesome-TS-anomaly-detection" alt></li>
</ul>
<h2 id="Thanks"><a href="#Thanks" class="headerlink" title="Thanks"></a>Thanks</h2><p>Thanks to authors of all the repositories I cite. :)</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（三） —— web3js编写以太坊脚本</title>
    <url>/posts/733c8737.html</url>
    <content><![CDATA[<h1 id="web3-js基础"><a href="#web3-js基础" class="headerlink" title="web3.js基础"></a>web3.js基础</h1><h2 id="web3-js是什么"><a href="#web3-js是什么" class="headerlink" title="web3.js是什么"></a>web3.js是什么</h2><ul>
<li>Web3 JavaScript app API</li>
<li>web3..js 是一个JavaScript API库。要使DApper在以太坊上运行，我们可以使用web3.js库提供的web3对象</li>
<li>web3.js通过RPC调用与本地节点通信，它可以用于任何暴露了RPC层的以太坊节点</li>
<li>web3包含了eth对象 - web3.eth（专门与以太坊区块链交互）和 shh对象 - web3.shh（用于与 Whisper交互）[Whisper是以太坊生态系统的一部分，主要用来做消息传递]</li>
</ul>
<p>如果我们想要在以太坊上开发合约，目前来说最方便的方法就是调用Web3.js库，它会给我们一个Web3对象。我们首先进入geth控制台，直接键入web3，下面对这些弹出的内容进行一个总览。</p>
<p>我们先看到db，db是操作区块链底层数据库的，整个以太坊的底层数据库就是LevelDB，其接口如下：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/db.png" alt></p>
<p>然后看到eth，一个我们已经很熟悉的模块：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/eth.png" alt></p>
<p>里面含有getBalance,gasPrice等最常用的操作。</p>
<p>再然后是personal，里面包含了我们创建账户的信息：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/personal.png" alt></p>
<p>还有shh等等，这里就不一一列举了：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/shh.png" alt></p>
<h2 id="web3-模块加载"><a href="#web3-模块加载" class="headerlink" title="web3 模块加载"></a>web3 模块加载</h2><ul>
<li>首先需要将 web3 模块安装在项目中，安装方式为（后面可以加版本也可以不加）  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install web3@0.20.1</span><br></pre></td></tr></table></figure></li>
<li>然后创建一个web3实例，设置一个”provider”</li>
<li>为了保证我们的MetaMask设置好的provider不被覆盖掉，在引入web3之前我们一般要做当前环境检查(以v0.20.1为例)：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if(typeof web3 !&#x3D;&#x3D; &#39;undifined&#39;)&#123;</span><br><span class="line">	web3 &#x3D; new Web3(web3.currentProvider);</span><br><span class="line">&#125;else&#123;</span><br><span class="line">	web3 &#x3D; new Web3(new Web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="异步回调（callback）"><a href="#异步回调（callback）" class="headerlink" title="异步回调（callback）"></a>异步回调（callback）</h2><ul>
<li>web3js API 设计的最初目的，主要是为了和本地RPC节点共同使用，所以默认情况下发送的是同步HTTP请求</li>
<li>如果要发送异步请求，可以在函数的最后一个参数位置上，传入一个回调函数，回调函数是可选的(optional)</li>
<li>我们一般采用的风格是所谓的“错误优先”，例如：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getback(48, function(error, result))&#123;</span><br><span class="line">	if(!error)</span><br><span class="line">    	console.log(JSON.stringify(result));</span><br><span class="line">    else</span><br><span class="line">    	console.error(error);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>我们直接在geth尝试这一过程，并与同步过程对比：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/ybdy.png" alt></p>
<p>我们看到输出的内容没有任何区别，不过由刚才的同步调用方式改成了异步调用，那有了更简便的同步调用，我们为何还需要异步调用呢？</p>
<p>同步调用会将当前执行的进程完全阻塞在这里，只有当前面的步骤拿到代码返回之后后面的代码才会执行，所以同步的顺序是指定的，让谁先执行谁就先执行，但劣势也在此，可能会一直被卡在这里，在开发DApp等实际应用的时候，往往都需要用异步，互不干扰。</p>
<h2 id="回调Promise事件"><a href="#回调Promise事件" class="headerlink" title="回调Promise事件"></a>回调Promise事件</h2><p>目前基本上所有的东西大家都默认了状态是异步调用，那我们是否就无法保证顺序了呢？实际上不是的。</p>
<ul>
<li>为了帮助web3集成到不同标准的所有类型项目中，1.0.0版本提供了多种方式来处理异步函数。大多数的web3对象允许将一个回调函数作为最后一个函数参数传入，同时返回一个promise用于链式函数调用。</li>
<li>以太坊作为一个区块链系统，一次请求具有不同的结束阶段。为了满足这样的请求，1.0.0版本将这类函数调用的返回之包成一个“承诺事件”(promiEvent)，这是一个promise和EventEmitter的结合体</li>
<li>PromiEvent的用法就像promise一样，另外还加入了.on,.once和.off方法  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.sendTransaction(&#123;from:&#39;0x123...&#39;, data:&#39;0x432...&#39;&#125;)</span><br><span class="line">.once(&#39;transactionHash&#39;, function(hash)&#123;...&#125;)</span><br><span class="line">.once(&#39;receipt&#39;, function(receipt)&#123;...&#125;)</span><br><span class="line">.on(&#39;confirmation&#39;, function(confNumber, receipt)&#123;...&#125;)</span><br><span class="line">.on(&#39;error&#39;, function&#123;...&#125;)</span><br><span class="line">.then(function(receipt)&#123;&#125;);</span><br></pre></td></tr></table></figure>
回调完成的标志是收到receipt，也就是交易打包进块。</li>
</ul>
<h2 id="应用二进制接口（ABI）"><a href="#应用二进制接口（ABI）" class="headerlink" title="应用二进制接口（ABI）"></a>应用二进制接口（ABI）</h2><ul>
<li>web3.js通过以太坊智能合约的json接口（Application Binary Interface， ABI）创建一个JavaScript对象，用来在js代码中描述</li>
<li>函数（functions）<ul>
<li>type:函数类型，默认“function”，也可能是”constructor”</li>
<li>constant, payable, stateMutability: 函数的状态可变性</li>
<li>inputs, outputs: 函数输入、输出参数描述列表</li>
</ul>
</li>
<li>事件（events）<ul>
<li>type: 类型，总是”event”</li>
<li>inputs: 输入对象列表，包括name、type、indexed</li>
</ul>
</li>
</ul>
<p>我们首先创建一个sol文件，并进行编译，Coin.sol文件如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;<span class="number">0.4</span><span class="number">.22</span>;</span><br><span class="line"></span><br><span class="line">contract Coin&#123;</span><br><span class="line">    address public minter;</span><br><span class="line">    mapping(address=&gt;uint) public balances;</span><br><span class="line">    event Sent(address <span class="keyword">from</span>, address to, uint amount);</span><br><span class="line">    constructor()public&#123;</span><br><span class="line">        minter = msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line">    function mint(address receiver, uint amount)public&#123;</span><br><span class="line">        require(msg.sender == minter);</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">    &#125;</span><br><span class="line">    function send(address receiver, uint amount)public&#123;</span><br><span class="line">        require(balances[msg.sender] &gt;= amount);</span><br><span class="line">        balances[msg.sender] -= amount;</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">        emit Sent(msg.sender, receiver, amount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/abi.png" alt></p>
<p>将上面的JSON文件稍微格式化一下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"constructor"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"anonymous"</span>: <span class="literal">false</span>,</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"from"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"to"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"Sent"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"event"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"balances"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"receiver"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"mint"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"minter"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"receiver"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"send"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure>
<p>我们集中看一下下面这小段：<br><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">"anonymous"</span>: <span class="literal">false</span>,</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"from"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"to"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"Sent"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"event"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>首先是一个<strong>annoymous</strong>，这是一个匿名参数，如果你填了false，我们的事件在日志中的第一条topic就会为空（不写入），主题是对整个事件做的哈希，也就相当于这个事件没有签名了，事实上他出发的其他事件的log仍会计入，只不过没有整个事件签名了。然后是<strong>index</strong>，定义参数时如果设置index=True，则这个参数会被设置成可索引参数，就会被记在topic下。<br>我们可以看到这段JSON对应的是下面这一行代码，于是其他部分我们也很容易一一对上了：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">event Sent(address <span class="keyword">from</span>, address to, uint amount);</span><br></pre></td></tr></table></figure></p>
<p>我们看到合约编译可生成两种文件，一种是字节码，这是要部署到以太坊上的，另一种是根据源码生成ABI，这一套二进制接口是给web3使用的。下面我们安装web3模块：</p>
<ul>
<li>安装nodejs</li>
<li>npm install web3@^0.20.0</li>
<li>npm i npm to update</li>
<li>npm cache verify</li>
<li>npm install -g ethereumjs-testrpc</li>
<li>在终端启动testrpc</li>
<li>切换新的终端，创建文件connect.js，文件内容为  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var Web3 &#x3D; require(&#39;web3&#39;)</span><br><span class="line">var web3 &#x3D; new Web3(new Web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;))</span><br><span class="line">console.log(web3.eth.accounts)</span><br><span class="line">console.log(&#39;OK&#39;)</span><br><span class="line">var version &#x3D; web3.version.node;</span><br><span class="line">console.log(version);</span><br></pre></td></tr></table></figure></li>
<li>node connect.js后显示：</li>
</ul>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_1.png" alt></p>
<p>事实上我们也可以一行行在node命令行中输入，这样可以更清晰地观察到结果：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_2.png" alt></p>
<p>代码同上，读者自行键入即可。我们还可以获得web3的其他信息：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_3.png" alt></p>
<h2 id="批处理请求-batch-requests"><a href="#批处理请求-batch-requests" class="headerlink" title="批处理请求(batch requests)"></a>批处理请求(batch requests)</h2><ul>
<li>批处理请求允许我们将请求排序，然后一起处理它们</li>
<li>注意：批处理请求不会更快，在某些情况下，一次性地发出许多请求会更快，因为请求是异步处理的。（我们想要加速通常会手动进行异步处理）</li>
<li>批处理请求主要用于确保请求的顺序，并串行处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">var batch = web3.createBatch();</span><br><span class="line">batch.add(web3.eth.getBalance.request(<span class="string">'0x0000000000000'</span>, <span class="string">'latest'</span>, callback));</span><br><span class="line">batch.add(web3.eth.contract(abi).at(address).balance.requests(address,callback2));</span><br><span class="line">batch.excute();</span><br></pre></td></tr></table></figure>
<h2 id="大数处理-big-numbers"><a href="#大数处理-big-numbers" class="headerlink" title="大数处理(big numbers)"></a>大数处理(big numbers)</h2><ul>
<li>JavaScript中默认的数字精度较小，所以web3.js会自动添加一个依赖库BigNumber，专门用于大数处理</li>
<li>对于数值，我们应该习惯将它转化为BigNumber对象来处理</li>
<li>BigNumber.toString(10)对小数只保留20位浮点精度，所以推荐的做法是，我们内部总是用wei来表示余额（大整数），只有在需要显示给用户看的时候才转化为Ether或其他单位</li>
</ul>
<p>定义方式如下：</p>
<p><img src alt="define_bignumber"></p>
<p>我们看到显示的s:1表示这个数是正数，若为负数s=-1，c则为 字符串拼接结果，e为科学计数法e跟的位数，c是所有有效数字，每14位对BigNumber切割一次形成的数组，toString()可以看到这个数字，也可以在括号内填如希望转化的进制。</p>
<h1 id="常用API-——-基本信息查询"><a href="#常用API-——-基本信息查询" class="headerlink" title="常用API —— 基本信息查询"></a>常用API —— 基本信息查询</h1><p>下面先列举常用命令，后面再一一敲代码~</p>
<h2 id="基本信息查询"><a href="#基本信息查询" class="headerlink" title="基本信息查询"></a>基本信息查询</h2><p><strong>查看web3版本</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.api</span><br></pre></td></tr></table></figure>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>查看web3连接到的节点版本(clientVersion)</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.node</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getNode((error,resuIt)&#x3D;&gt;console.log(result))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getNodeInfo().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>获取network</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.network</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getNetwork((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.getId().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>获取点以太坊版本</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.ethereum</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getEthereum((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getProtocolVersion().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>具体操作见下图：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/version.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/version2.png" alt></p>
<h2 id="网络状态查询"><a href="#网络状态查询" class="headerlink" title="网络状态查询"></a>网络状态查询</h2><p><strong>是否有节点连接/监听，返回true</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.isConnect() 或者 web3.net.listening</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.getListening((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.isListening().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>查看当前连接的peer节点</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.peerCount</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.getPeerCount((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.getPeerCount().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Provider</strong></p>
<ul>
<li><p>查看当前设置的web3 provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.currentPrrovider</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看浏览器环境设置的web3 provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.givenProvider</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.setProvider(new web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;))</span><br></pre></td></tr></table></figure>
<p>需要注意的是，0.20.1版本与1.0.0版本的操作有些出入。</p>
</li>
</ul>
<h2 id="web3通用工具方法"><a href="#web3通用工具方法" class="headerlink" title="web3通用工具方法"></a>web3通用工具方法</h2><ul>
<li>以太单位转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3..fromWei</span><br><span class="line">web3..toWei</span><br></pre></td></tr></table></figure></li>
<li>数据类型转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.toString</span><br><span class="line">web3.toDecimal</span><br><span class="line">web3.toBigNumber</span><br></pre></td></tr></table></figure></li>
<li>字符编码转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.toHex</span><br><span class="line">web3.toAscii</span><br><span class="line">web3.toUtf8</span><br><span class="line">web3.fromUtf8</span><br></pre></td></tr></table></figure></li>
<li>地址相关<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.isAddress</span><br><span class="line">web3.toChecksumAddress</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>注意</strong>：</p>
<ol>
<li>地址是40个16进制字符，比如：0x4DFdd4c39B99C88d795E7a200f05A6A8f5D80A5b</li>
<li>在1.0.0版本中，上述操作大多被放入web3.utils中</li>
</ol>
<h2 id="web3-eth-——-账户相关"><a href="#web3-eth-——-账户相关" class="headerlink" title="web3.eth —— 账户相关"></a>web3.eth —— 账户相关</h2><p><strong>coinbase查询</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.coinbase</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getCoinbase((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getCoinbase().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>账户查询</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.accounts</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getAccounts((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getAccounts().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="区块相关"><a href="#区块相关" class="headerlink" title="区块相关"></a>区块相关</h2><p><strong>区块高度查询</strong></p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.blockNumber</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBlockNumber(callback)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>gasPrice 查询</strong></p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.gasPrice</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getGasPrice(callback)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="交易相关"><a href="#交易相关" class="headerlink" title="交易相关"></a>交易相关</h2><ul>
<li><p>余额查询</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBalance(addressHexString [, defaultBlock])</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBalance(addressHexString [, defaultBlock] [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>交易查询</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransaction(transactionHash)</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransaction(transactionHash [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>交易收据查询</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransactionReceipt(hashString)</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransactionReceipt(hashString [, callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>估计gas消耗量</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.estimateGas(callObject)</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.estimateGas(callObject [, callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>发送交易</p>
<ul>
<li>from： 发送地址</li>
<li>to：接收地址</li>
<li>value：交易金额，以wei为单位，可选</li>
<li>gas：交易消耗gas上限，可选</li>
<li>gasPrice：交易gas单价，可选</li>
<li>data：交易携带的字串数据，可选</li>
<li>nonce：整数nonce值，可选<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.sendTransaction(transactionObject [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>发送交易过程如下：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/trans1.png" alt></p>
<h2 id="消息调用"><a href="#消息调用" class="headerlink" title="消息调用"></a>消息调用</h2><p>消息调用与交易的区别是，当我们想给合约发起调用（调用函数，这时我们使用的是call方法），给别人发币时使用sendTransaction()，而调用合约时其实也可以使用sendTransaction()，但一般我们将不需要提交交易的消息调用上，比如我们不做状态改变（纯计算、查询等）。若引发了状态改变则一定要用sendTransaction()。</p>
<p><strong>参数</strong>：</p>
<ul>
<li>调用对象：与交易对象相同，只是from也是可选的</li>
<li>默认区块：默认”latest“，可以传入指定的区块高度</li>
<li>回调函数：如果没有则为同步调用</li>
</ul>
<h2 id="日志过滤（事件监听）"><a href="#日志过滤（事件监听）" class="headerlink" title="日志过滤（事件监听）"></a>日志过滤（事件监听）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.filter(filterOptions [.callback])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>filterString可以是”latest”or”pending”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var filter &#x3D; web3.eth.filter(filterString);</span><br></pre></td></tr></table></figure>
</li>
<li><p>或者可以填入一个日志过滤options</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var filter &#x3D; web3.eth.filter(options);</span><br></pre></td></tr></table></figure>
</li>
<li><p>监听日志变化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filter.watch(options, function(error, result)&#123;</span><br><span class="line">	if (!error) console.log(result);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filter.stopWatching()</span><br></pre></td></tr></table></figure>
</li>
<li><p>还可以用传入回调函数的方法，立刻开始监听日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.filter(options, function(error, result)&#123;</span><br><span class="line">	if (!error) console.log(result);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/posts/Blockchain/filter.png" alt></p>
<h2 id="合约相关"><a href="#合约相关" class="headerlink" title="合约相关"></a>合约相关</h2><h3 id="创建合约"><a href="#创建合约" class="headerlink" title="创建合约"></a>创建合约</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.contract</span><br></pre></td></tr></table></figure>
<p>创建合约有两种方式，第一种方式为传入abi：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var MyContract &#x3D; web3.eth.contract(abiArray);</span><br></pre></td></tr></table></figure>
<p>这时候我们拿到了一个js中的合约实例，但还未和区块链关联起来，我们还需要通过地址初始化合约实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var contractinstance &#x3D; MyContract.at(address);</span><br></pre></td></tr></table></figure>
<p>或者是部署一个新合约：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var contractinstance &#x3D; MyContract.new([contructorParam1][contructorParam2],&#123;data:&quot;0x12345...&quot;,from:myAccount, gas&#125;)</span><br></pre></td></tr></table></figure>
<p>constructorParam1主要是传入需要赋的初值，data部分其实就是字节码。因此综上所述，在使用web3部署合约时需同时使用abi和字节码。接下来我们开始部署自己的合约：</p>
<p>合约如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;0.6.0;</span><br><span class="line"></span><br><span class="line">contract Coin&#123;</span><br><span class="line">    address public minter;</span><br><span class="line">    mapping(address&#x3D;&gt;uint) public balances;</span><br><span class="line">    event Sent(address from, address to, uint amount);</span><br><span class="line">    constructor()public&#123;</span><br><span class="line">        minter &#x3D; msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line">    function mint(address receiver, uint amount)public&#123;</span><br><span class="line">        require(msg.sender &#x3D;&#x3D; minter);</span><br><span class="line">        balances[receiver] +&#x3D; amount;</span><br><span class="line">    &#125;</span><br><span class="line">    function send(address receiver, uint amount)public&#123;</span><br><span class="line">        require(balances[msg.sender] &gt;&#x3D; amount);</span><br><span class="line">        balances[msg.sender] -&#x3D; amount;</span><br><span class="line">        balances[receiver] +&#x3D; amount;</span><br><span class="line">        </span><br><span class="line">        emit Sent(msg.sender, receiver, amount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract1.png" alt></p>
<p>我们在上面的步骤中，首先将前面已经得到的contract编译过后的abi传入：var coinContract = web3.eth.contract(abi)，然后获得字节码(byteCode)，连同其他参数一起new合约，于是就部署好了一个合约，这里需要注意的是，在传入字节码时需要在最前面加入’0x’，这是由于字节码本身就是16进制的，但编译出来的结果头部并没有带0x，需要手动去加。</p>
<p>我们看一看部署的合约长啥样：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract2.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract3.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract4.png" alt></p>
<p>以上是打印结果，然后我们可以通过coinContractInstance.address获得合约地址，同时也可以根据相应接口获得其他信息。</p>
<h3 id="调用合约"><a href="#调用合约" class="headerlink" title="调用合约"></a>调用合约</h3><p>可以通过已创建的合约实例，直接调用合约函数</p>
<ul>
<li><p>直接调用，自动按函数类型决定用sendTransaction还是call</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显式以消息调用形式call该函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod.call(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显式以发送交易形式调用该函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod.sendTransaction(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>最后的callback回调函数可选，若不选则为同步调用，但推荐使用异步调用方式。</p>
<p>我们还是来直接尝试调用一下合约：（由于testrpc不太方便，我在这里改用了ganache）</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin1.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin2.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin3.png" alt></p>
<p>需要注意的是，这里我们需要在mint的时候写上：{from:web3.eth.accounts[0]}，因为需要让合约知道调用合约的人是谁才能发币。</p>
<p>我们可以看到在ganache-cli中立刻出现了我们刚刚调用的合约情况：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin4.png" alt></p>
<h3 id="监听合约事件"><a href="#监听合约事件" class="headerlink" title="监听合约事件"></a>监听合约事件</h3><ul>
<li><p>合约的event类似于filter，可以设置过略选项来监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var event &#x3D; myContractInstance.MyEvent(&#123;valueA:23&#125; [,additionalFilterObject])</span><br></pre></td></tr></table></figure>
</li>
<li><p>监听事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">event.watch(function(err,, res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以使用传入回调函数的方法，立刻开始监听事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var event &#x3D; myContractInstance.MyEvent(&#123;valueA:23&#125;</span><br><span class="line">			[,additionalFilterObject], function(err, res)&#123;</span><br><span class="line">          			if(!err) console.log(res);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以通过以下方式触发监听：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/sent.png" alt></p>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（二） —— web3js</title>
    <url>/posts/67d22044.html</url>
    <content><![CDATA[<h1 id="web3-js基础"><a href="#web3-js基础" class="headerlink" title="web3.js基础"></a>web3.js基础</h1><h2 id="web3-js是什么"><a href="#web3-js是什么" class="headerlink" title="web3.js是什么"></a>web3.js是什么</h2><ul>
<li>Web3 JavaScript app API</li>
<li>web3..js 是一个JavaScript API库。要使DApper在以太坊上运行，我们可以使用web3.js库提供的web3对象</li>
<li>web3.js通过RPC调用与本地节点通信，它可以用于任何暴露了RPC层的以太坊节点</li>
<li>web3包含了eth对象 - web3.eth（专门与以太坊区块链交互）和 shh对象 - web3.shh（用于与 Whisper交互）[Whisper是以太坊生态系统的一部分，主要用来做消息传递]</li>
</ul>
<p>如果我们想要在以太坊上开发合约，目前来说最方便的方法就是调用Web3.js库，它会给我们一个Web3对象。我们首先进入geth控制台，直接键入web3，下面对这些弹出的内容进行一个总览。</p>
<p>我们先看到db，db是操作区块链底层数据库的，整个以太坊的底层数据库就是LevelDB，其接口如下：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/db.png" alt></p>
<p>然后看到eth，一个我们已经很熟悉的模块：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/eth.png" alt></p>
<p>里面含有getBalance,gasPrice等最常用的操作。</p>
<p>再然后是personal，里面包含了我们创建账户的信息：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/personal.png" alt></p>
<p>还有shh等等，这里就不一一列举了：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/shh.png" alt></p>
<h2 id="web3-模块加载"><a href="#web3-模块加载" class="headerlink" title="web3 模块加载"></a>web3 模块加载</h2><ul>
<li>首先需要将 web3 模块安装在项目中，安装方式为（后面可以加版本也可以不加）  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install web3@0.20.1</span><br></pre></td></tr></table></figure></li>
<li>然后创建一个web3实例，设置一个”provider”</li>
<li>为了保证我们的MetaMask设置好的provider不被覆盖掉，在引入web3之前我们一般要做当前环境检查(以v0.20.1为例)：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if(typeof web3 !&#x3D;&#x3D; &#39;undifined&#39;)&#123;</span><br><span class="line">	web3 &#x3D; new Web3(web3.currentProvider);</span><br><span class="line">&#125;else&#123;</span><br><span class="line">	web3 &#x3D; new Web3(new Web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="异步回调（callback）"><a href="#异步回调（callback）" class="headerlink" title="异步回调（callback）"></a>异步回调（callback）</h2><ul>
<li>web3js API 设计的最初目的，主要是为了和本地RPC节点共同使用，所以默认情况下发送的是同步HTTP请求</li>
<li>如果要发送异步请求，可以在函数的最后一个参数位置上，传入一个回调函数，回调函数是可选的(optional)</li>
<li>我们一般采用的风格是所谓的“错误优先”，例如：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getback(48, function(error, result))&#123;</span><br><span class="line">	if(!error)</span><br><span class="line">    	console.log(JSON.stringify(result));</span><br><span class="line">    else</span><br><span class="line">    	console.error(error);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>我们直接在geth尝试这一过程，并与同步过程对比：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/ybdy.png" alt></p>
<p>我们看到输出的内容没有任何区别，不过由刚才的同步调用方式改成了异步调用，那有了更简便的同步调用，我们为何还需要异步调用呢？</p>
<p>同步调用会将当前执行的进程完全阻塞在这里，只有当前面的步骤拿到代码返回之后后面的代码才会执行，所以同步的顺序是指定的，让谁先执行谁就先执行，但劣势也在此，可能会一直被卡在这里，在开发DApp等实际应用的时候，往往都需要用异步，互不干扰。</p>
<h2 id="回调Promise事件"><a href="#回调Promise事件" class="headerlink" title="回调Promise事件"></a>回调Promise事件</h2><p>目前基本上所有的东西大家都默认了状态是异步调用，那我们是否就无法保证顺序了呢？实际上不是的。</p>
<ul>
<li>为了帮助web3集成到不同标准的所有类型项目中，1.0.0版本提供了多种方式来处理异步函数。大多数的web3对象允许将一个回调函数作为最后一个函数参数传入，同时返回一个promise用于链式函数调用。</li>
<li>以太坊作为一个区块链系统，一次请求具有不同的结束阶段。为了满足这样的请求，1.0.0版本将这类函数调用的返回之包成一个“承诺事件”(promiEvent)，这是一个promise和EventEmitter的结合体</li>
<li>PromiEvent的用法就像promise一样，另外还加入了.on,.once和.off方法  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.sendTransaction(&#123;from:&#39;0x123...&#39;, data:&#39;0x432...&#39;&#125;)</span><br><span class="line">.once(&#39;transactionHash&#39;, function(hash)&#123;...&#125;)</span><br><span class="line">.once(&#39;receipt&#39;, function(receipt)&#123;...&#125;)</span><br><span class="line">.on(&#39;confirmation&#39;, function(confNumber, receipt)&#123;...&#125;)</span><br><span class="line">.on(&#39;error&#39;, function&#123;...&#125;)</span><br><span class="line">.then(function(receipt)&#123;&#125;);</span><br></pre></td></tr></table></figure>
回调完成的标志是收到receipt，也就是交易打包进块。</li>
</ul>
<h2 id="应用二进制接口（ABI）"><a href="#应用二进制接口（ABI）" class="headerlink" title="应用二进制接口（ABI）"></a>应用二进制接口（ABI）</h2><ul>
<li>web3.js通过以太坊智能合约的json接口（Application Binary Interface， ABI）创建一个JavaScript对象，用来在js代码中描述</li>
<li>函数（functions）<ul>
<li>type:函数类型，默认“function”，也可能是”constructor”</li>
<li>constant, payable, stateMutability: 函数的状态可变性</li>
<li>inputs, outputs: 函数输入、输出参数描述列表</li>
</ul>
</li>
<li>事件（events）<ul>
<li>type: 类型，总是”event”</li>
<li>inputs: 输入对象列表，包括name、type、indexed</li>
</ul>
</li>
</ul>
<p>我们首先创建一个sol文件，并进行编译，Coin.sol文件如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;<span class="number">0.4</span><span class="number">.22</span>;</span><br><span class="line"></span><br><span class="line">contract Coin&#123;</span><br><span class="line">    address public minter;</span><br><span class="line">    mapping(address=&gt;uint) public balances;</span><br><span class="line">    event Sent(address <span class="keyword">from</span>, address to, uint amount);</span><br><span class="line">    constructor()public&#123;</span><br><span class="line">        minter = msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line">    function mint(address receiver, uint amount)public&#123;</span><br><span class="line">        require(msg.sender == minter);</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">    &#125;</span><br><span class="line">    function send(address receiver, uint amount)public&#123;</span><br><span class="line">        require(balances[msg.sender] &gt;= amount);</span><br><span class="line">        balances[msg.sender] -= amount;</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">        emit Sent(msg.sender, receiver, amount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/abi.png" alt></p>
<p>将上面的JSON文件稍微格式化一下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"constructor"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"anonymous"</span>: <span class="literal">false</span>,</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"from"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"to"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"Sent"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"event"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"balances"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"receiver"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"mint"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"minter"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">""</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"receiver"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"send"</span>,</span><br><span class="line">	<span class="attr">"outputs"</span>: [],</span><br><span class="line">	<span class="attr">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure>
<p>我们集中看一下下面这小段：<br><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">"anonymous"</span>: <span class="literal">false</span>,</span><br><span class="line">	<span class="attr">"inputs"</span>: [&#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"from"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"address"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"to"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"address"</span></span><br><span class="line">	&#125;, &#123;</span><br><span class="line">		<span class="attr">"indexed"</span>: <span class="literal">false</span>,</span><br><span class="line">		<span class="attr">"internalType"</span>: <span class="string">"uint256"</span>,</span><br><span class="line">		<span class="attr">"name"</span>: <span class="string">"amount"</span>,</span><br><span class="line">		<span class="attr">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">	&#125;],</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"Sent"</span>,</span><br><span class="line">	<span class="attr">"type"</span>: <span class="string">"event"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>首先是一个<strong>annoymous</strong>，这是一个匿名参数，如果你填了false，我们的事件在日志中的第一条topic就会为空（不写入），主题是对整个事件做的哈希，也就相当于这个事件没有签名了，事实上他出发的其他事件的log仍会计入，只不过没有整个事件签名了。然后是<strong>index</strong>，定义参数时如果设置index=True，则这个参数会被设置成可索引参数，就会被记在topic下。<br>我们可以看到这段JSON对应的是下面这一行代码，于是其他部分我们也很容易一一对上了：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">event Sent(address <span class="keyword">from</span>, address to, uint amount);</span><br></pre></td></tr></table></figure></p>
<p>我们看到合约编译可生成两种文件，一种是字节码，这是要部署到以太坊上的，另一种是根据源码生成ABI，这一套二进制接口是给web3使用的。下面我们安装web3模块：</p>
<ul>
<li>安装nodejs</li>
<li>npm install web3@^0.20.0</li>
<li>npm i npm to update</li>
<li>npm cache verify</li>
<li>npm install -g ethereumjs-testrpc</li>
<li>在终端启动testrpc</li>
<li>切换新的终端，创建文件connect.js，文件内容为  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var Web3 &#x3D; require(&#39;web3&#39;)</span><br><span class="line">var web3 &#x3D; new Web3(new Web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;))</span><br><span class="line">console.log(web3.eth.accounts)</span><br><span class="line">console.log(&#39;OK&#39;)</span><br><span class="line">var version &#x3D; web3.version.node;</span><br><span class="line">console.log(version);</span><br></pre></td></tr></table></figure></li>
<li>node connect.js后显示：</li>
</ul>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_1.png" alt></p>
<p>事实上我们也可以一行行在node命令行中输入，这样可以更清晰地观察到结果：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_2.png" alt></p>
<p>代码同上，读者自行键入即可。我们还可以获得web3的其他信息：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/web3_3.png" alt></p>
<h2 id="批处理请求-batch-requests"><a href="#批处理请求-batch-requests" class="headerlink" title="批处理请求(batch requests)"></a>批处理请求(batch requests)</h2><ul>
<li>批处理请求允许我们将请求排序，然后一起处理它们</li>
<li>注意：批处理请求不会更快，在某些情况下，一次性地发出许多请求会更快，因为请求是异步处理的。（我们想要加速通常会手动进行异步处理）</li>
<li>批处理请求主要用于确保请求的顺序，并串行处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">var batch = web3.createBatch();</span><br><span class="line">batch.add(web3.eth.getBalance.request(<span class="string">'0x0000000000000'</span>, <span class="string">'latest'</span>, callback));</span><br><span class="line">batch.add(web3.eth.contract(abi).at(address).balance.requests(address,callback2));</span><br><span class="line">batch.excute();</span><br></pre></td></tr></table></figure>
<h2 id="大数处理-big-numbers"><a href="#大数处理-big-numbers" class="headerlink" title="大数处理(big numbers)"></a>大数处理(big numbers)</h2><ul>
<li>JavaScript中默认的数字精度较小，所以web3.js会自动添加一个依赖库BigNumber，专门用于大数处理</li>
<li>对于数值，我们应该习惯将它转化为BigNumber对象来处理</li>
<li>BigNumber.toString(10)对小数只保留20位浮点精度，所以推荐的做法是，我们内部总是用wei来表示余额（大整数），只有在需要显示给用户看的时候才转化为Ether或其他单位</li>
</ul>
<p>定义方式如下：</p>
<p><img src alt="define_bignumber"></p>
<p>我们看到显示的s:1表示这个数是正数，若为负数s=-1，c则为 字符串拼接结果，e为科学计数法e跟的位数，c是所有有效数字，每14位对BigNumber切割一次形成的数组，toString()可以看到这个数字，也可以在括号内填如希望转化的进制。</p>
<h1 id="常用API-——-基本信息查询"><a href="#常用API-——-基本信息查询" class="headerlink" title="常用API —— 基本信息查询"></a>常用API —— 基本信息查询</h1><p>下面先列举常用命令，后面再一一敲代码~</p>
<h2 id="基本信息查询"><a href="#基本信息查询" class="headerlink" title="基本信息查询"></a>基本信息查询</h2><p><strong>查看web3版本</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.api</span><br></pre></td></tr></table></figure>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>查看web3连接到的节点版本(clientVersion)</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.node</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getNode((error,resuIt)&#x3D;&gt;console.log(result))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getNodeInfo().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>获取network</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.network</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getNetwork((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.getId().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>获取点以太坊版本</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.ethereum</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.version.getEthereum((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getProtocolVersion().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>具体操作见下图：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/version.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/version2.png" alt></p>
<h2 id="网络状态查询"><a href="#网络状态查询" class="headerlink" title="网络状态查询"></a>网络状态查询</h2><p><strong>是否有节点连接/监听，返回true</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.isConnect() 或者 web3.net.listening</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.getListening((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.isListening().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>查看当前连接的peer节点</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.peerCount</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.net.getPeerCount((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.net.getPeerCount().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Provider</strong></p>
<ul>
<li><p>查看当前设置的web3 provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.currentPrrovider</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看浏览器环境设置的web3 provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.givenProvider</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置provider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.setProvider(new web3.providers.HttpProvider(&#39;http:&#x2F;&#x2F;localhost:8545&#39;))</span><br></pre></td></tr></table></figure>
<p>需要注意的是，0.20.1版本与1.0.0版本的操作有些出入。</p>
</li>
</ul>
<h2 id="web3通用工具方法"><a href="#web3通用工具方法" class="headerlink" title="web3通用工具方法"></a>web3通用工具方法</h2><ul>
<li>以太单位转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3..fromWei</span><br><span class="line">web3..toWei</span><br></pre></td></tr></table></figure></li>
<li>数据类型转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.toString</span><br><span class="line">web3.toDecimal</span><br><span class="line">web3.toBigNumber</span><br></pre></td></tr></table></figure></li>
<li>字符编码转换<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.toHex</span><br><span class="line">web3.toAscii</span><br><span class="line">web3.toUtf8</span><br><span class="line">web3.fromUtf8</span><br></pre></td></tr></table></figure></li>
<li>地址相关<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.isAddress</span><br><span class="line">web3.toChecksumAddress</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>注意</strong>：</p>
<ol>
<li>地址是40个16进制字符，比如：0x4DFdd4c39B99C88d795E7a200f05A6A8f5D80A5b</li>
<li>在1.0.0版本中，上述操作大多被放入web3.utils中</li>
</ol>
<h2 id="web3-eth-——-账户相关"><a href="#web3-eth-——-账户相关" class="headerlink" title="web3.eth —— 账户相关"></a>web3.eth —— 账户相关</h2><p><strong>coinbase查询</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.coinbase</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getCoinbase((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getCoinbase().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>账户查询</strong></p>
<ul>
<li><p>v0.2.x.x</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.accounts</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getAccounts((err,res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>v1.0.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getAccounts().then(console.log)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="区块相关"><a href="#区块相关" class="headerlink" title="区块相关"></a>区块相关</h2><p><strong>区块高度查询</strong></p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.blockNumber</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBlockNumber(callback)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>gasPrice 查询</strong></p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.gasPrice</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getGasPrice(callback)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="交易相关"><a href="#交易相关" class="headerlink" title="交易相关"></a>交易相关</h2><ul>
<li><p>余额查询</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBalance(addressHexString [, defaultBlock])</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getBalance(addressHexString [, defaultBlock] [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>交易查询</p>
<ul>
<li>同步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransaction(transactionHash)</span><br></pre></td></tr></table></figure></li>
<li>异步<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransaction(transactionHash [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>交易收据查询</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransactionReceipt(hashString)</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.getTransactionReceipt(hashString [, callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>估计gas消耗量</p>
<ul>
<li><p>同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.estimateGas(callObject)</span><br></pre></td></tr></table></figure>
</li>
<li><p>异步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.estimateGas(callObject [, callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>发送交易</p>
<ul>
<li>from： 发送地址</li>
<li>to：接收地址</li>
<li>value：交易金额，以wei为单位，可选</li>
<li>gas：交易消耗gas上限，可选</li>
<li>gasPrice：交易gas单价，可选</li>
<li>data：交易携带的字串数据，可选</li>
<li>nonce：整数nonce值，可选<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.sendTransaction(transactionObject [,callback])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>发送交易过程如下：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/trans1.png" alt></p>
<h2 id="消息调用"><a href="#消息调用" class="headerlink" title="消息调用"></a>消息调用</h2><p>消息调用与交易的区别是，当我们想给合约发起调用（调用函数，这时我们使用的是call方法），给别人发币时使用sendTransaction()，而调用合约时其实也可以使用sendTransaction()，但一般我们将不需要提交交易的消息调用上，比如我们不做状态改变（纯计算、查询等）。若引发了状态改变则一定要用sendTransaction()。</p>
<p><strong>参数</strong>：</p>
<ul>
<li>调用对象：与交易对象相同，只是from也是可选的</li>
<li>默认区块：默认”latest“，可以传入指定的区块高度</li>
<li>回调函数：如果没有则为同步调用</li>
</ul>
<h2 id="日志过滤（事件监听）"><a href="#日志过滤（事件监听）" class="headerlink" title="日志过滤（事件监听）"></a>日志过滤（事件监听）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.filter(filterOptions [.callback])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>filterString可以是”latest”or”pending”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var filter &#x3D; web3.eth.filter(filterString);</span><br></pre></td></tr></table></figure>
</li>
<li><p>或者可以填入一个日志过滤options</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var filter &#x3D; web3.eth.filter(options);</span><br></pre></td></tr></table></figure>
</li>
<li><p>监听日志变化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filter.watch(options, function(error, result)&#123;</span><br><span class="line">	if (!error) console.log(result);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filter.stopWatching()</span><br></pre></td></tr></table></figure>
</li>
<li><p>还可以用传入回调函数的方法，立刻开始监听日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.filter(options, function(error, result)&#123;</span><br><span class="line">	if (!error) console.log(result);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/posts/Blockchain/filter.png" alt></p>
<h2 id="合约相关"><a href="#合约相关" class="headerlink" title="合约相关"></a>合约相关</h2><h3 id="创建合约"><a href="#创建合约" class="headerlink" title="创建合约"></a>创建合约</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">web3.eth.contract</span><br></pre></td></tr></table></figure>
<p>创建合约有两种方式，第一种方式为传入abi：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var MyContract &#x3D; web3.eth.contract(abiArray);</span><br></pre></td></tr></table></figure>
<p>这时候我们拿到了一个js中的合约实例，但还未和区块链关联起来，我们还需要通过地址初始化合约实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var contractinstance &#x3D; MyContract.at(address);</span><br></pre></td></tr></table></figure>
<p>或者是部署一个新合约：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var contractinstance &#x3D; MyContract.new([contructorParam1][contructorParam2],&#123;data:&quot;0x12345...&quot;,from:myAccount, gas&#125;)</span><br></pre></td></tr></table></figure>
<p>constructorParam1主要是传入需要赋的初值，data部分其实就是字节码。因此综上所述，在使用web3部署合约时需同时使用abi和字节码。接下来我们开始部署自己的合约：</p>
<p>合约如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;0.6.0;</span><br><span class="line"></span><br><span class="line">contract Coin&#123;</span><br><span class="line">    address public minter;</span><br><span class="line">    mapping(address&#x3D;&gt;uint) public balances;</span><br><span class="line">    event Sent(address from, address to, uint amount);</span><br><span class="line">    constructor()public&#123;</span><br><span class="line">        minter &#x3D; msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line">    function mint(address receiver, uint amount)public&#123;</span><br><span class="line">        require(msg.sender &#x3D;&#x3D; minter);</span><br><span class="line">        balances[receiver] +&#x3D; amount;</span><br><span class="line">    &#125;</span><br><span class="line">    function send(address receiver, uint amount)public&#123;</span><br><span class="line">        require(balances[msg.sender] &gt;&#x3D; amount);</span><br><span class="line">        balances[msg.sender] -&#x3D; amount;</span><br><span class="line">        balances[receiver] +&#x3D; amount;</span><br><span class="line">        </span><br><span class="line">        emit Sent(msg.sender, receiver, amount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract1.png" alt></p>
<p>我们在上面的步骤中，首先将前面已经得到的contract编译过后的abi传入：var coinContract = web3.eth.contract(abi)，然后获得字节码(byteCode)，连同其他参数一起new合约，于是就部署好了一个合约，这里需要注意的是，在传入字节码时需要在最前面加入’0x’，这是由于字节码本身就是16进制的，但编译出来的结果头部并没有带0x，需要手动去加。</p>
<p>我们看一看部署的合约长啥样：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract2.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract3.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/contract4.png" alt></p>
<p>以上是打印结果，然后我们可以通过coinContractInstance.address获得合约地址，同时也可以根据相应接口获得其他信息。</p>
<h3 id="调用合约"><a href="#调用合约" class="headerlink" title="调用合约"></a>调用合约</h3><p>可以通过已创建的合约实例，直接调用合约函数</p>
<ul>
<li><p>直接调用，自动按函数类型决定用sendTransaction还是call</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显式以消息调用形式call该函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod.call(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显式以发送交易形式调用该函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myContractInstance.myMethod.sendTransaction(param1 [,param2,...][,transactionObject][,defaultBlock][,callback]);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>最后的callback回调函数可选，若不选则为同步调用，但推荐使用异步调用方式。</p>
<p>我们还是来直接尝试调用一下合约：（由于testrpc不太方便，我在这里改用了ganache）</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin1.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin2.png" alt></p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin3.png" alt></p>
<p>需要注意的是，这里我们需要在mint的时候写上：{from:web3.eth.accounts[0]}，因为需要让合约知道调用合约的人是谁才能发币。</p>
<p>我们可以看到在ganache-cli中立刻出现了我们刚刚调用的合约情况：</p>
<p><img src="/Pic/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20%E2%80%94%E2%80%94%20web3js%E7%BC%96%E5%86%99%E4%BB%A5%E5%A4%AA%E5%9D%8A%E8%84%9A%E6%9C%AC/coin4.png" alt></p>
<h3 id="监听合约事件"><a href="#监听合约事件" class="headerlink" title="监听合约事件"></a>监听合约事件</h3><ul>
<li><p>合约的event类似于filter，可以设置过略选项来监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var event &#x3D; myContractInstance.MyEvent(&#123;valueA:23&#125; [,additionalFilterObject])</span><br></pre></td></tr></table></figure>
</li>
<li><p>监听事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">event.watch(function(err,, res)&#x3D;&gt;console.log(res))</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以使用传入回调函数的方法，立刻开始监听事件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var event &#x3D; myContractInstance.MyEvent(&#123;valueA:23&#125;</span><br><span class="line">			[,additionalFilterObject], function(err, res)&#123;</span><br><span class="line">          			if(!err) console.log(res);</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以通过以下方式触发监听：</p>
<p><img src alt></p>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（一） —— Solidity</title>
    <url>/posts/e008c9c3.html</url>
    <content><![CDATA[<h1 id="Solidity-源文件布局"><a href="#Solidity-源文件布局" class="headerlink" title="Solidity 源文件布局"></a>Solidity 源文件布局</h1><h2 id="pragm（版本杂注）"><a href="#pragm（版本杂注）" class="headerlink" title="pragm（版本杂注）"></a>pragm（版本杂注）</h2><p>源文件可以被版本杂著pragma所注解，表明要求的编译器版本，例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br></pre></td></tr></table></figure>
<p>在上述限定下，源文件将不允许低于0.4.0版本的编译器编译，也不允许高于0.5.0版本的编译器编译</p>
<h2 id="import"><a href="#import" class="headerlink" title="import"></a>import</h2><p>solidity支持使用import导入其他源文件（语法与JavaScript类似），主要有以下几种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// 从<span class="string">"filename"</span>中导入所有的全局符号到当前全局作用域中</span><br><span class="line"><span class="keyword">import</span> <span class="string">"filename"</span></span><br><span class="line"></span><br><span class="line">// 创建一个新的全局符号 symbolName，其成员均来自<span class="string">"filename"</span>中全局符号</span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> symbolName <span class="keyword">from</span> <span class="string">"filenam"</span></span><br><span class="line"></span><br><span class="line">// 创建新的全局符号  alias 和 symbol2，分别从 <span class="string">"filename"</span>引用 symbol1 和 symbol2</span><br><span class="line"><span class="keyword">import</span> &#123;symbol1 <span class="keyword">as</span> alias, symbol2&#125; <span class="keyword">from</span> <span class="string">"filename"</span></span><br><span class="line"></span><br><span class="line">// 这条语句等同于 <span class="keyword">import</span> * <span class="keyword">as</span> symbolName <span class="keyword">from</span> <span class="string">"filename"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"filename"</span> <span class="keyword">as</span> symbolName;</span><br></pre></td></tr></table></figure>
<h1 id="Solidity-值类型"><a href="#Solidity-值类型" class="headerlink" title="Solidity 值类型"></a>Solidity 值类型</h1><div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>布尔(bool)</td>
<td>可能的取值为字符串常量值true或false</td>
</tr>
<tr>
<td>整型(int/uint)</td>
<td>分别表示有符号和无符号的不同位数的整型变量；支持关键字uint8到uint256（无符号，从第8位到第256位）以及int8到int256，以8位为步长递增</td>
</tr>
<tr>
<td>定长浮点型(fixed / ufixed)</td>
<td>表示各种大小的有符号和无符号的定长浮点型；在关键字unfixedMxN和fixedMxN中，M表示该类型占用的位数，N表示可用的小数位数（fixed是ufixed128x19的别名）</td>
</tr>
<tr>
<td>地址(address)</td>
<td>存储一个20字节（160位）的值（以太坊地址大小）</td>
</tr>
<tr>
<td>定长字节数组</td>
<td>关键字有bytes1, bytes2, … ,bytes32</td>
</tr>
<tr>
<td>枚举(enum)</td>
<td>一种用户可以定义类型的方法，与C语言类似，默认从0开始递增，一般用来模拟合约的状态</td>
</tr>
<tr>
<td>函数(function)</td>
<td>一种表示函数的类型</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Solidity-引用类型"><a href="#Solidity-引用类型" class="headerlink" title="Solidity 引用类型"></a>Solidity 引用类型</h1><p>Solidity中较难理解的是引用类型，主要有三种：数组(Array)、结构(Struct)、映射(Mapping)</p>
<p><strong>数组(Array)</strong></p>
<ul>
<li>数组可以在声明时指定长度（定长数组），也可以动态调整大小（变长数组、动态数组）</li>
<li>对于存储型（storage）的数组 来说，元素类型可以是任意的（即元素也可以是数组类型、映射类型或者结构体）；对于内存型（memory）的数组来说，元素类型不能是映射（mapping）的类型</li>
</ul>
<p><strong>结构(Struct)</strong></p>
<ul>
<li>Solidit支持通过构造结构体定义新的类型（与C语言的类似）</li>
</ul>
<p><strong>映射(Mapping)</strong></p>
<ul>
<li>映射可以视作哈希表，在实际的初始化过程中创建的每个可能的key，并将其映射到字节形式全是零的值（类型默认值）</li>
</ul>
<h1 id="Solidity-地址类型"><a href="#Solidity-地址类型" class="headerlink" title="Solidity 地址类型"></a>Solidity 地址类型</h1><h2 id="版本更新后的地址类型"><a href="#版本更新后的地址类型" class="headerlink" title="版本更新后的地址类型"></a>版本更新后的地址类型</h2><p><strong>address</strong></p>
<ul>
<li>地址类型存储一个20字节的值（以太坊地址的大小）；地址类型也有成员变量，并作为所有合约的基础</li>
</ul>
<p><strong>address payable(v0.5.0引入)</strong></p>
<ul>
<li>与地址类型基本相同，不过多出了transfer和send两个成员变量</li>
</ul>
<p><strong>两者的转换和区别</strong></p>
<ul>
<li>payable地址是可以发送ether的地址，而普通address不能</li>
<li>允许从payable address到address的隐式转换，而反过来的转换是不可能的（唯一方法是通过uint160进行中间转换）</li>
<li>从0.5.0版本起，合约不再是从地址类型派生而来，但如果它有payable的回退函数，则同样可以显示转换为address或者address payable类型</li>
</ul>
<p>我们可以看到，Solidity目前的发展趋势是越来越严格限制对地址合约交易的运用，这正是出于安全性的考量。</p>
<h2 id="地址类型成员变量"><a href="#地址类型成员变量" class="headerlink" title="地址类型成员变量"></a>地址类型成员变量</h2><ul>
<li>获得该地址的ether余额，以Wei为单位：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address&gt;.balance(uint256)</span><br></pre></td></tr></table></figure>
<ul>
<li>向指定地址发送数量为amount的ether（以Wei为单位），失败时抛出异常，发送2300gas的矿工费，不可调节</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address payable&gt;.transfer(uint256 amount)</span><br></pre></td></tr></table></figure>
<ul>
<li>向指定地址发送数量为amount的ether（以Wei为单位），失败时返回false，发送2300gas的矿工费，不可调节(默认是执行成功的，更推荐使用transfer)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address payable&gt;.send(uint256 amount) returns(bool)</span><br></pre></td></tr></table></figure>
<ul>
<li>发出底层函数CALL，失败时返回 false，发送所有可用gas，可调节（注意：如果CALL了别人的函数，这一段逻辑的控制权全部放在这个函数中，你不知道这个函数会做什么事情，可能把你的逻辑全部搞乱。。谨慎使用）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address&gt;.call(bytes memory) returns(bool, bytes memory)</span><br></pre></td></tr></table></figure>
<ul>
<li>发出底层函数DELEGATECALL，失败时返回 false，发送所有可用gas，可调节</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address&gt;.delegatecall(bytes memory) returns(bool, bytes memory)</span><br></pre></td></tr></table></figure>
<ul>
<li>发出底层函数STATICCALL，失败时返回 false，发送所有可用gas，可调节</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;address&gt;.staticcall(bytes memory) returns(bool, bytes memory)</span><br></pre></td></tr></table></figure>
<h2 id="地址成员变量用法"><a href="#地址成员变量用法" class="headerlink" title="地址成员变量用法"></a>地址成员变量用法</h2><ul>
<li>balance和transfer<ul>
<li>可以使用balance属性来查询一个地址的余额，可以使用transfer函数像一个payable地址发送以太币（Ether），以Wei为单位：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">address payable x = address(<span class="number">0x123</span>)</span><br><span class="line">address myAddress = address(this)</span><br><span class="line"><span class="keyword">if</span>(x.balance &lt; <span class="number">10</span> &amp;&amp; myAddress.balance &gt;= <span class="number">10</span>)</span><br><span class="line">	x.transfer(<span class="number">10</span>);   //给x发<span class="number">10</span>Wei</span><br></pre></td></tr></table></figure>
<ul>
<li><p>send</p>
<ul>
<li>send事transfer的低级版本，如果执行失败，当前的合约不会因为异常而终止，但send会返回false</li>
</ul>
</li>
<li><p>call</p>
<ul>
<li>也可以用call来实现转币的操作，通过添加.gas()和.value()修饰器（很底层）</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nameReg.call.gas(<span class="number">1000000</span>).value(<span class="number">1</span> ether)(abi.encodeWithSignature(<span class="string">"refister(string)"</span>, <span class="string">"MyName"</span>));</span><br></pre></td></tr></table></figure>
<h1 id="类型详解"><a href="#类型详解" class="headerlink" title="类型详解"></a>类型详解</h1><h2 id="字符数组-Bytes-Arrays"><a href="#字符数组-Bytes-Arrays" class="headerlink" title="字符数组(Bytes Arrays)"></a>字符数组(Bytes Arrays)</h2><p><strong>定长字符数组</strong></p>
<ul>
<li>属于值类型，bytes1,bytes2,…,bytes32分别代表了长度为1到32的字节序列</li>
<li>有一个.length属性，返回数组长度（只读）</li>
</ul>
<p><strong>变长字符数组</strong></p>
<ul>
<li>属于引用类型，包括bytes和string，不同的事bytes事Hex字符串，而string是UTF-8编码的字符串</li>
</ul>
<h2 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h2><ul>
<li>枚举类型用于用户自定义一组常量值</li>
<li>与C语言的枚举类型非常相似，对应整型值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;=<span class="number">0.4</span><span class="number">.0</span> &lt;<span class="number">0.6</span><span class="number">.0</span>;</span><br><span class="line">contract Purchase&#123;</span><br><span class="line">	enum State&#123;Create, Locked, Inactive&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数组-Array"><a href="#数组-Array" class="headerlink" title="数组(Array)"></a>数组(Array)</h2><ul>
<li>固定大小k和元素类型T的数组被写为T[k]，动态大小的数组为T[]。例如，一个由5个uint的动态数组组成的数组是uint[][5]（定义的方式与C相反）</li>
<li>要访问第三个动态数组中的第二个uint，可以使用x[2][1]</li>
<li>越界访问数组，会导致调用失败回退</li>
<li>如果要添加新元素，则必须使用.push()或将.length增大</li>
<li>变长的storage数组和bytes(不包括string)有一个push()方法。可以将一个新元素附加到数组末端，返回值为当前长度</li>
</ul>
<p><strong>数组实例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;=<span class="number">0.4</span><span class="number">.16</span> &lt;<span class="number">0.6</span><span class="number">.0</span>;</span><br><span class="line">contract&#123;</span><br><span class="line">	function f(uint len) public pure&#123;</span><br><span class="line">    	// 给a分配了<span class="number">7</span>个对应的存储空间</span><br><span class="line">    	uint[] memory a = new uint[](<span class="number">7</span>);</span><br><span class="line">        // 动态大小数组</span><br><span class="line">        bytes memory b = new bytes(len);</span><br><span class="line">        <span class="keyword">assert</span>(a.length == <span class="number">7</span>);</span><br><span class="line">        <span class="keyword">assert</span>(b.length == len);</span><br><span class="line">        a[<span class="number">6</span>] = <span class="number">8</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="结构-Struct"><a href="#结构-Struct" class="headerlink" title="结构(Struct)"></a>结构(Struct)</h2><ul>
<li>结构类型可以在映射和数组中使用，它们本身可以包含映射和数组</li>
<li>结构不能包含自己类型的成员，但可以作为自己数组的成员，也可以作为自己映射成员的值类型</li>
</ul>
<p><strong>结构实例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;=<span class="number">0.4</span><span class="number">.0</span> &lt;<span class="number">0.6</span><span class="number">.0</span>;</span><br><span class="line">contract Ballot&#123;</span><br><span class="line">	struct Voter &#123;</span><br><span class="line">    	uint weight;</span><br><span class="line">        bool voted;</span><br><span class="line">        uint vote;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="映射-Mapping"><a href="#映射-Mapping" class="headerlink" title="映射(Mapping)"></a>映射(Mapping)</h2><ul>
<li>声明一个映射：mapping(_KeyType =&gt; _ValueType)</li>
<li>_KeyType可以是任何基本类型，这意味着它可以是任何内置值类型加上字节和字符串，不允许使用用户自定义的或是更复杂的类型，如枚举，映射，结构以及除了bytes和string之外的所有数组类型</li>
<li>_ValueType可以是任何类型，包括映射</li>
</ul>
<p><strong>映射实例</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;=<span class="number">0.4</span><span class="number">.0</span> &lt;<span class="number">0.6</span><span class="number">.0</span>;</span><br><span class="line">contract MappingExample &#123;</span><br><span class="line">	mapping(address =&gt;uint) public balances;</span><br><span class="line">    function update(uint newBalance) public &#123;</span><br><span class="line">    	balances[msg.sender] = newBalance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">contract  MappingUser&#123;</span><br><span class="line">	function f() public returns (uint) &#123;</span><br><span class="line">    	MappingExample m = new MappingExample();</span><br><span class="line">        m.update(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">return</span> m.balances(address(this));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Solidity-数据存储位置"><a href="#Solidity-数据存储位置" class="headerlink" title="Solidity 数据存储位置"></a>Solidity 数据存储位置</h1><ul>
<li>所有的复杂类型，即数组、结构和映射类型，都有一个额外属性——”数据位置”，用来说明数据时 保存在内存(memory)中还是存储(storagez)中</li>
<li>根据上下文不同，大多数时候数据有默认的位置，但也可以通过在类型名后增加关键字 storage或memory进行修改</li>
<li>函数参数(包括返回的参数)的数据位置默认是memory，局部变量的数据位置默认是storage，状态变量的数据位置强制是storage</li>
<li>另外还存在第三种 数据位置，calldata，这是一块只读的且不会永久存储的位置，用来存储函数参数，外部函数的参数（非返回参数)的数据位置被强制指定为calldata，效果跟memory差不多</li>
</ul>
<p><strong>总结</strong>：</p>
<ul>
<li>强制指定地数据位置<ul>
<li>外部函数的参数（不包括返回参数）：calldata</li>
<li>状态变量：starage</li>
</ul>
</li>
<li>默认数据位置<ul>
<li>函数参数（包括返回参数）：memory</li>
<li>引用类型地局部变量：storage</li>
<li>值类型地局部变量：栈（stack）</li>
</ul>
</li>
</ul>
<p><strong>特别要求</strong>：</p>
<ul>
<li>公开可见（public visible）的函数参数一定是memory类型，如果要求是storage类型，则必须是private或者internal参数，这是为了防止随意的公开调用占用资源</li>
</ul>
<p><strong>数据存储实例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract C&#123;</span><br><span class="line">	uint[] data1;</span><br><span class="line">    uint[] data2;</span><br><span class="line">    function appendOne() public &#123;</span><br><span class="line">    	append(data1);</span><br><span class="line">    &#125;</span><br><span class="line">    function appendTwo() public &#123;</span><br><span class="line">    	append(data2);</span><br><span class="line">    &#125;</span><br><span class="line">    function append(uint[] storage d) internal &#123;</span><br><span class="line">    	d.push(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="改错题"><a href="#改错题" class="headerlink" title="改错题"></a>改错题</h1><h2 id="T1"><a href="#T1" class="headerlink" title="#T1"></a>#T1</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract C&#123;</span><br><span class="line">	uint someVariable;</span><br><span class="line">    uint[] data;</span><br><span class="line">    function f() public&#123;</span><br><span class="line">    	uint[] x;</span><br><span class="line">        x.push(<span class="number">2</span>);</span><br><span class="line">        data = x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>someVariable在这段代码里面会成为一个计数器，每一次调用f()都会使之递增，这是由于Solidity语法中，未指定的x在这里被设定为一个storage类型的指针(storage pointer)，由于所有状态变量和局部变量的默认存储位置都是在storage中，因此声明了一个可变长度的数组x又未给它赋值，因此在存储空间中它是一个没有分配存储空间的指针，最开始一轮a和b都赋初值0，而x会指向合约定义的整个存储空间的零位置（最开始的地方），也就是a的位置，因此后面在调用x之后a会随之变化。</p>
<p>但是为什么x变化2，而a只变化了1呢？因为我们定义的x这个长度可变的数组在顺序存储中会存储它的长度，找这个变量的时候就会直接找到它的长度，我们想要找到它的元素就会通过它元素的索引值加上本身的位置共同计算出一个哈希，哈希的位置就是元素对应的位置。</p>
<p>因此正确方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract C&#123;</span><br><span class="line">	uint someVariable;</span><br><span class="line">    uint[] data;</span><br><span class="line">    function f() public&#123;</span><br><span class="line">    	uint[] x = data;</span><br><span class="line">        x.push(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="T2"><a href="#T2" class="headerlink" title="#T2"></a>#T2</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract C&#123;</span><br><span class="line">	uint[] x;</span><br><span class="line">    function f(uint[] memoryArray) public &#123;</span><br><span class="line">    	x = memoryArray;</span><br><span class="line">        uint[] y = x;</span><br><span class="line">        y[<span class="number">7</span>];</span><br><span class="line">        y.length = <span class="number">2</span>;</span><br><span class="line">        delete x;</span><br><span class="line">        y = memoryArray;</span><br><span class="line">        delete y;</span><br><span class="line">        g(x);</span><br><span class="line">        h(x);</span><br><span class="line">    &#125;</span><br><span class="line">    function g(uint[] storage storageArray) internal &#123;&#125;</span><br><span class="line">    function h(uint[] memoryArray) public &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面这段代码在y = memoryArray处会报错：Type uint256[] memory is not implicitly convertible to expected type uint256[] storage pointer.显见，memoryArray为一个memory类型的变长数组，而要将之赋值给y，由于y是一个storage类型的指针，不会发生拷贝，产生错误。（两块空间的地址意义不同）</p>
<p>因此正确方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract C&#123;</span><br><span class="line">	uint[] x;</span><br><span class="line">    function f(uint[] memoryArray) public &#123;</span><br><span class="line">    	x = memoryArray;</span><br><span class="line">        uint[] y = x;</span><br><span class="line">        y[<span class="number">7</span>];</span><br><span class="line">        y.length = <span class="number">2</span>;</span><br><span class="line">        delete x;</span><br><span class="line">        uint[]memory z = memoryArray;</span><br><span class="line">        delete z;</span><br><span class="line">        g(x);</span><br><span class="line">        h(x);</span><br><span class="line">    &#125;</span><br><span class="line">    function g(uint[] storage storageArray) internal &#123;&#125;</span><br><span class="line">    function h(uint[] memoryArray) public &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="T3-猜数字游戏"><a href="#T3-猜数字游戏" class="headerlink" title="#T3 猜数字游戏"></a>#T3 猜数字游戏</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;<span class="number">0.4</span><span class="number">.22</span>;</span><br><span class="line">contract Honeypot &#123;</span><br><span class="line">	uint luckyNum = <span class="number">52</span>;</span><br><span class="line">    uint public last;</span><br><span class="line">    struct Guess&#123;</span><br><span class="line">    	address player;</span><br><span class="line">        uint number;</span><br><span class="line">    &#125;</span><br><span class="line">    Guess[] public guessHistory;</span><br><span class="line">    address owner = msg.sender;</span><br><span class="line">    function guess(uint_num) public payable&#123;</span><br><span class="line">    	Guess newGuess;</span><br><span class="line">        newGuess.player = msg.sender;</span><br><span class="line">        newGuess.number = _num;</span><br><span class="line">        guessHistory.push(newGuess);</span><br><span class="line">        <span class="keyword">if</span>(_num == luckyNum)</span><br><span class="line">        	msg.sender.transfer(msg.value*<span class="number">2</span>);</span><br><span class="line">        last = now;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当传入guess的参数52时，发现账户余额净少十个，并没有出现返还的两倍奖励。首先Guess newGuess和T1一样，被声明为一个storage类型的指针，也就是最初会直接指向luckyNum，并将之修改。这是Solidity上的一个钓鱼合约（蜜罐合约），发多少以太币丢多少以太币。另外事实上若将Guess中新得到的luckyNumber输进去是可以成功的，可以将币提出来，但实际上合约创作者不会愚蠢地把luckyNum设置为public。</p>
<p>我们想一下，若将Guess定义中的player和number互换一下次序会发生什么：由于Guess会首先传入调用者写入的内容，也就是number，因此调用者无论输入了 什么数字最终都会显示猜测正确，于是给调用者转钱。</p>
<p>不过上述问题在0.5.0版本之后已被更正，但以太坊上仍有不少钓鱼合约需要我们注意。</p>
<h1 id="函数详解"><a href="#函数详解" class="headerlink" title="函数详解"></a>函数详解</h1><h2 id="函数声明和类型"><a href="#函数声明和类型" class="headerlink" title="函数声明和类型"></a>函数声明和类型</h2><p>在下面的函数声明中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">function getBrand() public view returns (string)&#123;</span><br><span class="line">	<span class="keyword">return</span> brand;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getBrand()时函数名称，public view是函数类型,string是返回类型。</p>
<ul>
<li>函数的值类型有两类：内部函数和外部函数</li>
<li>内部函数只能在当前合约内被调用（更具体来说，在当前代码块内，包括内部库函数和继承的函数中），因为它们不能再当前合约上下文的外部被执行。调用一个内部函数是通过跳转到它的入口标签来实现的，就像在当前合约的内部 调用一个函数</li>
<li>外部函数由一个地址和一个函数签名组长城，可以通过外部函数调用传递或者返回</li>
<li>调用内部函数：直接使用名字f</li>
<li>调用外部函数: this.f(当前合约)，a.f(外部合约)</li>
</ul>
<h2 id="函数可见性"><a href="#函数可见性" class="headerlink" title="函数可见性"></a>函数可见性</h2><p>函数的可见性可以指定为external,public,internal或者private；对于状态变量，不能设置为external，默认是internal。</p>
<ul>
<li>external:外部函数作为合约接口的一部分，意味着我们可以从其他他合约和交易中调用。一个挖补函数f不能从内部调用（即f不起作用，但this.f()可以）。当收到大量数据时，外部函数有时会更有效率。</li>
<li>public：public函数是合约接口的一部分，可以在内部或通过消息调用。对于public状态变量，会自动生成一个getter函数。</li>
<li>internal：这些函数和状态变量只能 是内部访问（即从当前合约内部或从它的派生的合约访问），不能用this调用。</li>
<li>private：private函数和状态变量仅在当前按定义 它们的合约中使用，并且不能被派生合约使用。</li>
</ul>
<p>我们在将一个状态变量设置为public时，实际上就是生成一个getter函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">function a() public view returns(uint)&#123;</span><br><span class="line">	<span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="函数状态可变性"><a href="#函数状态可变性" class="headerlink" title="函数状态可变性"></a>函数状态可变性</h2><ul>
<li>pure:纯函数，不允许修改或访问状态</li>
<li>view:不允许修改状态</li>
<li>payable:允许从消息调用中接收以太币</li>
<li>constant:与view相同，一般只修饰状态变量，不允许赋值（只能初始化）</li>
<li>以下情况被认为是修改状态：<ul>
<li>修改状态变量</li>
<li>产生事件</li>
<li>创建其他合约</li>
<li>使用selfdestruct</li>
<li>通过调用发送以太币</li>
<li>通过任何没有标记为view或者pure的函数</li>
<li>使用低级调用（CALL等）</li>
<li>使用包含特定操作码的内联汇编</li>
</ul>
</li>
<li>以下被认为是读取状态：<ul>
<li>读取状态变量</li>
<li>访问this.balance胡总和<address>.balance</address></li>
<li>访问block,tx,msg中任意成员（除msg.sig和msg.data之外）</li>
<li>调用任何未标记为pure的函数</li>
<li>使用包含某些操作码的内联汇编</li>
</ul>
</li>
</ul>
<h2 id="函数修饰器-modifier"><a href="#函数修饰器-modifier" class="headerlink" title="函数修饰器(modifier)"></a>函数修饰器(modifier)</h2><ul>
<li>使用修饰器modifier可以轻松改变函数的行为，例如：它们可以在执行函数之前自动检查某个条件。修饰器modifier是合约的可继承属性，并可能被派生合约覆盖</li>
<li>如果同一个函数有多个修饰器modifier，它们之间以空格隔开，修饰器modifier会依次检查执行</li>
</ul>
<p><strong>modifier实例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt;=<span class="number">0.4</span><span class="number">.22</span> &lt;<span class="number">0.6</span><span class="number">.0</span>;</span><br><span class="line">contract Purchase &#123;</span><br><span class="line">	address public seller;</span><br><span class="line">    modifier onlySeller()&#123;</span><br><span class="line">    	requier(msg.sender == sender, <span class="string">"Only seller can sell."</span>);</span><br><span class="line">        _; // 占位符:原本函数的代码在此执行</span><br><span class="line">    &#125;</span><br><span class="line">    function abort public view onlySeller returns(uint)&#123;</span><br><span class="line">    	// Modifier usage</span><br><span class="line">    	<span class="keyword">return</span> <span class="number">200</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当换一个账户调用f时会报错，判断了require。同理函数修饰器也可以改变下划线 （占位符的位置），改变调用次序。</p>
<h2 id="回退函数-fallback"><a href="#回退函数-fallback" class="headerlink" title="回退函数(fallback)"></a>回退函数(fallback)</h2><ul>
<li>回退函数(fallback function)是合约中的特殊函数，没有名字，不能有参数也不能有返回值</li>
<li>如果在一个合约的调用中，没有其他函数与给定的函数标识符匹配（或没有提供调用数据），那么这个函数（fallback）函数会被执行</li>
<li>每当合约收到以太币（没有任何数据），回退函数就会执行。此外，为了接收以太币，fallback必须标记为payable。如果不存在这样的函数，则合约不能通过常规交易接收以太币</li>
<li>在上下文中通常只有很少的gas可以用来 完成回退函数的调用，所以使用fallback函数的调用尽量廉价很重要</li>
</ul>
<p>回退函数中如果有人加入恶意代码，比如获取地址，比如重复调用合约，就会产生一个比较大的安全隐患。这种现象一般会发生在发币的合约中，恶意账户在回退函数中加入能重复调用合约的代码，从而达成不断获得币的目的。以太坊历史上最大的一次攻击——The Dao，正是由于以太坊在合约中给人转币，调用了transfer方法，但没有判定地址到底是什么样的地址，结果就是黑客写了份合约，注册了The Dao帐号之后触发了函数Transfer往自己合约中转以太，并且调用到了自己写好的回退函数，当然还使用到了其他漏洞大家可以自行了解。</p>
<h1 id="事件"><a href="#事件" class="headerlink" title="事件"></a>事件</h1><ul>
<li>事件是以太坊EVM提供的一种日志基础设施，事件可以用来做操作记录，存储为日志。也可以用来实现一些交互功能，比如同值UI，返回函数调用结果等</li>
<li>当定义的事件触发时，我们可以将时间存储到EVM的交易日志中，日志是区块链中的一种特殊的数据结构，日志与合约关联，与合约的存储和并存入区块链中；只要某个区块可以访问，其相关的日志就可以访问；但在合约中我们不能直接访问日志和事件数据</li>
<li>可以通过日志实现简单支付验证SPV(Simpllified Payment Verification)，如果一个外部实体提供了一个带有这种证明的合约，它可以检查日志是否真实存在于区块链中</li>
</ul>
<h1 id="Solidity异常处理"><a href="#Solidity异常处理" class="headerlink" title="Solidity异常处理"></a>Solidity异常处理</h1><ul>
<li>Solidity使用“状态恢复异常”来处理异常。这样的异常将撤销 对当前调用（及其所有子调用）中的状态 所做的所有更改，并且向调用者返回错误</li>
<li>函数assert和require可用于判断条件，并在不满足条件时抛出异常</li>
<li>assert()一般只应用于测试内部错误，并检查常量</li>
<li>require()应用于确保 满足有效条件（如输入或合约状态变量），或验证调用外部合约的返回值</li>
<li>revert()用于抛出异常，它可以标记一个错误并返回当前调用回退（检查外部）</li>
</ul>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>CNN 卷积神经网络</title>
    <url>/posts/d7d85f1a.html</url>
    <content><![CDATA[<h1 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h1><h2 id="卷积神经网络主要层次"><a href="#卷积神经网络主要层次" class="headerlink" title="卷积神经网络主要层次"></a>卷积神经网络主要层次</h2><ul>
<li>数据输入层：Input layer</li>
<li>卷积计算层：CONV layer</li>
<li>ReLU激活层：ReLU layer</li>
<li>池化层：Pooling layer</li>
<li>全连接层：FC layer</li>
</ul>
<p><strong>激活层建议：</strong></p>
<ul>
<li>CNN尽量不要使用Sigmoid，如果要使用，建议只在全连接层使用</li>
<li>首先使用ReLU，因为迭代速度快，但有可能效果不佳</li>
<li>如果使用ReLU失效，考虑使用Leaky ReLU或者Maxout，一般都能解决的</li>
<li>tanh激活函数在某些情况下有比较好的效果，但是应用场景比较少</li>
</ul>
<p><strong>池化层：</strong> 在池化层中，进行压缩减小特征数量时常用两种策略：</p>
<ul>
<li>Max Pooling：最大池化，一般采用这种方式</li>
<li>Average Pooling：平均池化</li>
</ul>
<p><strong>全连接层：</strong> 两层之间所有神经元都有权重连接；通常情况下，在CNN中，FC层只有在尾部出现</p>
<p><strong>CNN结构</strong>： 一般为：</p>
<ul>
<li>Input</li>
<li>[[CONV ——&gt; ReLU] <em> N ——&gt;Pooling?] </em> M</li>
<li>[FC——&gt;ReLU] * K</li>
<li>FC</li>
</ul>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p><strong>与神经网络/机器学习一样，需要对输入的数据进行预处理操作，主要原因是：</strong></p>
<ul>
<li>输入数据单位不一样，可能会导致神经网络收敛速度慢，训练时间长</li>
<li>数据范围大的输入在模式分类中的作用可能偏大，而数据范围小的作用可能就偏小</li>
<li>由于神经网络中存在的激活函数是由值域限制的，因此需要将网络训练的目标数据映射到激活函数的值域</li>
<li>Sigmoid型激活函数在（0，1）区间以外区域很平缓，区分度太小</li>
</ul>
<p><strong>常见的3种数据预处理方式：</strong></p>
<ul>
<li>去均值<ul>
<li>将输入数据的各个维度中心化到0</li>
</ul>
</li>
<li>归一化<ul>
<li>将输入数据的各个维度的幅度归一化到同样的范围</li>
</ul>
</li>
<li>PCA/白化<ul>
<li>PCA降维</li>
<li>白化是对数据的每个特征轴上的幅度归一化</li>
</ul>
</li>
</ul>
<p><strong>白化使得学习算法的输入具有如下性质：</strong></p>
<ul>
<li>特征之间相关性较低</li>
<li>所有特征具有相同的方差</li>
</ul>
<h1 id="卷积神经网络优缺点"><a href="#卷积神经网络优缺点" class="headerlink" title="卷积神经网络优缺点"></a>卷积神经网络优缺点</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>共享卷积核（共享参数），对高维数据的处理没有压力</li>
<li>无需选择特征属性，只要训练好权重，即可得到特征值</li>
<li>深层次的网络抽取图像信息比较丰富，表达效果好</li>
</ul>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>需要调参，需要大量样本，训练迭代次数比较多，最好使用GPU训练</li>
<li>物理含义不明确，从每层输出中很难看出含义</li>
</ul>
<h1 id="卷积神经网络正则化和Dropout"><a href="#卷积神经网络正则化和Dropout" class="headerlink" title="卷积神经网络正则化和Dropout"></a>卷积神经网络正则化和Dropout</h1><p>神经网络的学习能力受神经元数目以及神经网络层次的影响，神经元数目越大，神经网络层次越高，那么神经网络的学习能力越强，那么就有可能出现过拟合的问题。正则化通过给cost函数添加正则项的方式来解决overfitting，Dropout是通过直接修改神经网络的结构来解决overfitting。</p>
<h2 id="Regulation"><a href="#Regulation" class="headerlink" title="Regulation"></a>Regulation</h2><p>正则化的目的是降低模型复杂度，通过在cost函数上添加一个正则项的方式来降低overfitting，主要有L1和L2两种方式</p>
<ul>
<li>L1：若希望知道哪个特征对最后结果产生了比较大的影响则使用之</li>
<li>L2：不在意对特征的分析</li>
</ul>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>通过随即删除神经网络中的神经元来解决overfitting的问题，在每次迭代的时候，只使用部分神经元训练模型获取W和d的值，其目的是：</p>
<ul>
<li>不要CNN具有太多的泛化能力（不能依赖某几个神经元）</li>
<li>多次迭代结果的合并可以增加模型的准确，多个不同模型的合并可以提高其准确率</li>
</ul>
<p>一般情况下，对于同一组训练数据，利用不同的神经网络训练后，求其输出的平均值可以减少overfitting，Dropout就是利用这个原理，每次丢掉一半左右的隐藏神经元，相当于在不同的神经网络上训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加robost的特征，另外Dropout不仅减少overfitting，还能提高准确率</p>
<h1 id="实现卷积神经网络"><a href="#实现卷积神经网络" class="headerlink" title="实现卷积神经网络"></a>实现卷积神经网络</h1><p>参数表：</p>
<ul>
<li>W1：是Layer1到Layer2的权重</li>
<li>b1：是Layer1到Layer2转换的截距</li>
<li>W2：是Layer2到Layer3的权重</li>
<li>b2：是Layer2到Layer3转换的截距</li>
<li>W3：是Layer3到Layer4的权重</li>
<li>b3：是Layer3到Layer4转换的截距</li>
<li>Dropout的概率为 p=0.5</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">p = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">    U1 = (np.random.rand(*H1.shape) &lt; p) / p</span><br><span class="line">    H1 *= U1</span><br><span class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">    U1 = (np.random.rand(*H2.shape) &lt; p) / p</span><br><span class="line">    H2 *= U2</span><br><span class="line">    out = np.dot(W3, H2) + b3</span><br><span class="line">    <span class="comment"># BP操作，计算梯度（省略）</span></span><br><span class="line">    <span class="comment"># 参数更新（省略）</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">    out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure>
<h1 id="卷积神经网络典型CNN"><a href="#卷积神经网络典型CNN" class="headerlink" title="卷积神经网络典型CNN"></a>卷积神经网络典型CNN</h1><ul>
<li>LeNet：最早用于数字识别的CNN，主要用于字符识别<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597059891678&amp;di=d80a92349c0e954b2a286ff74993bb1b&amp;imgtype=0&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180514%2Ff9524f078368481f8366c434da8d0e1a.png" alt></li>
<li>AlexNet：2012年ILSVRC比赛冠军，远超第二名的CNN，比LeNet更深，用多层小卷积叠加来替换单个的大卷积<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597060606682&amp;di=5dae3114e300abb27108fdc2b743c09c&amp;imgtype=0&amp;src=http%3A%2F%2Fimg2018.cnblogs.com%2Fblog%2F439761%2F201901%2F439761-20190129114344192-623663293.jpg" alt></li>
<li>ZF Net：2013 ILSVRC冠军<ul>
<li>基于AlexNet进行微调</li>
<li>Top5错误率11.2%</li>
<li>使用ReLU激活函数和交叉熵损失函数</li>
</ul>
</li>
<li>GoogleNet：2014 ILSVRC冠军(层次很深)<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597060786492&amp;di=e0bfdd91ac30a5d0061e30084be5190f&amp;imgtype=0&amp;src=http%3A%2F%2Fimage.bubuko.com%2Finfo%2F201803%2F20180307165329526469.png" alt><br><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=220584882,329355430&amp;fm=26&amp;gp=0.jpg" alt></li>
<li>VGGNet：2014ILSVRC比赛中算法模型，效果略低于GoogleNet<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597060916889&amp;di=4d3c057e379782ec0312bee9ea748f30&amp;imgtype=0&amp;src=http%3A%2F%2Flanbing510.info%2Fpublic%2Fimg%2Fposts%2Fvggnet.png" alt></li>
<li>ResNet：2015ILSVRC冠军，结构修正以适应更深层次的CNN训练(残差连接)<ul>
<li>允许模型存在一些shortcuts，可以让研究者成功地训练更深的神经网络，这样也能明显地优化Inception块<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597061053081&amp;di=9230f77241bff311478913d33f72e0ef&amp;imgtype=0&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20170901%2F22800c4024924f8a946ed8a7bc4d1d76.jpeg" alt></li>
</ul>
</li>
</ul>
<h1 id="卷积神经网络案例"><a href="#卷积神经网络案例" class="headerlink" title="卷积神经网络案例"></a>卷积神经网络案例</h1><p>任务：CNN卷积神经网络手写数字</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">X_holder = tf.placeholder(tf.float32)</span><br><span class="line">y_holder = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure>
<pre><code>Extracting MNIST_data\train-images-idx3-ubyte.gz
Extracting MNIST_data\train-labels-idx1-ubyte.gz
Extracting MNIST_data\t10k-images-idx3-ubyte.gz
Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_images = tf.reshape(X_holder, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment">#convolutional layer 1</span></span><br><span class="line">conv1_Weights = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">conv1_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">32</span>]))</span><br><span class="line">conv1_conv2d = tf.nn.conv2d(X_images, conv1_Weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) + conv1_biases</span><br><span class="line">conv1_activated = tf.nn.relu(conv1_conv2d)</span><br><span class="line">conv1_pooled = tf.nn.max_pool(conv1_activated, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment">#convolutional layer 2</span></span><br><span class="line">conv2_Weights = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">conv2_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv2_conv2d = tf.nn.conv2d(conv1_pooled, conv2_Weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) + conv2_biases</span><br><span class="line">conv2_activated = tf.nn.relu(conv2_conv2d)</span><br><span class="line">conv2_pooled = tf.nn.max_pool(conv2_activated, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment">#full connected layer 1</span></span><br><span class="line">connect1_flat = tf.reshape(conv2_pooled, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">connect1_Weights = tf.Variable(tf.truncated_normal([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">connect1_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">1024</span>]))</span><br><span class="line">connect1_Wx_plus_b = tf.add(tf.matmul(connect1_flat, connect1_Weights), connect1_biases)</span><br><span class="line">connect1_activated = tf.nn.relu(connect1_Wx_plus_b)</span><br><span class="line"><span class="comment">#full connected layer 2</span></span><br><span class="line">connect2_Weights = tf.Variable(tf.truncated_normal([<span class="number">1024</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">connect2_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">10</span>]))</span><br><span class="line">connect2_Wx_plus_b = tf.add(tf.matmul(connect1_activated, connect2_Weights), connect2_biases)</span><br><span class="line">predict_y = tf.nn.softmax(connect2_Wx_plus_b)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#loss and train</span></span><br><span class="line">loss = tf.reduce_mean(-tf.reduce_sum(y_holder * tf.log(predict_y), <span class="number">1</span>))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">0.0001</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1001</span>):</span><br><span class="line">    train_images, train_labels = mnist.train.next_batch(<span class="number">200</span>)</span><br><span class="line">    session.run(train, feed_dict=&#123;X_holder:train_images, y_holder:train_labels&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(predict_y, <span class="number">1</span>), tf.argmax(y_holder, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">        test_images, test_labels = mnist.test.next_batch(<span class="number">2000</span>)</span><br><span class="line">        train_accuracy = session.run(accuracy, feed_dict=&#123;X_holder:train_images, y_holder:train_labels&#125;)</span><br><span class="line">        test_accuracy = session.run(accuracy, feed_dict=&#123;X_holder:test_images, y_holder:test_labels&#125;)</span><br><span class="line">        print(<span class="string">'step:%d train accuracy:%.4f test accuracy:%.4f'</span> %(i, train_accuracy, test_accuracy))</span><br></pre></td></tr></table></figure>
<pre><code>step:0 train accuracy:0.0700 test accuracy:0.0685
step:100 train accuracy:0.8650 test accuracy:0.8935
step:200 train accuracy:0.9200 test accuracy:0.9255
step:300 train accuracy:0.9500 test accuracy:0.9460
step:400 train accuracy:0.9600 test accuracy:0.9595
step:500 train accuracy:0.9850 test accuracy:0.9715
step:600 train accuracy:0.9650 test accuracy:0.9680
step:700 train accuracy:0.9700 test accuracy:0.9690
step:800 train accuracy:0.9600 test accuracy:0.9760
step:900 train accuracy:0.9800 test accuracy:0.9765
step:1000 train accuracy:0.9850 test accuracy:0.9760
</code></pre>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>Microsoft Recommenders应用</title>
    <url>/posts/7f98fd00.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="d1767b88449b7ea88e4bf79176c25139f77a42b6bfa61b40f7c875b7af78e8ca">ca92632ce0d9be1239d2a7e448117bc3e342ef55d0462f2c82d877213d699b61722d47f6d828ee39ba60a66d72be00104245110fdd91568f88499c116b786fe5f8726941696e3d78b6891f7e1850c68673969e5018e5074dc25b73c1f2ceaeba4ed333e9959540ffe2d65c5b44930e5a0ff6fa7b018bbcb49a179256ff060eaf15427fa22653bbb56854f71ae2565280ae269cef465be412e3df8911b209c685e5fbe057b8f009fe0f3ad4457c1ea942e7e534588cd1cf7567e2632f85c64ee0</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Recommender system</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（十）—— 程序化交易实战问题及对策</title>
    <url>/posts/b7b393e5.html</url>
    <content><![CDATA[<h1 id="实盘与回测结果的偏离"><a href="#实盘与回测结果的偏离" class="headerlink" title="实盘与回测结果的偏离"></a>实盘与回测结果的偏离</h1><p>我们在网络上常见到各种几百块钱就能买到一个回测很漂亮的策略，激动地准备实现财富自由。但开始真金白银地去交易时，就不是那么回事儿了。其实用脚指头想一想，有这么好的策略为什么卖家不自己用，反而只卖几百块？</p>
<p>容易想到的问题就是：</p>
<ol>
<li>指数与合约价格的偏离<ul>
<li>指数是各个合约加权平均产生的结果，指数显得更光滑，是加权平均数，噪音比较小，常会好于合约价格</li>
<li>实际交易时只能用 合约不能用指数</li>
<li>减小指数与合约间的误差的方法：用合约配比来模拟指数</li>
</ul>
</li>
<li>低估交易成本（滑点、佣金、手续费等）<ul>
<li>滑点影响最大，非正规平台往往有更高的滑点和交易成本，尤其是交易现货的平台</li>
<li>开仓越频繁，交易成本越高</li>
</ul>
</li>
<li>止损止盈价格的陷阱</li>
</ol>
<p>上面讲了实盘与回测的区别，关于回测时本身可能出现的问题下面也一一列举了：</p>
<h2 id="为什么要回测"><a href="#为什么要回测" class="headerlink" title="为什么要回测"></a>为什么要回测</h2><p>我们首先来看一下什么叫做回测。回测就是基于历史数据，尽可能真实地还原实际交易过程，并检验交易策略绩效的过程。这样做有三个目的：</p>
<ol>
<li><p>验证交易信号的准确度；</p>
</li>
<li><p>验证交易逻辑和你的想法是否可行；</p>
</li>
<li><p>发现交易系统中的缺陷，并改进原始策略。</p>
</li>
</ol>
<hr>
<h2 id="回测陷阱之信号闪烁"><a href="#回测陷阱之信号闪烁" class="headerlink" title="回测陷阱之信号闪烁"></a>回测陷阱之信号闪烁</h2><p>交易策略在回测时是基于静态的历史数据。而真实的交易的数据是动态的。举个例子：如果最高价大于昨天的收盘价就买入开仓。这个开仓条件在实盘中，如果K线还未走完，那么最高价就是动态的，交易信号就有可能来回闪烁。而在回测时，回测引擎是基于静态的历史数据是可以模拟撮合成交的。</p>
<hr>
<h2 id="回测陷阱之未来函数"><a href="#回测陷阱之未来函数" class="headerlink" title="回测陷阱之未来函数"></a>回测陷阱之未来函数</h2><p>未来函数是用到了未来的价格，也就是说当前的条件在未来可能会被修改，同样未来函数也能造成信号闪烁的原因。所以任何函数都具有未来函数特性，比如说 Z 字函数。</p>
<hr>
<h2 id="回测陷阱之偷价"><a href="#回测陷阱之偷价" class="headerlink" title="回测陷阱之偷价"></a>回测陷阱之偷价</h2><p>　　所谓偷价行为是指利用过去的价格去交易。举个例子：如果最高价大于某个固定价位即以开盘价买入。这个条件就是在偷价格，因为在实盘中，最高价大于某个价位时，价格已经高于开盘价一定距离了，这时用开盘价是买不到的。但在回测中，是有买入信号的，并且能成交。</p>
<p>　　还有一种情况，如果价格跳空高开与策略设定的固定价格，回测时可以以固定价格成交，但是在实盘中这个固定价格显然是买不到的。</p>
<hr>
<h2 id="回测陷阱之不可能成交的价格"><a href="#回测陷阱之不可能成交的价格" class="headerlink" title="回测陷阱之不可能成交的价格"></a>回测陷阱之不可能成交的价格</h2><p>不能成交的价格分为几种情况：</p>
<ul>
<li>在实盘中，涨停时一般情况下是买不到的，反过来跌停也是如此。但是在回测中却是可以成交的。</li>
<li>交易所撮合机制是：价格优先、时间优先。有些品种盘口会经常有巨量订单，实盘时如果挂单买卖，需要等待盘口厚度，才能成交甚至不能成交。但是在回测时，挂单买卖是可以成交的。比如下面这张图的盘口厚度：</li>
</ul>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-1983635114c543aa0120c0964373d4e3_720w.jpg" alt="img"></p>
<ul>
<li><p>如果套利类策略，那么回测利润是很高的，因为回测时每次都已经假设了抢到了这些价差。真实的情况下，很多价差都抢不到，或者只抢到了一条腿，一般来说肯定是不利于你的方向的那条先成交，那么就需要马上去补另一条腿，这时候滑点已经不是1、2个点了，而套利策略本身就赚这几个点的价差，这种情况是回测中无法模拟的。真实利润完全不如回测。</p>
</li>
<li><p>黑天鹅事件。如下图红圈处，在外汇瑞郎黑天鹅事件中，尽管表面上看有开盘价、最高价、最低价、收盘价，其实当天的极端行情中，中间的价格是真空，大量的止损单，造成踩踏事件，流动性为零，成交难度非常大，但是在回测中却能止损。</p>
</li>
</ul>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-55351d2ee871968339fbf4ae61e0c8a8_720w.jpg" alt="img"></p>
<hr>
<h2 id="回测陷阱之过度拟合"><a href="#回测陷阱之过度拟合" class="headerlink" title="回测陷阱之过度拟合"></a>回测陷阱之过度拟合</h2><p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-f585baa98526a53f60b03494e5d00569_720w.jpg" alt="img"></p>
<p>针对量化交易来说，回测是基于历史数据，但历史数据的样本是有限的，如果交易策略的参数过多，或者交易逻辑过于复杂，导致交易策略过多的适应历史数据。</p>
<p>量化策略的建模过程本质上就是一个从大量的貌似随机的数据中找寻局部非随机数据的过程，如果不借助统计学的知识，很容易落入过度拟合的陷阱。</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-fa9587b32a88f491c036f3d1cc2dc870_720w.jpg" alt="img"></p>
<p>　　如果说天文学星座是一阶过拟合的话，那么星座性格学说是二阶过拟合。</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-31546da04ee7acfc2b6890c2175e906b_720w.jpg" alt="img"></p>
<p>所以，不要自欺欺人。如果发现样本外数据表现不好，又觉得丢掉模型太可惜或者不愿意承认自己这个模型不行，而对着样本外数据继续做模型优化，直到样本外数据上也表现得一样好，那最后受伤的一定是你的真金白银。</p>
<hr>
<h2 id="回测陷阱之幸存者偏差"><a href="#回测陷阱之幸存者偏差" class="headerlink" title="回测陷阱之幸存者偏差"></a>回测陷阱之幸存者偏差</h2><p>　　举个通俗的例子来解释什么是幸存者偏差：</p>
<p>　　1、某宝卖降落伞的商品都是好评。因为降落伞有问题的人都不存在了。</p>
<p>　　2、某电台记者在高铁上采访乘客是否买到车票。因为买不到车票的人根本上不了车。</p>
<p>　　3、媒体宣传买彩票可以中大奖。因为媒体不会宣传没有中奖的人。</p>
<p>　　4。站在风口，猪都会飞。</p>
<p>　　通过上面例子可以看到，实际上人们接受到的信息其实是经过筛选后的结果，因此生存者偏差造成的结果就是往往一开始就有大量的数据或样本被忽略了，导致基于生存者偏差的结论偏离实际。</p>
<p>　　那么在量化回测中，就要小心了。回测的结果也有其运气成分，许多情况下回测出来的结果，可能是在众多次数的回测中表现较好的一次。</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-40bad8e9e7461505860ee233d8fde82e_720w.jpg" alt="img"></p>
<p>　　金融市场中明星多、寿星少。如果交易者的策略刚好与市场行情契合，那么每年的行情就能造就一些明星。但是很难见到连续 3 年以上持续稳定盈利的寿星。</p>
<p>　　作为一个交易者，不管是量化交易也好，还是手工交易也好。都要扪心自问，自己的盈利究竟是来自于好运气，还是能力。往往许多人把自己的好运气当做自己的能力。</p>
<hr>
<h2 id="回测陷阱之小样本统计"><a href="#回测陷阱之小样本统计" class="headerlink" title="回测陷阱之小样本统计"></a>回测陷阱之小样本统计</h2><p>　　尽管拥有庞大数据的历史，但面对浩瀚无尽且不可预测的未来，历史数据就显得极度匮乏。即便是以现有的全部历史数据来验证，交易策略也不能被证实，但是可以被证伪，那么在证伪策略的时候，就需要尽量多的历史数据，来证伪策略本身。而不是拿很小样本的测试来说明问题。</p>
<hr>
<h2 id="回测陷阱之点差"><a href="#回测陷阱之点差" class="headerlink" title="回测陷阱之点差"></a>回测陷阱之点差</h2><p>　　盯过盘的都知道，买一价和卖一价的价格是至少一个点差的，不活跃的品种点差会更大，如果在发生信号后保证成交，得以对手价去下单，一般和信号模拟价相比要损失一个点差左右的，所以在写交易模型编写时必须要扣除点差，特别是在写交易频率较高的日内模型时特别重要，好多这类策略都是如果没有计算点差，那测试报告的资金曲线几乎是一根笔直的斜向上的直线，一旦加上点差，立马变为亏损。</p>
<hr>
<h2 id="回测陷阱之冲击成本"><a href="#回测陷阱之冲击成本" class="headerlink" title="回测陷阱之冲击成本"></a>回测陷阱之冲击成本</h2><p>　　无论多么精细的回测引擎，其回测只是基于在静态数据，很难模拟出真实的交易环境。举个例子：下单价格是1050买入，但实际成交价可能是1051。造成这种现象的原理有很多，比如：极端行情时流动性真空、网络延迟、软硬件系统、服务器响应等。</p>
<p>不加滑点回测：</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-b998786144e863d226c18a4e439bc231_720w.jpg" alt="img"></p>
<p>加滑点回测：</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-b5478717d12e8a0f6c64ee267c452da5_720w.jpg" alt="img"></p>
<p>　　但是在回测中是理想环境，没有价格冲击成本，回测时的成交就是1050。特别是高频或短线策略，想象一下，在回测中成千上万次交易，其回测结果也是不真实的。如下图：</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-690ebe0725dab56c3ee4eb92b2c1353b_720w.jpg" alt="img"></p>
<hr>
<h2 id="回测陷阱之风险收益比"><a href="#回测陷阱之风险收益比" class="headerlink" title="回测陷阱之风险收益比"></a>回测陷阱之风险收益比</h2><p>　　交易策略的几何年化收益率/最大回撤的比值在2以上。对于一些粗糙模型，在单个品种上很难让几何年化收益率/最大回撤这个比值大于2，如果在相关度低的多品种上进行组合投资，使组合投资报告上的几何年化收益率/最大回撤这个比值大于2，也是可以的。</p>
<p>下面的策略长期能赚钱，那才不正常。</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-98317d93ecc5d89c1b61bdcf68b1bcc7_720w.jpg" alt="img"></p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%AF%B9%E7%AD%96/v2-09db388893d8591b0a4eb67fe3976674_720w.jpg" alt="img"></p>
<h1 id="仓位多少合适"><a href="#仓位多少合适" class="headerlink" title="仓位多少合适"></a>仓位多少合适</h1><h2 id="轻仓重仓，无法一概而论"><a href="#轻仓重仓，无法一概而论" class="headerlink" title="轻仓重仓，无法一概而论"></a>轻仓重仓，无法一概而论</h2><p>在实战交易中，有的人偏好一直轻仓，从容应对，风险第一。有的人喜欢偶尔重仓，有的放矢，重锤出击。</p>
<p>量化之王西蒙斯，由于交易标的分散，所以对单个标的的仓位实行轻仓，而金融巨鳄索罗斯却相反，对狙击英镑，泰铢等战役，都是重仓押注。巴菲特的老师格雷厄姆，通过持有大量股票，每个股票都极其轻仓的方法来分散风险，进行获利。</p>
<p>而股神巴菲特则反其道而行之，对深入研究看好的公司，进行集中重仓，这才有了现在千亿美金的财富，用巴菲特自己的话说，从分散到集中的转变，如同自己从猿人进化到了人类。</p>
<p>我们在实际的外汇交易过程中，仓位也没有教科书般的标准答案，因为仓位和交易者的心理承受能力，和交易系统有关，所以在不同的人眼里，有着不同的理解。</p>
<h2 id="凯利公式计算仓位"><a href="#凯利公式计算仓位" class="headerlink" title="凯利公式计算仓位"></a>凯利公式计算仓位</h2><p>凯利公式：$F = P-\frac{1-P}{B}$，$P$为胜率，$B$为赔率（盈亏比），根据凯利公式可以算出每一次交易止损位应放在哪里。例如$P=35\%$，$B=2$，则$F = 0.35-\frac{0.6}2 = 5\%$，也就是每次最大允许$5\%$的亏损</p>
<p>想要具体了解可以看<a href="https://www.fxeye.com/201901286504347500.html" target="_blank" rel="noopener">参考链接3</a>，其中讲述了最常用的7种仓位控制方法。</p>
<h1 id="过多追求胜率"><a href="#过多追求胜率" class="headerlink" title="过多追求胜率"></a>过多追求胜率</h1><ul>
<li>胜率不是越高越好</li>
<li>过度追求胜率往往导致过拟合</li>
</ul>
<blockquote>
<p>“就专业交易员而言，其获利交易百分率经常低于40%”                 ———-《高级技术分析》</p>
<p>“如果期货外汇交易有20％的成功率，那他就非常走运了。”        ———-《操盘建议—全球顶尖交易员的成功实践和心路历程》</p>
<p>“任何人进入金融市场，如果他预期将有一半以上的交易会获利，这项预期会被很粗鲁的惊醒。”        ———-《专业投机原理》</p>
</blockquote>
<p><strong>如何提高盈亏比才是赚钱关键？</strong></p>
<p>盈亏比是滞后指标，等你做了很多交易以后，你统计了自己的盈亏比大概是多少。这时你才会知道自己盈亏比大概是多少。</p>
<p>所以我们入场，很难以盈亏比来指导。你能赚多少，是市场给的，不是你算出来的。如果强制性设置盈亏比，很可能盈利变亏损，也可能踏空行情。笔者认为相对而言，设置止损容易，盈利后合理的盈利出场点更难，给出几个建议：</p>
<ol>
<li><p>因此应该按你的交易系统做好交易计划，在你所遵循的行情判断的走势发生变化时按照止盈出场。</p>
</li>
<li><p>根据交易计划盈利后，建议将成本线设置为强制出场点，保证至少不亏损。</p>
</li>
<li><p>有一定的浮盈空间，可以适度拿盈利博取更大的行情，扩大盈亏比！再结合自身情况进行匹配选择，最终慢慢完善自己的交易体系！坚持执行，保证交易的一致性！</p>
</li>
</ol>
<h1 id="缺乏规则的主观干预"><a href="#缺乏规则的主观干预" class="headerlink" title="缺乏规则的主观干预"></a>缺乏规则的主观干预</h1><ul>
<li>规则时程序化交易的精髓</li>
<li>主管干预的结果通常不如不干预（长期来看）</li>
</ul>
<p>可以看看这篇访谈：<a href="http://ds.hexun.com/article/detail_96.html" target="_blank" rel="noopener">王阳军：必须要相信自己的策略，不要人为主观地去干预它</a></p>
<h1 id="长假日是否要减仓或平仓"><a href="#长假日是否要减仓或平仓" class="headerlink" title="长假日是否要减仓或平仓"></a>长假日是否要减仓或平仓</h1><ul>
<li>用历史回测来检验是否需要减仓</li>
<li>减仓或平仓往往效果不如不减（长期实盘交易来看）</li>
</ul>
<p>参考链接：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/31373112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31373112</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/111789502" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/111789502</a></li>
<li><a href="https://www.fxeye.com/201901286504347500.html" target="_blank" rel="noopener">https://www.fxeye.com/201901286504347500.html</a></li>
<li><a href="https://www.jianshu.com/p/74e6c8a93eb2" target="_blank" rel="noopener">https://www.jianshu.com/p/74e6c8a93eb2</a></li>
</ul>
]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（九）—— 如何平滑收益曲线</title>
    <url>/posts/167437d7.html</url>
    <content><![CDATA[<h1 id="什么是平滑"><a href="#什么是平滑" class="headerlink" title="什么是平滑"></a>什么是平滑</h1><p>先来看两个曲线。</p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%20%E5%A6%82%E4%BD%95%E5%B9%B3%E6%BB%91%E6%94%B6%E7%9B%8A%E6%9B%B2%E7%BA%BF/611ef59fae557c48702b4ef8f1d21b3a_720w.jpg" alt="img"></p>
<p><img src="/Pic/%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%20%E5%A6%82%E4%BD%95%E5%B9%B3%E6%BB%91%E6%94%B6%E7%9B%8A%E6%9B%B2%E7%BA%BF/0cf0512c9c4fad7dbd8f9dc00380a1ef_720w.jpg" alt="img"></p>
<p>很明显，下面的曲线更好。上面的曲线也是盈利的曲线，可为什么看起来会有这样明显的差别？原因就在于<strong>回撤小</strong>，所以下面的曲线更加平滑。</p>
<h1 id="平滑曲线常用的方式"><a href="#平滑曲线常用的方式" class="headerlink" title="平滑曲线常用的方式"></a>平滑曲线常用的方式</h1><p>我们平滑收益曲线通常用是三种方式：多元化、仓位控制与资产曲线管理。</p>
<h2 id="多元化"><a href="#多元化" class="headerlink" title="多元化"></a>多元化</h2><p>注意：相关程度越低越好，负相关效果更好</p>
<ul>
<li>多品种：操作品种的多少。品种间关联程度低，可以进行风险对冲的更平滑。例如只操作黄金一个商品，和同时操作黄金、咖啡、日元和沪深300指数这个投资组合相比，在稳定盈利的前提下，一般是后面的组合收益曲线更平滑。尽管股票、商品、汇率、利率等市场也会存在同涨同跌的情形，但毕竟不同品类会出现“板块轮动”，涨跌的节奏不会完全一样。此消彼长之间，收益曲线就会平滑一些。</li>
<li>多策略：不同的策略加载在同一个品种（多个不同品种）上</li>
<li>多参数：参数的最佳区域是随时间改变的，永远不知道哪个参数效果最好，可以在策略上 加载多套参数，总体而言 不会太差（到处撒网不至于网网空）</li>
<li>多周期：同一个策略可以应用在不同周期二点图表上（找出哪些周期可以利用这一策略使收益为正），可以试着用不常用（非整数）的周期</li>
<li>多市场：可以进行多市场组合，如国别地区、股票期货，对资金和知识要求较高</li>
</ul>
<h3 id="多品种的权重分配（头寸分配与调整）"><a href="#多品种的权重分配（头寸分配与调整）" class="headerlink" title="多品种的权重分配（头寸分配与调整）"></a>多品种的权重分配（头寸分配与调整）</h3><ul>
<li>等合约价值：每个合约都有一定价值，让权重配置能使两种品种的合约价值相等</li>
<li>等保证金金额：与等合约价值类似</li>
<li>等风险分配：在设置止损位时使得每个交易品种的止损金额相等</li>
<li>等最大回撤：在历史回撤报告中可找出每个品种的最大回撤，根据历史最大回撤数值找到最大公倍数从而分配权重 </li>
<li>等单跳价值</li>
</ul>
<h2 id="仓位控制（根本）"><a href="#仓位控制（根本）" class="headerlink" title="仓位控制（根本）"></a>仓位控制（根本）</h2><ul>
<li>凯利公式：$F = P-\frac{1-P}B$，其中$P$为胜率，$B$为赔率（盈亏比），$F$为下注金额（在期货交易中体现为每一手交易应该设置的止损位，每次交易应该允许的最大损失）。赌博与期货使用情况不同需要调整</li>
<li>操作仓位的轻重。轻仓的更平滑。例如：每次用1%的资本冒险，开仓错了，资金回撤1%；每次用10%的资本冒险，开仓错了，资金回撤10%。将10%的资金用于一个品种，和50%的资金用于一个品种，在市场出现不利波动时，回撤幅度自然也是不一样的。在命中率和回报比确定的前提下，风险资金比例越大，持仓仓位越重，回撤越大，就这么简单。</li>
</ul>
<h2 id="资产曲线管理"><a href="#资产曲线管理" class="headerlink" title="资产曲线管理"></a>资产曲线管理</h2><ul>
<li><p>若策略有失效的可能，或行情对走势不利则暂停做交易，若对策略很有信息相信资产曲线会重新走上支撑位，就在这个期间等待，不再进行交易，从而避免较大的回撤，使得交易曲线更光滑。在突破前期高点红线的位置重新做交易。</p>
</li>
<li><p>资产收益曲线在每次往下回撤10%到15%之后必定会有一波上涨，可以平时不交易，关注模拟盘中的资产收益曲线，当资产曲线往下回撤10-15%后开始交易，当资产曲线又增长了10-15%就停止交易</p>
</li>
</ul>
<h1 id="曲线平滑真的那么令人高兴吗"><a href="#曲线平滑真的那么令人高兴吗" class="headerlink" title="曲线平滑真的那么令人高兴吗"></a>曲线平滑真的那么令人高兴吗</h1><p>现在我们的问题是：平滑的资金曲线真的是一件令人高兴的事吗？如果你还在追求减少回撤，你会觉得平滑的曲线确实是好事。但当身临其境时，也许并非如此。</p>
<p>平滑的收益曲线意味着回撤幅度更小，但资金增长的幅度同样变得平滑。</p>
<ul>
<li>当你轻仓的时候，资金在增长，你会不会后悔，“早知道多进一点了”？</li>
<li>当你采取多品种对冲策略时候，手中的组合大赚小赔，你会不会后悔，“早知道会止损，就不动某某品种了”？</li>
</ul>
<p>亏损令人感到沮丧，少赚也会让人沮丧。这种沮丧就像惦记着该买没买又开出了大奖的彩票，挥之不去，余音不绝。</p>
<p>因此，当你在追求小幅度回撤的时候，当你在为着平滑的曲线努力的时候，有必要想清楚，能不能抵抗上面两种感受带来的失落和沮丧？那是平滑的代价，也是利润对风险的妥协。事实上我们应该追求两件事：</p>
<ul>
<li><strong>首先，搞清楚适合自己的MDD%（Max Drawdown），然后努力控制住它。</strong></li>
</ul>
<p>一般来讲，无论什么样的风险偏好，这个比值以不超过30%为宜。30%的浮亏对于回本来讲需要实现43%左右的盈利，比亏损幅度高出了40个百分点。超过这个水平，回本难度变大，浮亏基本就是“被套”了。</p>
<p>控制MDD%的方法有很多。比如，随着浮亏的加大，逐级降低R值；设立警戒线，必要时停止交易，通过无风险品种获取固定收益弥补亏损后继续操作；多品种小仓位操作不同市场的不同品种，对冲波动风险等。这些方法不一而足，但背后有一个共同的逻辑：<strong>控制好你的交易冲动和欲望</strong>。</p>
<ul>
<li><strong>其次，在自己能接受的MDD%水平上尽量将收益率最大化，找到盈利的方法，努力提高MAR比值。</strong></li>
</ul>
<p>要结合最大回撤比确定交易策略。你确定的MDD%只有5%，那么就不要动用2成仓位以上的资金；你确定的MDD%可以到达25%，那么可以适当的下重手，以2%的R值去交易。总之，无论怎么设计、规划，确定MDD%以后再找方法。</p>
<p>交易的目的不在于收益率，而在于保持一致性。<strong>通过保持一致性，确保稳定的MAR比值。这才是我们追求“平滑的收益曲线/资金曲线”背后的东西。</strong></p>
<p>参考链接：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/19832895" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/19832895</a></li>
<li>人人宽客量化交易课程</li>
</ul>
]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（八）—— 量化策略交易风险防范</title>
    <url>/posts/7224cc4c.html</url>
    <content><![CDATA[<p>量化交易此种交易模式最显著的缺点就是，如果指数往一个方向发生显著变化，数量巨大且量化运算模式相似的量化基金会在同一时间段触发，造成短时间内指数价格大幅度向此方向发生变化。 面对此情景，就非常有必要探讨金融市场量化交易策略和风险，这对有效降低极端行情的发生有着至关重要的作用。<strong>量化交易一定会失效，不同的只是时间长短</strong>，因此我们需要判断量化交易策略何时失效。</p>
<h1 id="量化交易的策略"><a href="#量化交易的策略" class="headerlink" title="量化交易的策略"></a>量化交易的策略</h1><h2 id="趋势动量类策略"><a href="#趋势动量类策略" class="headerlink" title="趋势动量类策略"></a>趋势动量类策略</h2><p>趋势动量类策略的内在含义主要是以市场均衡理论为根本，<strong>在一个事件发生后，市场将达到供需的新平衡</strong>，这个周期就形成了原始趋势。</p>
<p>在价格趋于平稳阶段，因为市场参与者所获取的信息在时间上存在着一定的延迟，以及受到的情感上的影响，所以其价格一般来讲就不会发生变化。量化投资者通过对统计手段的利用，然后充分的挖掘量价数据，从而以某些特别指定因子变化的概率分布为基础，最终将某类资产价格计算出来，并通过对这些统计量和仓位管理算法的密切联系来实施交易的策略。</p>
<p>此种类型的算法和大部分数量化策略并无差异，其可以在把握好长尾风险的同时，借由长期交易来实现累积收益的目标。其根<strong>本内涵在于经济中个体的从众跟风心理</strong>，在现如今备受关注的行为金融学中，市场投资决策者在面对突然事件时，过于注重眼前信息的原理，在技术分析法中也有着非常高的使用概率。</p>
<h2 id="均值回归类策略"><a href="#均值回归类策略" class="headerlink" title="均值回归类策略"></a>均值回归类策略</h2><p>均值回归策略有着较强的适用性，在动量策略中所提到的驱使会在相应周期内发生多头行情或者空头行情的现象， 以此来对短期内来回交易次数过多的情况实施改进，其根本内涵在于价格需要和价值呈正比，<strong>从长远分析来看价格始终 围绕着价值在来回发生变化</strong>，即存在长期偏离正常价格的可能性，大多数情况下会在相应时间内表现出<strong>基差收敛的特征</strong>。通过对此特征的利用，量化投资者不但能够通过定价模型将 资产长期价值的均值和价格偏离的情况计算出来，而且<strong>在超过一定阈值时通过做多或做空某资产价格的方法，从而等待其对价值的收敛</strong>。</p>
<p>但因为在定价资产价格的过程中会受到不同原因的影响，所以配对交易就自然而然地成为在实战中应用性非常强的一种交易策略。如在权益市场当中，将埃克森美孚和福特组成风险中性的配对交易，在大宗商品交易场所中， 可将玉米和大豆当成一组均值回归的交易对象，假如其价格对比另一价格偏离大于阈值，<strong>卖出过高估计价格的资产并看好等价的价格低谷的资产，等待价差收敛</strong>。</p>
<p>基本来讲，<strong>套利策略也称之为回归型策略</strong>，如跨期等其实从根本来看都可以归之为对标产品合约，创建配对交易组合，可以获取广泛的应用，但由于交易机会如跨期等实质上都属于为寻找对标产品合约，建立配对交易组合，此类策略有着众多的应用，但因为交易机会暴露 时间过短，所以就经常需要借助技术的支持，以便可以加快交易的速度，从而在交易上占据绝对性的优势。</p>
<h2 id="技术情绪"><a href="#技术情绪" class="headerlink" title="技术情绪"></a>技术情绪</h2><p>技术情绪策略，实质上主要是<strong>通过行为金融学知识和技术方法的有机联系，从而最大程度上寻找和发现市场中隐藏的基本规律</strong>，如期权市场通过对历史平均水平相对变化的观察，来将其作为市场情绪指标，然后再依据历史基准水平的情况来评估市场情绪的情况。</p>
<p>另外，针对限价订单薄，高频量交易者可实施进一步的研究，然后通过充分的挖掘众多历史订单薄数据，从而预先推知多空博弈在未来一段周期内所带给市场价格的变化。此种交易主要应用在市商中，做市商通过对价格的变现来实现盈利的目标。</p>
<p>技术情绪类策略经常是对某种情况的深层次挖掘，并无特殊规定的模式。虽然价格变化在整个过程中贯穿，但这并不代表价格并不存在缺口的情况，即一旦出现两根相邻 k 线的趋势，就会发生上述的结果。 此种缺口主要是指股的开盘价使 K 线图发生空档的情况，然后当成趋势操作的信号，一旦发生跳空缺口回补就可清仓停止损失时停止盈利。</p>
<h1 id="量化交易的风险"><a href="#量化交易的风险" class="headerlink" title="量化交易的风险"></a>量化交易的风险</h1><h2 id="在历史数据的选用上容易出现幸存者偏差"><a href="#在历史数据的选用上容易出现幸存者偏差" class="headerlink" title="在历史数据的选用上容易出现幸存者偏差"></a>在历史数据的选用上容易出现幸存者偏差</h2><p>由于权益投资市场中向外提供的公司股票数据，大部分是当前上市公司的股票，而在投资决策模型中，反而并没有将 一些未持续经营公司作为训练数据输入到其中，显而易见，这是一种缺乏对关键步骤重视的表现，而如此一来，就出现了回溯测试和实盘交易两者之间结果偏差的情况。</p>
<h2 id="数据来源的风险（未来函数）"><a href="#数据来源的风险（未来函数）" class="headerlink" title="数据来源的风险（未来函数）"></a>数据来源的风险（未来函数）</h2><p>在训练以往各时间段K线走势图时需要注意尽量不要出现将未知变量当成已知因子的情况，随着数据获取的区域越来越大，所以此种情况很难发现，由此就需要交易员增强对不同类型数据来源的认识。</p>
<p>如公司发布的年报季报、国家每年发布的CPI、GDP、基尼系数、恩格尔系数等， 其审核后的发布时间均明显后于统计描述时间，这样的结果极易造成对特定发布时间的忽视。针对此种情况，就可以通过检查清单来避免此种风险的发生。</p>
<h2 id="拟合风险"><a href="#拟合风险" class="headerlink" title="拟合风险"></a>拟合风险</h2><p>在模型训练过程中，经常会应用到一些机器学习算法，但采取这些手段极易产生拟合风险，特别是在一些研究方向训练数据中比较少，所以就需要进一步提升模型的泛化能力。</p>
<p>如参数项设置的越简单，那么就预示着其模型也并无明显的复杂性，有着非常高的泛化能力，不但如此，其算法欠拟合程度也会得到较好的改善，降低预测价格的能力，而这就离不开策略开发工作者的支持，依据模型的情况来寻找最佳的平衡点。</p>
<h2 id="交易成本风险"><a href="#交易成本风险" class="headerlink" title="交易成本风险"></a>交易成本风险</h2><p>事实上对于量化交易而言，手续费等交易成本对其有着不可或缺的作用，所以就非常有必要设置独立的成本函数，然后评估每次开仓信号的预期收益和成本间的关系，过度的开仓会增加成本，而过于严格的开仓条件，则不可避免地会减少交易数量，进而导致交易频率在整个交易中发生明显地变化。</p>
<p>其实，量化交易的本质在于借由概率，最终对盈利情况进行统计，在小交易样本下，产品的投资回报率更多情况下极易会受到长尾的干扰，致使预测无法顺利地进行。</p>
<h2 id="市场风格分形风险"><a href="#市场风格分形风险" class="headerlink" title="市场风格分形风险"></a>市场风格分形风险</h2><p>交易策略的差异性，极易受到市场风格分形的影响，如均值回归策略和趋势跟踪策略就是最好的例子，即前者在震荡市中有着更为显著的收益，而后者则更适宜于在牛市和熊市中。</p>
<h1 id="策略失效"><a href="#策略失效" class="headerlink" title="策略失效"></a>策略失效</h1><h2 id="怎样判断策略失效"><a href="#怎样判断策略失效" class="headerlink" title="怎样判断策略失效"></a>怎样判断策略失效</h2><ul>
<li>最大回撤判断：大于历史最大回撤的150%</li>
<li>收益曲线走势判断</li>
</ul>
<h2 id="突发行情的止损与取舍"><a href="#突发行情的止损与取舍" class="headerlink" title="突发行情的止损与取舍"></a>突发行情的止损与取舍</h2><ul>
<li>峰值止损：当价格变动超过某个峰值立刻停止，但不能判断随后又迅速拉回的情形</li>
<li>均值止损：会参考几条K线，看一段时间内的价格变化确定是否止损，反应不那么快，是看一段时间的行情</li>
<li>速率止损：考虑价格变化幅度与变化时间，兼顾了前两者</li>
</ul>
<p>上面三种各有优缺点，交易者可以根据自己对市场的理解与交易品种的特征决定止损方式，或者做一个组合得到自认为最优的止损方案。</p>
<h2 id="跟踪止盈止损的陷阱"><a href="#跟踪止盈止损的陷阱" class="headerlink" title="跟踪止盈止损的陷阱"></a>跟踪止盈止损的陷阱</h2><p>常见的看似很优秀的量化策略常面临跟踪止盈止损的陷阱，这体现在实盘效果永远比回测效果差，我们应当如何防范跟踪止盈止损的陷阱，有以下亮点：</p>
<ul>
<li>回落幅度不能设置过小</li>
<li>在不影响策略运行的情况下，尽量使用小周期的K线</li>
</ul>
<h2 id="量化策略风险防范"><a href="#量化策略风险防范" class="headerlink" title="量化策略风险防范"></a>量化策略风险防范</h2><ul>
<li>指数与合约的不同步误差：在量化策略中，无论是回测还是调试通常都是使用某一商品的指数，交易时是交易商品的具体合约，这两者之间存在误差！</li>
<li>交易成本：交易所需手续费；期货公司佣金；软件平台费用；滑点成本；云端服务器租金；数据费用 ；税金；心理成本</li>
<li>软硬件事故</li>
</ul>
<h3 id="交易成本"><a href="#交易成本" class="headerlink" title="交易成本"></a>交易成本</h3><h4 id="滑点成本"><a href="#滑点成本" class="headerlink" title="滑点成本"></a>滑点成本</h4><p>对于交易而言，最大的成本来自于手续费和滑点。中长线交易对这点成本很无所谓，别小看这点成本，对于短线客来说，几次下来对利润有很大吞噬。手续费不说了，要找到想的对便宜的交易商，可以多了解本行业，或者做个居间人代理，手续费相对低一些。</p>
<p><strong>滑点是指客户下单交易点位与实际交易点位有差别的一种交易现象</strong>。很多人知道什么叫滑点，但是至于滑点是怎么产生的就不知道了。有的人说行情变化大时，所以有滑点；甚至有人因此说，不滑点是不可能的，其实这不正确。正确的说法是滑点要么是交易商故意的，要么是交易商服务跟不上。</p>
<p>第一是人为方面的因素，比如黄金交易商后台人为操控，使得黄金交易平台行情报价与实际报价存在差别，从而导致黄金白银交易者在该买入的时候没有及时买入，卖出的时候没有及时卖出。一般来说投资者很难分辨滑点是由什么因素造成的，因此也就给一些不良交易商带来了获取不正当利益的机会。</p>
<p>第二是硬件方面的因素，比如黄金价bai格行情波动剧烈时，网络延迟，软件系统以及服务器响应等方面造成成交价格与挂单价格不一致。</p>
<h4 id="股票投资增值税"><a href="#股票投资增值税" class="headerlink" title="股票投资增值税"></a>股票投资增值税</h4><p>投资收益要区分不同的情况，投资国债的投资收益是免交税的，其他的投资如股权投资等是要纳税的；对于分回的投资收益还必须确认是税前的还是税后的。如果是税后的，一是要看被投资企业的企业所得税率是否与公司的税率相同，若相同则不用补交若，不同(被投资企业税率小于公司)则要按差价补税。如果是税前的，分回后并入公司的利润缴税。</p>
<p>投资收益下交增值税的有关情况：</p>
<ol>
<li><p>公司取得投资收益，应该按25%的税率缴纳企业所得税。</p>
</li>
<li><p>投资所分的利润、利息、股息、红利所得。投资收益在税收上是作为企业所得税的应税项目，应依法计征企业所得税。</p>
</li>
</ol>
<h3 id="如何防范软硬件事故"><a href="#如何防范软硬件事故" class="headerlink" title="如何防范软硬件事故"></a>如何防范软硬件事故</h3><ul>
<li>租用云服务器</li>
<li>建立应急预案</li>
<li>建立监督报警机制</li>
<li>随时可以用手机登陆账户进行操作</li>
<li>关注交易平台的群消息</li>
</ul>
<p>参考链接：</p>
<ul>
<li>魏平.金融市场量化交易策略与风险探讨[J].现代营销(下旬刊),2020(01):37-38.</li>
</ul>
]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（七）—— 避免未来函数</title>
    <url>/posts/3e13cfa8.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="ab277805dcf7eda2f20dfb15a51729ae7b0cc00f6f5a7f7d78f1dcf19eb34db0">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（六）—— 常用的止损止盈方法</title>
    <url>/posts/9e920d41.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e7db69c8a5c3093e016fce68c813a4fabd12e4720e8792ceea7744bec0d0c4c3">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（五）—— 策略优化的方法与陷阱</title>
    <url>/posts/3d4d876a.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="1b63082c8946ae6d1cb85e9c9158f4214f74b1fcc6c5daf41fae051c85cc7f7e">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（四）—— 回测报告分析</title>
    <url>/posts/ba7b3298.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e5538000ba79ac948228a0365cce75173f94a48dd1a65c30f78718eccfeca1ac">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（三）—— 高频交易</title>
    <url>/posts/135a295c.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="27ff3043b8f627760a37717da44d659d9d514012cd02c4692049d98d29c4b6a1">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（二）—— 套利</title>
    <url>/posts/12d4b8eb.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="0e536ea5ad66ccd0c1c0c399aa03b5f3e7be1456618bef1e7df7ecdfc0151fa4">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>金融量化分析基础（一）—— 名词大赏</title>
    <url>/posts/4250a155.html</url>
    <content><![CDATA[<h1 id="股票"><a href="#股票" class="headerlink" title="股票"></a>股票</h1><ul>
<li>股票是股份公司发给出资人的一种凭证，股票的持票者就是股份公司的股东</li>
<li>股票的面值与市值：<ul>
<li>面值表示票面金额</li>
<li>市值表示市场价值</li>
</ul>
</li>
<li>上市/IPO：企业通过证券交易所公开向社会增发股票以募集资金</li>
<li><p>股票的作用</p>
<ul>
<li>出资证明、证明股东身份、对公司经营发表意见</li>
<li>公司分红、交易获利</li>
</ul>
</li>
<li><p>股票的分类</p>
<ul>
<li>股票按业绩分类：<ul>
<li>蓝筹股：资本雄厚、信誉优良的公司的股票</li>
<li>绩优股：业绩优良的公司的股票</li>
<li>ST股：特别处理股票，连续两年亏损或每股净 资产低于股票面值</li>
</ul>
</li>
<li>股票按上市地区分类<ul>
<li>A股：中国大陆上市，人民币认购买卖（T+1，涨跌幅10%）</li>
<li>B股：中国大陆上市，外币认购买卖（T+1，T+3）</li>
<li>H股：中国香港上市（T+0，涨跌幅不做限制）</li>
<li>N股：美国纽约上市</li>
<li>S股：新加坡 上市</li>
</ul>
</li>
</ul>
</li>
<li><p>股票市场的构成</p>
<ul>
<li>上市公司</li>
<li>投资者（包括机构投资者）</li>
<li>证监会、证券业协会、交易所<ul>
<li>上海证券交易所：只有一个主板（沪指 ）</li>
<li>深圳证券交易所：<ul>
<li>主板：大型成熟企业（深成指）</li>
<li>中小板：经营规模较小</li>
<li>创业板：尚处于成长期的创业企业</li>
</ul>
</li>
</ul>
</li>
<li>证券中介机构</li>
</ul>
</li>
<li><p>影响股价的因素</p>
<ul>
<li>市场内部因素</li>
<li>基本面因素</li>
<li>政策因素</li>
</ul>
</li>
<li><p>股票买卖</p>
<ul>
<li>委托买卖股票：个人不能直接买卖，需要在券商开户，进行委托购买</li>
<li>股票交易日：周一到周五（非法定节假日和交易所休市日）</li>
<li>股票交易时间：<ul>
<li>9：15-9：25    开盘集合竞价时间</li>
<li>9：30-11：30   前市，连续竞价时间</li>
<li>13：00-15：00    后市，连续竞价时间</li>
<li>14：57-15：00    深交所收盘集合竞价时间</li>
</ul>
</li>
<li>T+1交易制度：股票买入后当天不能卖出，要在买入后的下一个交易日才能卖出</li>
</ul>
</li>
</ul>
<h1 id="金融分析"><a href="#金融分析" class="headerlink" title="金融分析"></a>金融分析</h1><ul>
<li>基本面分析<ul>
<li>宏观经济基本面分析：国家的财政政策、货币政策等</li>
<li>行业分析</li>
<li>公司分析：财务数据、业绩报告</li>
</ul>
</li>
<li>技术面分析：各项技术指标<ul>
<li>K线</li>
<li>平均线</li>
<li>KDJ</li>
<li>MACD</li>
</ul>
</li>
</ul>
<h1 id="金融量化投资"><a href="#金融量化投资" class="headerlink" title="金融量化投资"></a>金融量化投资</h1><ul>
<li>量化投资：利用计算机技术并且采用一定的数学模型去实践投资理念，实现投资策略的过程</li>
<li><p>量化投资的优势：</p>
<ul>
<li>避免主观情绪、人性弱点和认知偏差，选择更加客观</li>
<li>能同时包括多角度的观察和多层次的模型</li>
<li>及时跟踪市场变化，不断发现新的统计模型，寻找交易机会</li>
<li>再决定投资策略后，能通过 回测验证其效果</li>
</ul>
</li>
<li><p>量化策略：通过一套固定的逻辑来分析、判断和决策，自动化地进行股票交易</p>
<ul>
<li>核心内容：<ul>
<li>选股</li>
<li>择时</li>
<li>仓位管理</li>
<li>止盈止损</li>
</ul>
</li>
<li>策略的周期<ul>
<li>产生想法</li>
<li>实现策略</li>
<li>检验策略：回测、模拟交易</li>
<li>实盘交易</li>
<li>优化策略、放弃策略</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Quantitative Investment</category>
      </categories>
  </entry>
  <entry>
    <title>HOG特征描述算子-行人检测</title>
    <url>/posts/91c329d0.html</url>
    <content><![CDATA[<h1 id="HOG特征"><a href="#HOG特征" class="headerlink" title="HOG特征"></a>HOG特征</h1><h2 id="HOG特征概述"><a href="#HOG特征概述" class="headerlink" title="HOG特征概述"></a>HOG特征概述</h2><p>方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。Hog特征结合SVM分类器已经被广泛应用于图像识别中，尤其在行人检测中获得了极大的成功。需要提醒的是，HOG+SVM进行行人检测的方法是法国研究人员Dalal在2005的CVPR上提出的，而如今虽然有很多行人检测算法不断提出，但基本都是以HOG+SVM的思路为主。</p>
<p><strong>主要思想</strong></p>
<p>在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。</p>
<p><strong>具体的实现方法</strong></p>
<p>首先将图像分成小的连通区域，我们把它叫细胞单元（胞元）。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。</p>
<p><strong>提高性能</strong></p>
<p>把这些局部直方图在图像的更大的范围内（我们把它叫区间或block）进行对比度归一化（contrast-normalized），所采用的方法是：先计算各直方图在这个区间（block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。</p>
<p><strong>优点</strong></p>
<ul>
<li>由于HOG是在图像的局部方格单元上操作，所以它对图像几何的和光学的形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上；</li>
<li>在粗的空域抽样、精细的方向抽样以及较强的局部光学归一化等条件下，只要行人大体上能够保持直立的姿势，可以容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。因此HOG特征是特别适合于做图像中的人体检测的。</li>
</ul>
<h2 id="HOG特征提取方法"><a href="#HOG特征提取方法" class="headerlink" title="HOG特征提取方法"></a>HOG特征提取方法</h2><p>大体来说，首先对输入的图片进行预处理，然后计算像素点的梯度特特性，包括梯度幅值和梯度方向。然后投票统计形成梯度直方图，然后对blocks进行normalize，最后收集到HOG feature（其实是一行多维的vector）放到SVM里进行监督学习，从而实现行人的检测。</p>
<p><img src="/posts/CV/hog1.png" alt></p>
<p>将上述过程拆分如下，下面分别进行讲解。</p>
<h3 id="灰度化（将图像看做一个-x-y-z-（灰度）的三维图像）；"><a href="#灰度化（将图像看做一个-x-y-z-（灰度）的三维图像）；" class="headerlink" title="灰度化（将图像看做一个$x,y,z$（灰度）的三维图像）；"></a>灰度化（将图像看做一个$x,y,z$（灰度）的三维图像）；</h3><ul>
<li>灰度处理是可选操作，因为灰度图像和彩色图像都可以用于计算梯度图。对于彩色图像，先对三通道颜色值分别计算梯度，然后取梯度值最大的那个作为该像素的梯度。</li>
</ul>
<h3 id="采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）"><a href="#采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）" class="headerlink" title="采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）"></a>采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）</h3><p><strong>目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；</strong></p>
<ul>
<li>伽马矫正公式：$f(I)=I^{\gamma}$，其中$I$表示原图像<ul>
<li>当$\gamma&lt;1$时，输入图像的低灰度值区域动态范围变大，进而图像低灰度值区域对比度得以增强；在高灰度值区域，动态范围变小，进而图像高灰度值区域对比度得以降低。 最终，图像整体的灰度变亮。</li>
<li>当$\gamma&gt;1$时，输入图像的高灰度值区域动态范围变小，进而图像低灰度值区域对比度得以降低；在高灰度值区域，动态范围变大，进而图像高灰度值区域对比度得以增强。 最终，图像整体的灰度变暗。</li>
<li>比如可以取$\gamma=\frac12$</li>
</ul>
</li>
</ul>
<p>灰度图转化以及Gamma校正处理代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv2.imread(<span class="string">'*.png'</span>, <span class="number">0</span>)</span><br><span class="line">img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)</span><br><span class="line">img2 = np.power(img/float(np.max(img)),<span class="number">1</span>/<span class="number">2.2</span>)</span><br><span class="line">plt.imshow(img2)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="计算图像每个像素的梯度（包括大小和方向）"><a href="#计算图像每个像素的梯度（包括大小和方向）" class="headerlink" title="计算图像每个像素的梯度（包括大小和方向）"></a>计算图像每个像素的梯度（包括大小和方向）</h3><p><strong>主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。</strong></p>
<ul>
<li>为了得到梯度直方图，那么首先需要计算图像水平方向和垂直方向梯度。一般使用特定的卷积核对图像滤波实现，可选用的卷积模板有：soble算子、Prewitt算子、Roberts模板等等。一般采用soble算子，OpenCV也是如此，最常用的方法是：首先用$[-1,0,1]$梯度算子对原图像做卷积运算，得到$x$方向（水平方向，以向右为正方向）的梯度分量gradscalx，然后用$[1,0,-1]^T$梯度算子对原图像做卷积运算，得到$y$方向（竖直方向，以向上为正方向）的梯度分量gradscaly。然后再用以上公式计算该像素点的梯度大小和方向。</li>
<li><p>利用soble水平和垂直算子与输入图像卷积计算$dx、dy$的计算方式为：</p>
<script type="math/tex; mode=display">Sobel_X=\begin{bmatrix}1\\0\\-1\end{bmatrix}*\begin{bmatrix}1&2&1\end{bmatrix}=\begin{bmatrix}1&2&1\\0&0&0\\-1&-2&-1\end{bmatrix}\\Sobel_X=\begin{bmatrix}1\\2\\1\end{bmatrix}*\begin{bmatrix}1&0&-1\end{bmatrix}=\begin{bmatrix}1&0&-1\\2&0&-2\\1&0&-1\end{bmatrix}</script><script type="math/tex; mode=display">d_x=f(x,y)*Sobel_X(x,y)\\
d_y=f(x,y)*Sobel_Y(x,y)</script><p>或者更广泛地说：</p>
<script type="math/tex; mode=display">d_x=H(x+1,y)-H(x-1,y)</script><script type="math/tex; mode=display">d_y=H(x,y+1)-H(x,y-1)</script><p>其中$H(x,y)$为像素点$(x,y)$处的像素值。</p>
</li>
<li><p>进一步可以得到图像梯度的幅值：</p>
<script type="math/tex; mode=display">M(x,y)=\sqrt{d_x^2(x,y)+d_y^2(x,y)}</script><p>为了简化计算，幅值也可以作如下近似：</p>
<script type="math/tex; mode=display">M(x,y)=|d_x(x,y)|+|d_y(x,y)|</script><p>而梯度方向为：</p>
<script type="math/tex; mode=display">\alpha(x,y)=tan^{-1}(\frac{G_y(x,y)}{G_x(x,y)})</script></li>
<li><p>计算梯度代码如下：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read image</span></span><br><span class="line">img = cv2.imread(<span class="string">'*.jpg'</span>)</span><br><span class="line">img = np.float32(img) / <span class="number">255.0</span>  <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算x和y方向的梯度</span></span><br><span class="line">gx = cv2.Sobel(img, cv2.CV_32F, <span class="number">1</span>, <span class="number">0</span>, ksize=<span class="number">1</span>)</span><br><span class="line">gy = cv2.Sobel(img, cv2.CV_32F, <span class="number">0</span>, <span class="number">1</span>, ksize=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算合梯度的幅值和方向（角度）</span></span><br><span class="line">mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="为每个细胞单元构建梯度方向直方图"><a href="#为每个细胞单元构建梯度方向直方图" class="headerlink" title="为每个细胞单元构建梯度方向直方图"></a>为每个细胞单元构建梯度方向直方图</h3><p><strong>将图像划分成小cells（例如$6*6$像素/cell），统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor</strong></p>
<ul>
<li><p>经过前两步计算，每一个像素点都会有两个值：梯度幅值/梯度方向。在这一步中，图像被分成若干个8×8的cell(每个cell可以是矩形圆形或是星形)，例如我们将图像resize至64x128的大小，那么这幅图像就被划分为8x16个8x8的cell单元，并为每个8×8的cell计算梯度直方图。当然，cell的划分也可以是其他值：16x16，8x16等，根据具体的场景确定。</p>
</li>
<li><p>在计算梯度直方图，让我们先了解一下为什么我们将图像分成若干个cell?这是因为如果对一整张梯度图逐像素计算，其中的有效特征是非常稀疏的，不但运算量大，而且会受到一些噪声干扰。于是我们就使用局部特征描述符来表示一个更紧凑的特征，计算这种局部cell上的梯度直方图更具鲁棒性。</p>
</li>
<li><p>以8x8的cell为例，一个8x8的cell包含了8x8x2 = 128个值，因为每个像素包括梯度的大小和方向。在HOG中，每个8x8的cell的梯度直方图本质是一个由9个数值组成的向量， 对应于0、20、40、60…160的梯度方向(角度)。那么原本cell中8x8x2 = 128个值就由长度为9的向量来表示，用这种梯度直方图的表示方法，大大降低了计算量，同时又对光照等环境变化更加地鲁棒。首先看下图：</p>
</li>
</ul>
<p><img src="/posts/CV/hog2.png" alt></p>
<ul>
<li><p>左图是衣服64x128的图像，被划分为8x16个8x8的cell；中间的图像表示一个cell中的梯度矢量，箭头朝向代表梯度方向，箭头长度代表梯度大小。右图是 8×8 的cell中表示梯度的原始数值，注意角度的范围介于0到180度之间，而不是0到360度， 这被称为“无符号”梯度，因为两个完全相反的方向被认为是相同的。</p>
</li>
<li><p>接下来，我们来计算cell中像素的梯度直方图，将0-180度分成9等份，称为9个bins，分别是0，20，40…160。然后对每个bin中梯度的贡献进行统计：</p>
</li>
</ul>
<p><img src="/posts/CV/hog3.png" alt></p>
<p>统计方法是一种加权投票统计， 如上图所示，某像素的梯度幅值为13.6，方向为36，36度两侧的角度bin分别为20度和40度，那么就按一定加权比例分别在20度和40度对应的bin加上梯度值，加权公式为：</p>
<ul>
<li>20度对应的bin：$((40-36)/20) * 13.6$，分母的20表示20等份，而不是20度；</li>
<li>40度对应的bin：$(36-20)/20) * 13.6$，分母的20表示20等份，而不是20度；</li>
</ul>
<p>还有一个细节需要注意，如果某个像素的梯度角度大于160度，也就是在160度到180度之间，那么把这个像素对应的梯度值按比例分给0度和160度对应的bin。如左下图绿色圆圈中的角度为165度，幅值为85，则按照同样的加权方式将85分别加到0度和160度对应的bin中。</p>
<p><img src="/posts/CV/hog4.png" alt></p>
<p>对整个cell进行投票统计，正是在HOG特征描述子中创建直方图的方式，最终得到由9个数值组成的向量—梯度方向图：</p>
<p><img src="/posts/CV/hog5.png" alt></p>
<h3 id="把细胞单元组合成大的block，块内归一化梯度直方图"><a href="#把细胞单元组合成大的block，块内归一化梯度直方图" class="headerlink" title="把细胞单元组合成大的block，块内归一化梯度直方图"></a>把细胞单元组合成大的block，块内归一化梯度直方图</h3><p><strong>将每几个cell组成一个block（例如$3*3$cell/block或4cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor</strong></p>
<p>由于局部光照的变化以及前景-背景对比度的变化，使得梯度强度的变化范围非常大。这就需要对梯度强度做归一化。</p>
<p>作者采取的办法是：把各个细胞单元组合成大的、空间上连通的区间（blocks）。这样，一个block内所有cell的特征向量串联起来便得到该block的HOG特征。这些区间是互有重叠的，这就意味着：每一个单元格的特征会以不同的结果多次出现在最后的特征向量中。我们将归一化之后的块描述符（向量）就称之为HOG描述符。</p>
<p><strong>归一化能够进一步地对光照、阴影和边缘进行压缩。</strong>归一化的方法有很多：L1-norm、L2-norm、max/min等等，一般选择L2-norm。具体方式为：</p>
<ul>
<li>一个cell有一个梯度方向直方图，包含9个数值，一个block有4个cell，那么一个block就有4个梯度方向直方图，将这4个直方图拼接成长度为36的向量，然后对这个向量进行归一化。</li>
<li>而每一个block将按照上图滑动的方式进行重复计算，直到整个图像的block都计算完成</li>
</ul>
<h3 id="收集HOG特征，将block串联得到image"><a href="#收集HOG特征，将block串联得到image" class="headerlink" title="收集HOG特征，将block串联得到image"></a>收集HOG特征，将block串联得到image</h3><p>最后一步就是将检测窗口中所有重叠的块进行HOG特征的收集，并将它们结合成最终的特征向量供分类使用。将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor，这个就是最终的可供分类使用的特征向量。</p>
<h3 id="HOG过程梳理及特征维数"><a href="#HOG过程梳理及特征维数" class="headerlink" title="HOG过程梳理及特征维数"></a>HOG过程梳理及特征维数</h3><ul>
<li>把样本图像分割为若干个像素的单元（cell），把梯度方向平均划分为9个区间（bin）</li>
<li>在每个单元里面对所有像素的梯度方向在各个方向区间进行直方图统计，得到一个9维的特征向量</li>
<li>每相邻的4个单元构成一个块（block），把一个块内的特征向量联起来得到36维的特征向量</li>
<li>用块对样本图像进行扫描，扫描步长为一个单元</li>
<li>最后将所有块的特征串联起来，就得到了人体的特征</li>
</ul>
<p>例如，对于$64<em>128$的图像而言，每$8</em>8$的像素组成一个cell，每$2<em>2$个cell组成一个块，因为每个cell有9个特征，所以每个块内有$4</em>9=36$个特征，以8个像素为步长，那么，水平方向将有7个扫描窗口，垂直方向将有15个扫描窗口。也就是说，$64<em>128$的图片，总共有$36</em>7*15=3780$个特征。</p>
<h1 id="HOG行人检测"><a href="#HOG行人检测" class="headerlink" title="HOG行人检测"></a>HOG行人检测</h1><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol>
<li>提取正负样本hog特征</li>
<li>投入svm分类器训练，得到model</li>
<li>由model生成检测子</li>
<li>利用检测子检测负样本，得到hardexample</li>
<li>提取hardexample的hog特征并结合第一步中的特征一起投入训练，得到最终检测子。</li>
</ol>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    src = cv.imread(<span class="string">"*.jpg"</span>)</span><br><span class="line">    cv.imshow(<span class="string">"input"</span>, src)</span><br><span class="line"></span><br><span class="line">    hog = cv.HOGDescriptor()</span><br><span class="line">    hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())</span><br><span class="line">    <span class="comment"># Detect people in the image</span></span><br><span class="line">    (rects, weights) = hog.detectMultiScale(src,</span><br><span class="line">                                            winStride=(<span class="number">2</span>,<span class="number">4</span>),</span><br><span class="line">                                            padding=(<span class="number">8</span>, <span class="number">8</span>),</span><br><span class="line">                                            scale=<span class="number">1.2</span>,</span><br><span class="line">                                            useMeanshiftGrouping=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> (x, y, w, h) <span class="keyword">in</span> rects:</span><br><span class="line">        cv.rectangle(src, (x, y), (x + w, y + h), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    cv.imshow(<span class="string">"hog-detector"</span>, src)</span><br><span class="line">    cv.imwrite(<span class="string">"hog-detector.jpg"</span>,src)</span><br><span class="line">    cv.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>可视化方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> feature, exposure</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image = cv2.imread(<span class="string">'sp_g.jpg'</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">fd, hog_image = feature.hog(image, orientations=<span class="number">9</span>, pixels_per_cell=(<span class="number">8</span>, <span class="number">8</span>),</span><br><span class="line">                    cells_per_block=(<span class="number">2</span>, <span class="number">4</span>), visualize=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Rescale histogram for better display</span></span><br><span class="line">hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(<span class="number">0</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">cv2.namedWindow(<span class="string">"img"</span>,cv2.WINDOW_NORMAL)</span><br><span class="line">cv2.imshow(<span class="string">'img'</span>, image)</span><br><span class="line">cv2.namedWindow(<span class="string">"hog"</span>,cv2.WINDOW_NORMAL)</span><br><span class="line">cv2.imshow(<span class="string">'hog'</span>, hog_image_rescaled)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)==ord(<span class="string">'q'</span>)</span><br></pre></td></tr></table></figure>
<p>我们的实验图片为：</p>
<p><img src="/posts/CV/xr.jpg" alt></p>
<p>实验结果为：</p>
<p><img src="/posts/CV/hd.jpg" alt></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>HOG算法具有以下优点：</p>
<ol>
<li>HOG描述的是边缘结构特征，可以描述物体的结构信息</li>
<li>对光照影响不敏感</li>
<li>分块的处理可以使特征得到更为紧凑的表示</li>
</ol>
<p>HOG算法具有以下缺点：</p>
<ol>
<li>特征描述子获取过程复杂，维数较高，导致实时性差</li>
<li>遮挡问题很难处理</li>
<li>对噪声比较敏感</li>
</ol>
<blockquote>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/liulina603/article/details/8291093" target="_blank" rel="noopener">https://blog.csdn.net/liulina603/article/details/8291093</a></li>
<li><a href="https://www.cnblogs.com/hehuaizhou/p/13206415.html" target="_blank" rel="noopener">https://www.cnblogs.com/hehuaizhou/p/13206415.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Haar特征描述算子-人脸检测</title>
    <url>/posts/7e50584a.html</url>
    <content><![CDATA[<h1 id="人脸检测例子"><a href="#人脸检测例子" class="headerlink" title="人脸检测例子"></a>人脸检测例子</h1><h2 id="人脸检测"><a href="#人脸检测" class="headerlink" title="人脸检测"></a>人脸检测</h2><p>在开始介绍Haar特征描述算子之前，为了便于理解，我们直接看具体怎么通过opencv调用训练好的haar模型，从而实现人脸识别。首先Mark一下一些已经训练好的haar的模型，可以直接下载，里面包含了多种人类特征检测的训练模型，包括脸、身体、眼睛、笑脸等，链接<a href="https://github.com/opencv/opencv/tree/master/data/haarcascades" target="_blank" rel="noopener">戳我</a></p>
<p>我们打开的github链接长这样：</p>
<p><img src="/posts/CV/haar1.png" alt></p>
<p>我们下载其中一个haarcascade_frontalface_default.xml作为示例（点Raw后下载网页）。然后我们导入待识别图：</p>
<p><img src="/posts/CV/4.jpg" alt></p>
<p>执行以下代码，即可获得人脸检测的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"Pic/1.jpg"</span>)</span><br><span class="line"></span><br><span class="line">face_engine = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="string">'haarcascade_frontalface_default.xml'</span>)</span><br><span class="line"></span><br><span class="line">faces = face_engine.detectMultiScale(img,scaleFactor=<span class="number">1.3</span>,minNeighbors=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    img = cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>检测结果不负众望：</p>
<p><img src="/posts/CV/reba.png" alt></p>
<p>我们留意到以上代码的face_engine步骤，其作用是导入人脸级联分类器引擎，’.xml’文件里包含训练出来的人脸特征。随后用人脸级联分类器引擎进行人脸识别，返回的faces为人脸坐标列表，1.3是放大比例，5是重复识别次数。</p>
<h2 id="人脸检测和人眼检测"><a href="#人脸检测和人眼检测" class="headerlink" title="人脸检测和人眼检测"></a>人脸检测和人眼检测</h2><p>我们也可以尝试前面xml文件中的人眼检测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入opencv</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入人脸级联分类器引擎，'.xml'文件里包含训练出来的人脸特征，cv2.data.haarcascades即为存放所有级联分类器模型文件的目录</span></span><br><span class="line">face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="string">'haarcascade_frontalface_default.xml'</span>)</span><br><span class="line"><span class="comment"># 导入人眼级联分类器引擎吗，'.xml'文件里包含训练出来的人眼特征</span></span><br><span class="line">eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="string">'haarcascade_eye.xml'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入一张图片，引号里为图片的路径，需要你自己手动设置</span></span><br><span class="line">img = cv2.imread(<span class="string">'image3.png'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用人脸级联分类器引擎进行人脸识别，返回的faces为人脸坐标列表，1.3是放大比例，5是重复识别次数</span></span><br><span class="line">faces = face_cascade.detectMultiScale(img, <span class="number">1.3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每一张脸，进行如下操作</span></span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    <span class="comment"># 画出人脸框，蓝色（BGR色彩体系），画笔宽度为2</span></span><br><span class="line">    img = cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 框选出人脸区域，在人脸区域而不是全图中进行人眼检测，节省计算资源</span></span><br><span class="line">    face_area = img[y:y+h, x:x+w]</span><br><span class="line">    eyes = eye_cascade.detectMultiScale(face_area)</span><br><span class="line">    <span class="comment"># 用人眼级联分类器引擎在人脸区域进行人眼识别，返回的eyes为眼睛坐标列表</span></span><br><span class="line">    <span class="keyword">for</span> (ex,ey,ew,eh) <span class="keyword">in</span> eyes:</span><br><span class="line">        <span class="comment">#画出人眼框，绿色，画笔宽度为1</span></span><br><span class="line">        cv2.rectangle(face_area,(ex,ey),(ex+ew,ey+eh),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在"img2"窗口中展示效果图</span></span><br><span class="line">cv2.imshow(<span class="string">'img2'</span>,img)</span><br><span class="line"><span class="comment"># 监听键盘上任何按键，如有案件即退出并关闭窗口，并将图片保存为output.jpg</span></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">cv2.imwrite(<span class="string">'output.jpg'</span>,img)</span><br></pre></td></tr></table></figure>
<p>上面的代码最值得注意的就是face_area = img[y:y+h, x:x+w]，这一步会将人脸区域框出来，在其中执行人眼检测。同样对上图进行检测，结果如下：</p>
<p><img src="/posts/CV/output.jpg" alt></p>
<p>结果还行吧。</p>
<h2 id="调用电脑摄像头进行实时人脸识别和人眼识别"><a href="#调用电脑摄像头进行实时人脸识别和人眼识别" class="headerlink" title="调用电脑摄像头进行实时人脸识别和人眼识别"></a>调用电脑摄像头进行实时人脸识别和人眼识别</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="string">'haarcascade_frontalface_default.xml'</span>)</span><br><span class="line"></span><br><span class="line">eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="string">'haarcascade_eye.xml'</span>)</span><br><span class="line"><span class="comment"># 调用摄像头摄像头</span></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">    <span class="comment"># 获取摄像头拍摄到的画面</span></span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    faces = face_cascade.detectMultiScale(frame, <span class="number">1.3</span>, <span class="number">5</span>)</span><br><span class="line">    img = frame</span><br><span class="line">    <span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    	<span class="comment"># 画出人脸框，蓝色，画笔宽度微</span></span><br><span class="line">        img = cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    	<span class="comment"># 框选出人脸区域，在人脸区域而不是全图中进行人眼检测，节省计算资源</span></span><br><span class="line">        face_area = img[y:y+h, x:x+w]</span><br><span class="line">        eyes = eye_cascade.detectMultiScale(face_area)</span><br><span class="line">    	<span class="comment"># 用人眼级联分类器引擎在人脸区域进行人眼识别，返回的eyes为眼睛坐标列表</span></span><br><span class="line">        <span class="keyword">for</span> (ex,ey,ew,eh) <span class="keyword">in</span> eyes:</span><br><span class="line">            <span class="comment">#画出人眼框，绿色，画笔宽度为1</span></span><br><span class="line">            cv2.rectangle(face_area,(ex,ey),(ex+ew,ey+eh),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 实时展示效果画面</span></span><br><span class="line">    cv2.imshow(<span class="string">'frame2'</span>,img)</span><br><span class="line">    <span class="comment"># 每5毫秒监听一次键盘动作</span></span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">5</span>) &amp; <span class="number">0xFF</span> == ord(<span class="string">'q'</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，关闭所有窗口</span></span><br><span class="line">cap.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>以上我们都可以通过调整参数调整精度要求。</p>
<h1 id="Haar原理解析"><a href="#Haar原理解析" class="headerlink" title="Haar原理解析"></a>Haar原理解析</h1><h2 id="Haar特征"><a href="#Haar特征" class="headerlink" title="Haar特征"></a>Haar特征</h2><p>Haar特征包含三种：边缘特征、线性特征、中心特征和对角线特征。每种分类器都从图片中提取出对应的特征。有点类似于卷积神经网络中的卷积核，每个卷积核提取出对应的特征。</p>
<p>我们最基础的卷积核（划掉），哦不haar特征为下图中的Basic Haar Set：</p>
<p><img src="/posts/CV/haar2.jpg" alt></p>
<p>我们通常将Haar特征分为以下三类，我们根据名字就可以分辨出这三类的用途:</p>
<ul>
<li><p>第一类是边缘特征：<br><img src="/posts/CV/haar3.png" alt></p>
</li>
<li><p>第二类是线性特征：<br><img src="/posts/CV/haar4.png" alt></p>
</li>
<li><p>第三类是中心特征：<br><img src="/posts/CV/haar5.jpg" alt></p>
</li>
</ul>
<p>特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。</p>
<p>例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。但矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向（水平、垂直、对角）的结构。由于有时候人脸未必是定向的，可能是会有歪曲的，因此我们可以训练旋转一定角度的矩形特征来识别人脸。</p>
<p><img src="/posts/CV/haar6.png" alt></p>
<p>总而言之，Haar特征就是利用一些固定的特征来模拟人脸中的相关特征。</p>
<p>矩形特征可位于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征，如：在24*24像素大小的检测窗口内矩形特征数量可以达到16万个。这样就有两个问题需要解决了：</p>
<p>（1）如何快速计算那么多的特征？—-积分图大显神通；</p>
<p>（2）哪些矩形特征才是对分类器分类最有效的？—-如通过AdaBoost算法来训练。</p>
<h2 id="积分图构建"><a href="#积分图构建" class="headerlink" title="积分图构建"></a>积分图构建</h2><p>在一个图像窗口中，可以提取出大量的Haar矩形特征区域，如果在计算Haar特征值时，每次都遍历矩形特征区域，将会造成大量重复计算，严重浪费时间。积分图是一种快速计算矩形特征的方法，主要思想是将图像起始像素点到每一个像素点之间所形成的矩形区域的像素值的和，作为一个元素保存下来，即将原始图像转换为积分图(或者求和图)，当求某一矩形区域的像素和时，只需要索引矩形区域4个角点在积分图中的取值，进行普通的加减运算，即可求得Haar特征值，整个过程只需遍历一次图像，计算特征的时间复杂度为常数(O(1))(O(1))，可以大大提升计算效率。<br>积分图中元素的公式定义如下：</p>
<script type="math/tex; mode=display">ii(x,y) = \sum_{k\le x,l\le y}f(k,l)</script><p>上式含义是在$(x,y)$位置处，积分图中元素为原图像中对应像素左上角所有像素值之和，$ii(x,y)$表示一个积分图像。在具体实现时，可用下式进行迭代运算:</p>
<script type="math/tex; mode=display">s(x,y) = s(x,y-1)+i(x,y)</script><script type="math/tex; mode=display">ii(x,y) = ii(x-1,y)+s(x,y)</script><p>其中$s$是行方向的累加和，初始值$s(x,-1)=0,ii(-1,y)=0$，但这个公式不是很好（为什么？），一个比较好的替代是下面这个公式：</p>
<script type="math/tex; mode=display">ii(x,y) = ii(x-1,y)+ii(x,y-1)+f(x,y)-ii(i-1,j-1)</script><h2 id="计算Haar特征值"><a href="#计算Haar特征值" class="headerlink" title="计算Haar特征值"></a>计算Haar特征值</h2><h3 id="矩形特征"><a href="#矩形特征" class="headerlink" title="矩形特征"></a>矩形特征</h3><p>构建好积分图后，图像中任何矩形区域的像素值累加和都可以通过简单的加减运算快速得到，如下图所示，矩形区域D的像素和值计算公式如下：</p>
<p>$\operatorname{Sum}(D)=i i\left(x_{4}, y_{4}\right)-i i\left(x_{2}, y_{2}\right)-i i\left(x_{3}, y_{3}\right)+i i\left(x_{1}, y_{1}\right)$</p>
<p><img src="/posts/CV/haar7.png" alt></p>
<p>矩形区域求和示意图 在下图中，以水平向右为$x$轴正方向，垂直向下为$y$轴正方向，可定义积分图公式Summed Area Table$(S A T(x, y))$</p>
<script type="math/tex; mode=display">
S A T(x, y)=\Sigma_{x^{\prime} \leq x, y^{\prime} \leq y} i\left(x^{\prime}, y^{\prime}\right)</script><p>以及迭代求解式</p>
<script type="math/tex; mode=display">
\begin{array}{c}
S A T(x, y)=S A T(x, y-1)+S A T(x-1, y)-S A T(x-1, y-1)+I(x, y) \\
S A T(-1, y)=0, S A T(x,-1)=0
\end{array}</script><p>对于左上角坐标为$(x, y),$宽高为$(w, h)$的矩形区域$r(x, y, w, h, 0),$可利用积分图$S A T(x, y)$求取像素和值</p>
<script type="math/tex; mode=display">
\operatorname{RecSum}(r)=S A T(x+w-1, y+h-1)+S A T(x-1, y-1)-S A T(x+w-1, y-1)-S A T(x-1, y+h-1)</script><p><img src="/posts/CV/haar8.png" alt></p>
<h3 id="旋转矩形特征"><a href="#旋转矩形特征" class="headerlink" title="旋转矩形特征"></a>旋转矩形特征</h3><p>对于旋转矩形特征，相应的有$45$°倾斜积分图用于快速计算Haar特征值，如下图所示，倾斜积分图的定义为像素点左上角$45$°区域和左下角$45$°区域的像素和，公式表示如下：</p>
<script type="math/tex; mode=display">RSAT(x, y)=\Sigma_{x^{\prime} \leq x, x^{\prime} \leq x-\left|y-y^{\prime}\right|} i\left(x^{\prime}, y^{\prime}\right)</script><p>其递推公式计算如下：</p>
<script type="math/tex; mode=display">\begin{array}{c}
R S A T(x, y)=R S A T(x-1, y-1)+R S A T(x-1, y)-R S A T(x-2, y-1)+I(x, y) \\
R S A T(x, y)=R S A T(x, y)+R S A T(x-1, y+1)-R S A T(x-2, y) \\
\end{array}</script><p>其中$R S A T(-1, y)=R S A T(-2, y)=R S A T(x,-1)=0$</p>
<p>也可直接通过下式递归计算:</p>
<script type="math/tex; mode=display">R S A T(x, y)=R S A T(x-1, y-1)+R S A T(x+1, y-1)-R S A T(x, y-2)+I(x-1, y)+I(x, y)</script><p>以上3个积分图计算公式是等价的。</p>
<p>如下图所示，构建好倾斜积分图后，可快速计算倾斜矩形区域r$=\left(x, y, w, h, 45^{\circ}\right)$的像素和值</p>
<p><img src="/posts/CV/haar9.png" alt></p>
<script type="math/tex; mode=display">\operatorname{RecSum}(r)=R S A T(x+w, y+w)+R S A T(x-h, y+h)-R S A T(x, y)-R S A T(x+w-h, y+w+h)</script><h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><p>了解了特征值的计算之后，我们来看看不同的特征值的含义是什么。我们选取MIT人脸库中2706个大小为$20*20$的人脸正样本图像，计算如下图所示的Haar特征：</p>
<p><img src="/posts/CV/haar10.jpg" alt></p>
<p>左边对应的人眼区域，右边无具体意义。</p>
<p><img src="/posts/CV/harr11.png" alt></p>
<p>可以看到，图中2个不同Haar特征在同一组样本中具有不同的特征值分布，左边特征计算出的特征值基本都大于0（对样本的区分度大），而右边特征的特征值基本均匀分布于0两侧（对样本的区分度小）。所以，正是由于样本中Haar特征值分布不均匀，导致了不同Haar特征分类效果不同。显而易见，对正负样本区分度越大的特征分类效果越好，即红色曲线对应图中的的左边Haar特征分类效果好于右边Haar特征。</p>
<p>那么看到这里，应该理解了下面2个问题：</p>
<ol>
<li>在检测窗口通过平移+缩放可以产生一系列Haar特征，这些特征由于位置和大小不同，分类效果也不同；</li>
<li>通过计算Haar特征的特征值，可以有将图像矩阵映射为1维特征值，有效实现了降维。</li>
</ol>
<h2 id="Haar特征归一化"><a href="#Haar特征归一化" class="headerlink" title="Haar特征归一化"></a>Haar特征归一化</h2><p>从上图我们可以发现，仅仅一个$128$维大小的Haar特征计算出的特征值变化范围从$-2000~+6000$，跨度非常大。这种跨度大的特性不利于量化评定特征值，所以需要进行“归一化”，压缩特征值范围。假设当前检测窗口中的图像像素为$i(x,y)$，当前检测窗口为$w∗h$大小（例如上图中为2020大小），OpenCV采用如下方式“归一化”：</p>
<ol>
<li>计算检测窗口中图像的灰度值和灰度值平方和：$sum=\sum i(x,y)$</li>
</ol>
<script type="math/tex; mode=display">sq_{sum}=\sum i^2(x,y)</script><ol>
<li>计算平均值：</li>
</ol>
<script type="math/tex; mode=display">mean = \frac{sum}{w*h}</script><script type="math/tex; mode=display">sq_{mean}=\frac{sq_{sum}}{w*h}</script><ol>
<li><p>计算归一化因子：</p>
<script type="math/tex; mode=display">varNormFactor=\sqrt{sq_{mean}-mean^2}</script></li>
<li><p>归一化特征值：$normValue=\frac{featureValue}{varNormFactor}$之后使用归一化的特征值$normValue$与阈值对比。</p>
</li>
</ol>
<h2 id="级联"><a href="#级联" class="headerlink" title="级联"></a>级联</h2><h3 id="白话解释"><a href="#白话解释" class="headerlink" title="白话解释"></a>白话解释</h3><p>这里我们并不打算详细阐述AdaBoost的完整工作机制以及一些更细节的部分，我们将从宏观层面来看级联的流程。</p>
<p><strong>基于Haar特征的cascade级联分类器</strong>是Paul Viola和 Michael Jone在2001年的论文”Rapid Object Detection using a Boosted Cascade of Simple Features”中提出的一种有效的物体检测方法。</p>
<p><strong>Cascade级联分类器的训练方法：Adaboost</strong></p>
<p>级联分类器的函数是通过大量带人脸和不带人脸的图片通过机器学习得到的。对于人脸识别来说，需要几万个特征，通过机器学习找出人脸分类效果最好、错误率最小的特征。训练开始时，所有训练集中的图片具有相同的权重，对于被分类错误的图片，提升权重，重新计算出新的错误率和新的权重。直到错误率或迭代次数达到要求。这种方法叫做Adaboost，在Opencv中可以直接调用级联分类器函数。</p>
<p><strong>将弱分类器聚合成强分类器</strong></p>
<p>最终的分类器是这些弱分类器的加权和。之所以称之为弱分类器是因为每个分类器不能单独分类图片，但是将他们聚集起来就形成了强分类器。论文表明，只需要200个特征的分类器在检测中的精确度达到了95%。最终的分类器大约有6000个特征。(将超过160000个特征减小到6000个，这是非常大的进步了）。</p>
<p><strong>级联的含义：需过五关斩六将才能被提取出来</strong></p>
<p>事实上，一张图片绝大部分的区域都不是人脸。如果对一张图片的每个角落都提取6000个特征，将会浪费巨量的计算资源。</p>
<p>如果能找到一个简单的方法能够检测某个窗口是不是人脸区域，如果该窗口不是人脸区域，那么就只看一眼便直接跳过，也就不用进行后续处理了，这样就能集中精力判别那些可能是人脸的区域。 为此，有人引入了Cascade 分类器。它不是将6000个特征都用在一个窗口，而是将特征分为不同的阶段，然后一个阶段一个阶段的应用这些特征(通常情况下，前几个阶段只有很少量的特征)。如果窗口在第一个阶段就检测失败了，那么就直接舍弃它，无需考虑剩下的特征。如果检测通过，则考虑第二阶段的特征并继续处理。如果所有阶段的都通过了，那么这个窗口就是人脸区域。 作者的检测器将6000+的特征分为了38个阶段，前五个阶段分别有1，10，25，25，50个特征(前文图中提到的识别眼睛和鼻梁的两个特征实际上是Adaboost中得到的最好的两个特征)。根据作者所述，平均每个子窗口只需要使用6000+个特征中的10个左右。</p>
<p>简单地说，在进行人脸检测的过程中，需要使用一个强分类器，且其由多个弱分类器组成。那么其中的每个弱分类器都只包含一个Haar特征。每个分类器都将确定一个阈值，如果某区域的处理差值小于该阈值，则被归为负类，反之则进行下一级的弱分类，最终经过多个弱分类器后，可完成检测。其分类过程如下图所示：</p>
<p><img src="/posts/CV/haar13.png" alt></p>
<h3 id="举个例子-1"><a href="#举个例子-1" class="headerlink" title="举个例子"></a>举个例子</h3><p><img src="/posts/CV/haar12.png" alt></p>
<ol>
<li>首先，对于一幅图像，它可能存在K个面部特征，假设这些面部特征可以用来区分眼睛、眉毛、鼻子、嘴等特征。</li>
<li>确定一些超参数，如滑动窗口的大小，及窗口的移动步长。窗口从上往下，从左向右地滑动。在滑动的过程中，每次都可以计算出一个数值$K$。</li>
<li>滑动结束时，将得到的特征值进行排序，并选取一个最佳特征值（最优阈值），使得在该特征值下，对于该特征而言，样本的加权错误率最低。这样就训练出了一个弱分类器。</li>
<li>因为面部特征的不同，我们将采用不同的滑动窗口进行特征提取。所以根据不同的窗口识别不同的特征，进而训练出了不同的弱分类器。</li>
<li>对于每个弱分类器都将计算它的错误率，选择错误率最低的K个弱分类器，组合成强分类器。</li>
<li>一组样本投入强分类器后，在每个渐进的阶段，分类器逐渐在较少的图像窗口上使用更多的特征（负类被丢弃）。如果某个矩形区域在所有弱分类器中都被归结为正类，那么可以认为该区域是存在人脸的。</li>
</ol>
<p>其中，弱分类器训练的具体步骤如下：</p>
<ol>
<li>对于每个特征$f$，计算所有训练样本的特征值，并将其排序：</li>
<li>扫描一遍排好序的特征值，对排好序的表中的每个元素，计算下面四个值：<ul>
<li>计算全部正例的权重和$T^+$；</li>
<li>计算全部负例的权重和$T^-$；</li>
<li>计算该元素前之前的正例的权重和$S^+$；</li>
<li>计算该元素前之前的负例的权重和$S^-$</li>
</ul>
</li>
<li>选取当前元素的特征值$F_{k,j}$和它前面的一个特征值$F_{k,j−1}$之间的数作为阈值，所得到的弱分类器就在当前元素处把样本分开 —— 也就是说这个阈值对应的弱分类器将当前元素前的所有元素分为人脸（或非人脸），而把当前元素后（含）的所有元素分为非人脸（或人脸）。该阈值的分类误差为：</li>
</ol>
<script type="math/tex; mode=display">e=min(S^++(T^−−S^−),S^−+(T^+−S^+))</script><p>于是，通过把这个排序表从头到尾扫描一遍就可以为弱分类器选择使分类误差最小的阈值（最优阈值），也就是选取了一个最佳弱分类器。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h3><p>从上面所述内容我们可以总结Haar分类器训练的五大步骤：</p>
<ol>
<li>准备人脸、非人脸样本集；</li>
<li>使用Haar特征做检测；</li>
<li>使用积分图（Integral Image）对Haar特征求值进行加速；</li>
<li>使用AdaBoost算法训练区分人脸和非人脸的强分类器；</li>
<li>使用筛选式级联把强分类器级联到一起，提高准确率</li>
</ol>
<h3 id="Haar的局限性"><a href="#Haar的局限性" class="headerlink" title="Haar的局限性"></a>Haar的局限性</h3><ul>
<li>仅为人脸检测，非人脸“辩识”，即只能框出人脸的位置，看不出人脸是谁。</li>
<li>仅能标出静态图片和视频帧上的人脸、人眼和微笑，不能进行“活体识别”，即不能看出这张脸是真人还是手机上的照片，如果用于人脸打卡签到、人脸支付的话会带来潜在的安全风险。</li>
<li>仅为普通的机器学习方法，没有用到深度学习和深层神经网络。</li>
</ul>
<blockquote>
<p>参考链接：</p>
</blockquote>
<ul>
<li><a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html#face-detection" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html#face-detection</a></li>
<li><a href="https://github.com/TommyZihao/zihaoopencv/blob/master/%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%B8%8E%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E3%80%90%E5%AD%90%E8%B1%AA%E5%85%84opencv-python%E6%95%99%E7%A8%8B%E3%80%91.md" target="_blank" rel="noopener">https://github.com/TommyZihao/zihaoopencv/blob/master/%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%B8%8E%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E3%80%90%E5%AD%90%E8%B1%AA%E5%85%84opencv-python%E6%95%99%E7%A8%8B%E3%80%91.md</a></li>
<li><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%8B%EF%BC%89/Task03%20Haar%E7%89%B9%E5%BE%81%E6%8F%8F%E8%BF%B0%E7%AE%97%E5%AD%90.md" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%8B%EF%BC%89/Task03%20Haar%E7%89%B9%E5%BE%81%E6%8F%8F%E8%BF%B0%E7%AE%97%E5%AD%90.md</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/100217697" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/100217697</a></li>
<li><a href="https://blog.csdn.net/chutu2018/article/details/106983147/" target="_blank" rel="noopener">https://blog.csdn.net/chutu2018/article/details/106983147/</a></li>
<li><a href="https://blog.csdn.net/playezio/article/details/80471000" target="_blank" rel="noopener">https://blog.csdn.net/playezio/article/details/80471000</a></li>
<li><a href="https://www.cnblogs.com/recoverableTi/p/13214405.html" target="_blank" rel="noopener">https://www.cnblogs.com/recoverableTi/p/13214405.html</a></li>
</ul>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>LBP特征描述算子-人脸检测</title>
    <url>/posts/2b071c91.html</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>LBP（Local Binary Pattern，局部二值模式）特征是一种用来描述图像局部纹理特征的算子，它具有旋转不变性和灰度不变性的优点，用于图像局部纹理特征提取。</p>
<p><strong>注意初基本LBP没有旋转不变性</strong></p>
<h2 id="基本LBP"><a href="#基本LBP" class="headerlink" title="基本LBP"></a>基本LBP</h2><p>基本LBP十分简单，我们对原图像的每一个像素点（边界除外）可以抽取出以之为中心的3×3矩阵，为了计算中间点的LBP，我们用除了中间点此外的8个点依次与中间点比较，比它(也就是15)大的记成1，比它小的记成0，如下图所示：</p>
<p><img src="/posts/CV/lbp.png" alt></p>
<p>用公式可以表示成：</p>
<script type="math/tex; mode=display">LBP(x_c,y_c) = \sum^{p-1}_{p=0}2^ps(i_p-i_c)</script><p>其中$(x_c,y_c)$是中心像素，$i_c$是中心像素灰度值，$i_n$是相邻像素的灰度值，$s$是一个符号函数：</p>
<script type="math/tex; mode=display">s(x) = \left\{
\begin{aligned}
1 ,x \ge 0\\
0 ,x \lt 0
\end{aligned}
\right.</script><h2 id="圆形LBP算子"><a href="#圆形LBP算子" class="headerlink" title="圆形LBP算子"></a>圆形LBP算子</h2><p>原始LBP算子计算区域为像素点的周围8个点，在图像尺寸和频率纹理发生改变时会出现很大的偏差，不能正确反映像素点周围的纹理信息。为适应不同尺寸纹理特征，LBP原作者将圆形邻域代替正方形邻域。同时<strong>增加了旋转不变的特性，在对LBP特征值的存储部分，也进行了改进</strong>。</p>
<p>改进后的LBP算子长成这个样：</p>
<p><img src="/posts/CV/lbp2.jpg" alt></p>
<p>上面是不同半径与采样点数量的LBP算子</p>
<p>我们将领域由正方形变为圆形后，面对的第一个问题就是如何获得采样点的坐标，获得方法为：</p>
<script type="math/tex; mode=display">x_t = x_d + R\cos(2\pi\frac{p}{P})</script><script type="math/tex; mode=display">y_t = y_d - R\sin(2\pi\frac{p}{P})</script><p>看到上面这个简单的取圆周上的点的函数，我们又想到，事实上实际操作时得到的坐标未必是整数，也就无从单论这个点的像素值，因此我们需要用一个方法利用取点周围的值来定义获得的非整数坐标的像素值，在这里我们使用<strong>双线性插值法</strong>（当然我们可以使用其他插值法，不过opencv使用的是双线性插值）。</p>
<p>双线性插值又是什么意思呢？说白了双线性插值几乎就是一个线性插值，只不过是有两个变量的插值函数的线性插值扩展，其核心思想是在两个方向分别进行一次线性插值，如下：（下述一维插值均使用拉格朗日插值法表示）</p>
<p><img src="/posts/CV/xxcz.jpg" alt></p>
<ul>
<li>第一步：$X$方向的线性插值，在$Q_{12},Q_{22}$中插入蓝色点$R2，Q11，Q21$中插入蓝色点$R1$；</li>
<li>第二步：$Y$方向的线性插值 ,通过第一步计算出的$R1$与$R2$在$y$方向上插值计算出$P$点。</li>
</ul>
<p>首先由于线性插值的结果与插值的顺序无关，同时首先进行$Y$方向的插值，然后进行$X$方向的插值，所得到的结果是一样的，因此<strong>双线性插值的结果与先进行哪个方向的插值无关</strong>。如果选择一个坐标系统使得四个已知点坐标分别为$(0, 0),(0, 1),(1, 0)$和$(1, 1)$，那么插值公式就可以化简为</p>
<script type="math/tex; mode=display">f(x,y)=f(0,0)(1-x)(1-y)+f(1,0)x(1-y)+f(0,1)(1-x)y+f(1,1)xy</script><p>表示成矩阵形式即为：</p>
<script type="math/tex; mode=display">f(x,y)=\left[ \begin{matrix} 1-x & x \end{matrix} \right ] \left[ \begin{matrix} f(0,0) & f(0,1\\f(1,0) & f(1,1) \end{matrix} \right ]\left[ \begin{matrix} 1-y\\y \end{matrix} \right ]</script><p>我们看到右边项其实正是二维泰勒公式的前几项，而由已知的一维拉格朗日线性插值法的展开式可知，上述等式实际上就是<strong>拉格朗日插值法的二维形式</strong>。</p>
<p>实际上到目前为止，我们得到的LBP方法仍然不具有旋转不变性，这也很好理解，我们只是取了一个圆周而已，当圆周上旋转（起始选点位置发生变化）LBP当然也会发生变化，那我们要做什么才能让一个圆周确定唯一一个LBP算子呢（<strong>也就是说对于同一个圆周需要有一个固定的顺序</strong>），我们可以对圆上所有点遍历，将遍历到的点作为初始点计算圆周的LBP，然后选取其中LBP最小的特征值作为LBP中心像素点的特征值。用数学表示为：</p>
<script type="math/tex; mode=display">LBP_{P、R}^{ri} = \min \{ ROR(LBP_{P、R}^i) \}</script><p>其中$ROR（x，i）$执顺时针方向将$P$位数$x$移动$i$次。对于图像像素而言，就是将邻域集合按照时钟方向旋转很多次，直到当前旋转下构成的LBP值最小。如下图所示：</p>
<p><img src="/posts/CV/lbp3.jpg" alt></p>
<p>问题来了，我们为什么要选择最小的，选最大的或者选中位数、平均数它不香吗？我也不知道hhhh，大家可以看看下面参考链接的原论文（我看了看似乎也没有讲），若有新的想法欢迎在下面留言交流~</p>
<p>如下图所示，对于8个采样点，将有36种唯一的旋转不变二值模式：【黑点为0，白点为1】</p>
<p><img src="/posts/CV/lbp4.png" alt></p>
<h2 id="LBP等价模式（ULBP）"><a href="#LBP等价模式（ULBP）" class="headerlink" title="LBP等价模式（ULBP）"></a>LBP等价模式（ULBP）</h2><p>基于上面的讨论：<strong>对于8个采样点，灰度不变性LBP（基本LBP）将产生256种输出，旋转不变性LBP将产生36个输出，而基于unifrom的旋转不变LBP将只有9中输出</strong>。第一个是由于中心像素附近有八个像素（有八个可以填0,1的位置），因此共有$2^8=256$种输出，旋转不变性LBP已于上面一一列举，基于uniform的旋转不变性LBP将于下面叙述：</p>
<p>一个LBP算子可以产生不同的二进制模式，对于半径为$R$的圆形区域内含有$P$个采样点的LBP算子将会产生$P^2$种模式。很显然，随着邻域集内采样点数的增加，二进制模式的种类是急剧增加的。例如：$5×5$邻域内（基本LBP）$20$个采样点，有$2^20＝1,048,576$种二进制模式。如此多的二值模式无论对于纹理的提取还是对于纹理的识别、分类及信息的存取都是不利的。同时，过多的模式种类对于纹理的表达是不利的。例如，将LBP算子用于纹理分类或人脸识别时，常采用LBP模式的统计直方图来表达图像的信息，而较多的模式种类将使得数据量过大，且直方图过于稀疏。因此，需要对原始的LBP模式进行降维，使得数据量减少的情况下能最好的代表图像的信息。</p>
<p>为了解决二进制模式过多的问题，提高统计性，Ojala提出了采用一种“等价模式”（Uniform Pattern）来对LBP算子的模式种类进行降维。Ojala等认为，在实际图像中，绝大多数LBP模式最多只包含两次从1到0或从0到1的跳变。因此，Ojala将“等价模式”定义为：当某个LBP所对应的循环二进制数从0到1或从1到0最多有两次跳变时，该LBP所对应的二进制就称为一个等价模式类。如00000000（0次跳变），00000111（只含一次从0到1的跳变），10001111（先由1跳到0，再由0跳到1，共两次跳变）都是等价模式类。除等价模式类以外的模式都归为另一类，称为混合模式类，例如10010111（共四次跳变）</p>
<p>通过这样的改进，二进制模式的种类大大减少，而不会丢失任何信息。模式数量由原来的2P种减少为$P( P-1)+2+1$种，其中$P$表示邻域集内的采样点数。对于$3×3$邻域内$8$个采样点来说，二进制模式由原始的$256$种减少为$58$种，这使得特征向量的维数更少，并且可以减少高频噪声带来的影响。</p>
<p>举个例子：采样点数8个，即256个LBP特征值，分成59类：跳变0次——2个，跳变1次——0个，跳变2次——56个，跳变3次——0个，跳变4次——140个，跳变5次——0个，跳变6次——56个，跳变7次——0个，跳变8次——2个。</p>
<p>另外LBP还有许多其他变形，比如MB-LBP（Multiscale Block LBP）特征等，就不在此一一阐述。</p>
<h1 id="LBP检测原理"><a href="#LBP检测原理" class="headerlink" title="LBP检测原理"></a>LBP检测原理</h1><h2 id="LBP特征统计直方图"><a href="#LBP特征统计直方图" class="headerlink" title="LBP特征统计直方图"></a>LBP特征统计直方图</h2><p>显而易见的是，上述提取的LBP算子在每个像素点都可以得到一个LBP“编码”，那么，对一幅图像（记录的是每个像素点的灰度值）提取其原始的LBP算子之后，得到的原始LBP特征依然是“一幅图片”（记录的是每个像素点的LBP值）。</p>
<p>在LBP的应用中，如纹理分类、人脸分析等，一般都不将LBP图谱作为特征向量用于分类识别，而是<strong>采用LBP特征谱的统计直方图作为特征向量用于分类识别</strong>。这是因为从上面的分析我们可以看出，这个“特征”跟位置信息是紧密相关的。直接对两幅图片提取这种“特征”，并进行判别分析的话，会因为“位置没有对准”而产生很大的误差。后来，研究人员发现，可以<strong>将一幅图片划分为若干的子区域</strong>，对每个子区域内的每个像素点都提取LBP特征，然后，在每个子区域内建立LBP特征的统计直方图。如此一来，<strong>每个子区域，就可以用一个统计直方图来进行描述</strong>；整个图片就由若干个统计直方图组成；</p>
<p>例如：一幅$100<em>100$像素大小的图片，划分为$10</em>10=100$个子区域（可以通过多种方式来划分区域），每个子区域的大小为$10<em>10$像素；在每个子区域内的每个像素点，提取其LBP特征，然后，建立统计直方图；这样，这幅图片就有$10</em>10$个子区域，也就有了$10<em>10$个统计直方图，利用这$10</em>10$个统计直方图，就可以描述这幅图片了。之后，我们利用各种相似性度量函数，就可以判断两幅图像之间的相似性了；</p>
<h2 id="对LBP特征向量进行提取的步骤"><a href="#对LBP特征向量进行提取的步骤" class="headerlink" title="对LBP特征向量进行提取的步骤"></a>对LBP特征向量进行提取的步骤</h2><ol>
<li>计算图像的LBP特征图像（对每个像素点获得一个LBP）</li>
<li>将LBP特征图像进行分块，Opencv中默认将LBP特征图像分成8行8列64块区域</li>
<li>计算每块区域特征图像的直方图cell_LBPH，将直方图进行归一化，直方图大小为$1*numPatterns$</li>
<li>将上面计算的每块区域特征图像的直方图按分块的空间顺序依次排列成一行，形成LBP特征向量，大小为$1<em>（numPatterns</em>64）$</li>
<li>用机器学习的方法对LBP特征向量进行训练，用来检测和识别目标</li>
</ol>
<p>举例说明LBPH的维度：采样点为8个，如果用的是原始的LBP或Extended LBP特征，其LBP特征值的模式为256种，则一幅图像的LBP特征向量维度为：$64<em>256=16384$维，而如果使用的UniformPatternLBP特征，其LBP值的模式为59种，其特征向量维度为：$64</em>59=3776$维，可以看出，使用等价模式特征，其特征向量的维度大大减少，这意味着使用机器学习方法进行学习的时间将大大减少，而性能上没有受到很大影响。</p>
<h1 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h1><h2 id="人脸检测"><a href="#人脸检测" class="headerlink" title="人脸检测"></a>人脸检测</h2><p>首先我们导入周杰伦的图片：</p>
<p><img src="/posts/CV/jay.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">img= cv.imread(<span class="string">'Pic/jay.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img, code=cv.COLOR_BGR2GRAY)</span><br><span class="line">face_detect = cv.CascadeClassifier(<span class="string">"lbpcascade_frontalface_improved.xml"</span>)</span><br></pre></td></tr></table></figure>
<p>上面几行代码是导入相关包，导入图片，转换灰度图，欸第四行是什么东西？第四行是一个训练模型的xml文件，下载地址为：<a href="https://github.com/opencv/opencv/blob/master/data/lbpcascades/lbpcascade_frontalface_improved.xml" target="_blank" rel="noopener">戳这里</a></p>
<p>这个文件（模型）内容为：</p>
<p><img src="/posts/CV/train.png" alt></p>
<p>咱先不管它是个啥，直接用就vans~（不然的话还得跑一堆训练数据从头训练，再问就是懒）</p>
<p>下面两行代码会调用前面训练好的模型进行人脸检测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 检查人脸 按照1.1倍放到 周围最小像素为5</span></span><br><span class="line">face_zone = face_detect.detectMultiScale(gray, scaleFactor = <span class="number">2</span>, minNeighbors = <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'识别人脸的信息：\n'</span>,face_zone)</span><br></pre></td></tr></table></figure>
<p>要理解上面传入的参数，就得看一看对应的源代码定义部分：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cv::CascadeClassifier::detectMultiScale(</span><br><span class="line">    <span class="keyword">const</span> cv::Mat&amp; image, <span class="comment">// 输入待检测的图像（灰度）</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;cv::Rect&gt;&amp; objects, <span class="comment">// 输出的目标窗口</span></span><br><span class="line">    <span class="keyword">double</span> scaleFactor = <span class="number">1.1</span>, <span class="comment">// 尺度系数</span></span><br><span class="line">    <span class="keyword">int</span> minNeighbors = <span class="number">3</span>, <span class="comment">// 需要的邻域数</span></span><br><span class="line">    <span class="keyword">int</span> flags = <span class="number">0</span>, <span class="comment">// flag (旧风格的cascades)</span></span><br><span class="line">    cv::Size minSize = cv::Size(), <span class="comment">// 最小检测窗口</span></span><br><span class="line">    cv::Size maxSize = cv::Size() <span class="comment">// 最大检测窗口</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>需要的邻域数我的理解就是中心点依赖的周围的像素点的半径（有不妥请指正）</p>
<p>下一步就是绘制检测框框：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制矩形和圆形检测人脸</span></span><br><span class="line"><span class="keyword">for</span> x, y, w, h <span class="keyword">in</span> face_zone:</span><br><span class="line">    <span class="comment"># 绘制矩形人脸区域</span></span><br><span class="line">    cv.rectangle(img, pt1 = (x, y), pt2 = (x+w, y+h), color = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>], thickness=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 绘制圆形人脸区域 radius表示半径</span></span><br><span class="line">    cv.circle(img, center = (x + w//<span class="number">2</span>, y + h//<span class="number">2</span>), radius = w//<span class="number">2</span>, color = [<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>], thickness = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>然后就是一些附加的通用操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置图片可以手动调节大小</span></span><br><span class="line">cv.namedWindow(<span class="string">"Easmount-CSDN"</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">cv.imshow(<span class="string">"Easmount-CSDN"</span>, img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待显示 设置任意键退出程序</span></span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>我们看一看结果长啥样：</p>
<p><img src="/posts/CV/jayoutput.png" alt></p>
<h2 id="LBP源码分析"><a href="#LBP源码分析" class="headerlink" title="LBP源码分析"></a>LBP源码分析</h2><p><strong>注：下面的源码都是使用Cpp实现（直接copy的），内容较多，可略过不看</strong></p>
<p>下面是原始LBP的旧版源码：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LBP</span> <span class="params">(IplImage *src,IplImage *dst)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tmp[<span class="number">8</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    CvScalar s;</span><br><span class="line"></span><br><span class="line">    IplImage * temp = cvCreateImage(cvGetSize(src), IPL_DEPTH_8U,<span class="number">1</span>);</span><br><span class="line">    uchar *data=(uchar*)src-&gt;imageData;</span><br><span class="line">    <span class="keyword">int</span> step=src-&gt;widthStep;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"step"</span>&lt;&lt;step&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;src-&gt;height<span class="number">-1</span>;i++)</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;src-&gt;width<span class="number">-1</span>;j++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[(i<span class="number">-1</span>)*step+j<span class="number">-1</span>]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[i*step+(j<span class="number">-1</span>)]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">1</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[(i+<span class="number">1</span>)*step+(j<span class="number">-1</span>)]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">2</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">2</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span> (data[(i+<span class="number">1</span>)*step+j]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">3</span>]=<span class="number">1</span>;</span><br><span class="line">      	  <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">3</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span> (data[(i+<span class="number">1</span>)*step+(j+<span class="number">1</span>)]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">4</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">4</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[i*step+(j+<span class="number">1</span>)]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">5</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">5</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[(i<span class="number">-1</span>)*step+(j+<span class="number">1</span>)]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">6</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">6</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span>(data[(i<span class="number">-1</span>)*step+j]&gt;data[i*step+j])</span><br><span class="line">            tmp[<span class="number">7</span>]=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            tmp[<span class="number">7</span>]=<span class="number">0</span>;</span><br><span class="line">          <span class="comment">//计算LBP编码</span></span><br><span class="line">            s.val[<span class="number">0</span>]=(tmp[<span class="number">0</span>]*<span class="number">1</span>+tmp[<span class="number">1</span>]*<span class="number">2</span>+tmp[<span class="number">2</span>]*<span class="number">4</span>+tmp[<span class="number">3</span>]*<span class="number">8</span>+tmp[<span class="number">4</span>]*<span class="number">16</span>+tmp[<span class="number">5</span>]*<span class="number">32</span>+tmp[<span class="number">6</span>]*<span class="number">64</span>+tmp[<span class="number">7</span>]*<span class="number">128</span>);</span><br><span class="line">        cvSet2D(dst,i,j,s);写入LBP图像</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码旧版LBP的实现代码，我们可以看见中间大段地逻辑都是在判断周围地像素点和中心像素点谁打谁小，其他代码逻辑也没有难度。</p>
<p>下面是原始LBP的新版源码(cpp)：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> _tp&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getOriginLBPFeature</span><span class="params">(InputArray _src,OutputArray _dst)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src = _src.getMat();</span><br><span class="line">    _dst.create(src.rows<span class="number">-2</span>,src.cols<span class="number">-2</span>,CV_8UC1);</span><br><span class="line">    Mat dst = _dst.getMat();</span><br><span class="line">    dst.setTo(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;src.rows<span class="number">-1</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;src.cols<span class="number">-1</span>;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            _tp center = src.at&lt;_tp&gt;(i,j);</span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> lbpCode = <span class="number">0</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i<span class="number">-1</span>,j<span class="number">-1</span>) &gt; center) &lt;&lt; <span class="number">7</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i<span class="number">-1</span>,j  ) &gt; center) &lt;&lt; <span class="number">6</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i<span class="number">-1</span>,j+<span class="number">1</span>) &gt; center) &lt;&lt; <span class="number">5</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i  ,j+<span class="number">1</span>) &gt; center) &lt;&lt; <span class="number">4</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i+<span class="number">1</span>,j+<span class="number">1</span>) &gt; center) &lt;&lt; <span class="number">3</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i+<span class="number">1</span>,j  ) &gt; center) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i+<span class="number">1</span>,j<span class="number">-1</span>) &gt; center) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            lbpCode |= (src.at&lt;_tp&gt;(i  ,j<span class="number">-1</span>) &gt; center) &lt;&lt; <span class="number">0</span>;</span><br><span class="line">            dst.at&lt;uchar&gt;(i<span class="number">-1</span>,j<span class="number">-1</span>) = lbpCode;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;python</span><br></pre></td></tr></table></figure>
<p>嗯…看起来也差不多，但代码好看多了hhh。</p>
<p>圆形LBP的源码：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> _tp&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getCircularLBPFeature</span><span class="params">(InputArray _src,OutputArray _dst,<span class="keyword">int</span> radius,<span class="keyword">int</span> neighbors)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src = _src.getMat();</span><br><span class="line">    <span class="comment">//LBP特征图像的行数和列数的计算要准确</span></span><br><span class="line">    _dst.create(src.rows<span class="number">-2</span>*radius,src.cols<span class="number">-2</span>*radius,CV_8UC1);</span><br><span class="line">    Mat dst = _dst.getMat();</span><br><span class="line">    dst.setTo(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">//循环处理每个像素</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=radius;i&lt;src.rows-radius;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=radius;j&lt;src.cols-radius;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//获得中心像素点的灰度值</span></span><br><span class="line">            _tp center = src.at&lt;_tp&gt;(i,j);</span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> lbpCode = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;neighbors;k++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//根据公式计算第k个采样点的坐标，这个地方可以优化，不必每次都进行计算radius*cos，radius*sin</span></span><br><span class="line">                <span class="keyword">float</span> x = i + <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">cos</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">                <span class="keyword">float</span> y = j - <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">sin</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">                <span class="comment">//根据取整结果进行双线性插值，得到第k个采样点的灰度值</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">//1.分别对x，y进行上下取整</span></span><br><span class="line">                <span class="keyword">int</span> x1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(x));</span><br><span class="line">                <span class="keyword">int</span> x2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(x));</span><br><span class="line">                <span class="keyword">int</span> y1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(y));</span><br><span class="line">                <span class="keyword">int</span> y2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(y));</span><br><span class="line"></span><br><span class="line">                <span class="comment">//2.计算四个点(x1,y1),(x1,y2),(x2,y1),(x2,y2)的权重</span></span><br><span class="line">                <span class="comment">//下面的权重计算方式有个问题，如果四个点都相等，则权重全为0，计算出来的插值为0</span></span><br><span class="line">                <span class="comment">//float w1 = (x2-x)*(y2-y); //(x1,y1)</span></span><br><span class="line">                <span class="comment">//float w2 = (x2-x)*(y-y1); //(x1,y2)</span></span><br><span class="line">                <span class="comment">//float w3 = (x-x1)*(y2-y); //(x2,y1)</span></span><br><span class="line">                <span class="comment">//float w4 = (x-x1)*(y-y1); //(x2,y2)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">//将坐标映射到0-1之间</span></span><br><span class="line">                <span class="keyword">float</span> tx = x - x1;</span><br><span class="line">                <span class="keyword">float</span> ty = y - y1;</span><br><span class="line">                <span class="comment">//根据0-1之间的x，y的权重计算公式计算权重</span></span><br><span class="line">                <span class="keyword">float</span> w1 = (<span class="number">1</span>-tx) * (<span class="number">1</span>-ty);</span><br><span class="line">                <span class="keyword">float</span> w2 =    tx  * (<span class="number">1</span>-ty);</span><br><span class="line">                <span class="keyword">float</span> w3 = (<span class="number">1</span>-tx) *    ty;</span><br><span class="line">                <span class="keyword">float</span> w4 =    tx  *    ty;</span><br><span class="line">                <span class="comment">//3.根据双线性插值公式计算第k个采样点的灰度值</span></span><br><span class="line">                <span class="keyword">float</span> neighbor = src.at&lt;_tp&gt;(x1,y1) * w1 + src.at&lt;_tp&gt;(x1,y2) *w2                     + src.at&lt;_tp&gt;(x2,y1) * w3 +src.at&lt;_tp&gt;(x2,y2) *w4;</span><br><span class="line">                <span class="comment">//通过比较获得LBP值，并按顺序排列起来</span></span><br><span class="line">                lbpCode |= (neighbor&gt;center) &lt;&lt;(neighbors-k<span class="number">-1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            dst.at&lt;uchar&gt;(i-radius,j-radius) = lbpCode;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面是优化版本：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//圆形LBP特征计算，效率优化版本，声明时默认neighbors=8</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> _tp&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getCircularLBPFeatureOptimization</span><span class="params">(InputArray _src,OutputArray _dst,<span class="keyword">int</span> radius,<span class="keyword">int</span> neighbors)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src = _src.getMat();</span><br><span class="line">    <span class="comment">//LBP特征图像的行数和列数的计算要准确</span></span><br><span class="line">    _dst.create(src.rows<span class="number">-2</span>*radius,src.cols<span class="number">-2</span>*radius,CV_8UC1);</span><br><span class="line">    Mat dst = _dst.getMat();</span><br><span class="line">    dst.setTo(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;neighbors;k++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//计算采样点对于中心点坐标的偏移量rx，ry</span></span><br><span class="line">        <span class="keyword">float</span> rx = <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">cos</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">        <span class="keyword">float</span> ry = -<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">sin</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">        <span class="comment">//为双线性插值做准备</span></span><br><span class="line">        <span class="comment">//对采样点偏移量分别进行上下取整</span></span><br><span class="line">        <span class="keyword">int</span> x1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(rx));</span><br><span class="line">        <span class="keyword">int</span> x2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(rx));</span><br><span class="line">        <span class="keyword">int</span> y1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(ry));</span><br><span class="line">        <span class="keyword">int</span> y2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(ry));</span><br><span class="line">        <span class="comment">//将坐标偏移量映射到0-1之间</span></span><br><span class="line">        <span class="keyword">float</span> tx = rx - x1;</span><br><span class="line">        <span class="keyword">float</span> ty = ry - y1;</span><br><span class="line">        <span class="comment">//根据0-1之间的x，y的权重计算公式计算权重，权重与坐标具体位置无关，与坐标间的差值有关</span></span><br><span class="line">        <span class="keyword">float</span> w1 = (<span class="number">1</span>-tx) * (<span class="number">1</span>-ty);</span><br><span class="line">        <span class="keyword">float</span> w2 =    tx  * (<span class="number">1</span>-ty);</span><br><span class="line">        <span class="keyword">float</span> w3 = (<span class="number">1</span>-tx) *    ty;</span><br><span class="line">        <span class="keyword">float</span> w4 =    tx  *    ty;</span><br><span class="line">        <span class="comment">//循环处理每个像素</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=radius;i&lt;src.rows-radius;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=radius;j&lt;src.cols-radius;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//获得中心像素点的灰度值</span></span><br><span class="line">                _tp center = src.at&lt;_tp&gt;(i,j);</span><br><span class="line">                <span class="comment">//根据双线性插值公式计算第k个采样点的灰度值</span></span><br><span class="line">                <span class="keyword">float</span> neighbor = src.at&lt;_tp&gt;(i+x1,j+y1) * w1 + src.at&lt;_tp&gt;(i+x1,j+y2) *w2                     + src.at&lt;_tp&gt;(i+x2,j+y1) * w3 +src.at&lt;_tp&gt;(i+x2,j+y2) *w4;</span><br><span class="line">                <span class="comment">//LBP特征图像的每个邻居的LBP值累加，累加通过与操作完成，对应的LBP值通过移位取得</span></span><br><span class="line">                dst.at&lt;uchar&gt;(i-radius,j-radius) |= (neighbor&gt;center) &lt;&lt;(neighbors-k<span class="number">-1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>旋转不变性圆形LBP：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> _tp&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getRotationInvariantLBPFeature</span><span class="params">(InputArray _src,OutputArray _dst,<span class="keyword">int</span> radius,<span class="keyword">int</span> neighbors)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src = _src.getMat();</span><br><span class="line">    <span class="comment">//LBP特征图像的行数和列数的计算要准确</span></span><br><span class="line">    _dst.create(src.rows<span class="number">-2</span>*radius,src.cols<span class="number">-2</span>*radius,CV_8UC1);</span><br><span class="line">    Mat dst = _dst.getMat();</span><br><span class="line">    dst.setTo(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;neighbors;k++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//计算采样点对于中心点坐标的偏移量rx，ry</span></span><br><span class="line">        <span class="keyword">float</span> rx = <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">cos</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">        <span class="keyword">float</span> ry = -<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(radius * <span class="built_in">sin</span>(<span class="number">2.0</span> * CV_PI * k / neighbors));</span><br><span class="line">        <span class="comment">//为双线性插值做准备</span></span><br><span class="line">        <span class="comment">//对采样点偏移量分别进行上下取整</span></span><br><span class="line">        <span class="keyword">int</span> x1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(rx));</span><br><span class="line">        <span class="keyword">int</span> x2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(rx));</span><br><span class="line">        <span class="keyword">int</span> y1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">floor</span>(ry));</span><br><span class="line">        <span class="keyword">int</span> y2 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(<span class="built_in">ceil</span>(ry));</span><br><span class="line">        <span class="comment">//将坐标偏移量映射到0-1之间</span></span><br><span class="line">        <span class="keyword">float</span> tx = rx - x1;</span><br><span class="line">        <span class="keyword">float</span> ty = ry - y1;</span><br><span class="line">        <span class="comment">//根据0-1之间的x，y的权重计算公式计算权重，权重与坐标具体位置无关，与坐标间的差值有关</span></span><br><span class="line">        <span class="keyword">float</span> w1 = (<span class="number">1</span>-tx) * (<span class="number">1</span>-ty);</span><br><span class="line">        <span class="keyword">float</span> w2 =    tx  * (<span class="number">1</span>-ty);</span><br><span class="line">        <span class="keyword">float</span> w3 = (<span class="number">1</span>-tx) *    ty;</span><br><span class="line">        <span class="keyword">float</span> w4 =    tx  *    ty;</span><br><span class="line">        <span class="comment">//循环处理每个像素</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=radius;i&lt;src.rows-radius;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=radius;j&lt;src.cols-radius;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//获得中心像素点的灰度值</span></span><br><span class="line">                _tp center = src.at&lt;_tp&gt;(i,j);</span><br><span class="line">                <span class="comment">//根据双线性插值公式计算第k个采样点的灰度值</span></span><br><span class="line">                <span class="keyword">float</span> neighbor = src.at&lt;_tp&gt;(i+x1,j+y1) * w1 + src.at&lt;_tp&gt;(i+x1,j+y2) *w2                     + src.at&lt;_tp&gt;(i+x2,j+y1) * w3 +src.at&lt;_tp&gt;(i+x2,j+y2) *w4;</span><br><span class="line">                <span class="comment">//LBP特征图像的每个邻居的LBP值累加，累加通过与操作完成，对应的LBP值通过移位取得</span></span><br><span class="line">                dst.at&lt;uchar&gt;(i-radius,j-radius) |= (neighbor&gt;center) &lt;&lt;(neighbors-k<span class="number">-1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//进行旋转不变处理</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;dst.rows;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;dst.cols;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> currentValue = dst.at&lt;uchar&gt;(i,j);</span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> minValue = currentValue;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>;k&lt;neighbors;k++)</span><br><span class="line">            &#123;</span><br><span class="line">    <span class="comment">//循环左移</span></span><br><span class="line">                <span class="keyword">unsigned</span> <span class="keyword">char</span> temp = (currentValue&gt;&gt;(neighbors-k)) | (currentValue&lt;&lt;k);</span><br><span class="line">                <span class="keyword">if</span>(temp &lt; minValue)</span><br><span class="line">                &#123;</span><br><span class="line">                    minValue = temp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            dst.at&lt;uchar&gt;(i,j) = minValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参考链接：</p>
</blockquote>
<ol>
<li><a href="https://www.cnblogs.com/lxy2017/p/3927280.html" target="_blank" rel="noopener">https://www.cnblogs.com/lxy2017/p/3927280.html</a></li>
<li><a href="http://www.cse.msu.edu/~rossarun/BiometricsTextBook/Papers/Face/Ojala_LBP_PAMI02.pdf" target="_blank" rel="noopener">http://www.cse.msu.edu/~rossarun/BiometricsTextBook/Papers/Face/Ojala_LBP_PAMI02.pdf</a></li>
<li><a href="https://www.cnblogs.com/urglyfish/p/12424087.html" target="_blank" rel="noopener">https://www.cnblogs.com/urglyfish/p/12424087.html</a></li>
<li><a href="https://blog.csdn.net/qq_26898461/article/details/46875517" target="_blank" rel="noopener">https://blog.csdn.net/qq_26898461/article/details/46875517</a></li>
<li><a href="http://www.mamicode.com/info-detail-2947114.html" target="_blank" rel="noopener">http://www.mamicode.com/info-detail-2947114.html</a></li>
</ol>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Harris特征点检测器-兴趣点检测</title>
    <url>/posts/5f014252.html</url>
    <content><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="特征提取（检测）"><a href="#特征提取（检测）" class="headerlink" title="特征提取（检测）"></a>特征提取（检测）</h2><p>基于特征的图像配准方法是图像配准中最常见的方法之一。它不是直接利用图像像素值，而是通过像素值导出的符号特征（如特征点、特征线、特征区域）来实现图像配准，因此可以克服利用灰度信息进行图像配准的缺点，优势主要体现在以下三个方面：</p>
<ol>
<li>利用特征点而不是图像灰度信息，大大减少了在匹配过程中的计算量；</li>
<li>特征点的匹配度量值相对位置变化比较敏感，可以提高匹配的精度；</li>
<li>特征点的提取过程可以减少噪声的影响，对灰度变化、图像形变以及遮挡等都有较好的适应</li>
</ol>
<p>总结起来，就是：<strong>特征点又被称为兴趣点或者角点</strong>，是图像的重要特征。要定义图像，我们需要关注的是<strong>图像中的特征点而不是绝对位置信息</strong>，特征点比如我们肉眼看到的突出的角点、边缘端点、极值点等。</p>
<p>用于<strong>特征点提取</strong>的算子称为“兴趣点提取检测算子”，常用的算子有：<strong>Harris角点检测、FAST特征检测、SIFT特征检测和SURF特征检测</strong>，且这些算子大都是对于<strong>图像的灰度图色彩空间</strong>上进行的像素值的计算！</p>
<p>让我们把目光放长远点看，图像的局部特征当然不只有角点一种，列举如下：</p>
<ul>
<li>角点：Harris算子，SUSAN算子, FAST算子。</li>
<li>梯度特征点：SIFT、SURF、GLOH、ASIFT、PSIFT算子 等。</li>
<li>边缘特征（线型）：Canny算子, Marr算子。</li>
<li>纹理特征：灰度共生矩阵，小波Gabor算子。</li>
</ul>
<p>于是我们可以看到这一节Harris特征点提取的地位是图像局部特征之一的一种提取方法。</p>
<h2 id="角点"><a href="#角点" class="headerlink" title="角点"></a>角点</h2><p>我们先看下面这个讲解：</p>
<p><img src="/posts/CV/harris1.png" alt></p>
<p>角点（特征点）是当窗口向各方向移动，都会引起像素值发生很大变化的一个位置点（右图）；边缘特征是仅当窗口单方向上来回移动，才会引起像素值发生较大变化的一个位置区域（中图）；平坦区域是无论窗口移动方向如何，都不会引起像素值发生很大的变化的区域（左图）。</p>
<h2 id="Harris算法"><a href="#Harris算法" class="headerlink" title="Harris算法"></a>Harris算法</h2><p>图像梯度表示一种现象：像素值发生了很大变化。<br>从表现形式来看，在数学中可以用微分或者导数表示；在数字图像中是二维离散函数求梯度后使用差分近似求导；在实操中，即计算图像中每个像素的某个领域内的灰度变化差值（细化流程：设置合适的算子与窗口大小进行卷积运算）。</p>
<p>具体的数学推导如下：</p>
<p>将图像窗口平移$[u,v]$产生灰度变化$E(u,v)$</p>
<script type="math/tex; mode=display">E(u,v) = \sum_{x,y}w(x,y)[I(x+u,y+v)-I(x,y)]^2</script><p>其中$w(x,y)$为窗口函数，可以用高斯分布或0-1分布刻画，$I(x+u,y+v)$和$I(x,y)$分别为平移后与平移前的图像灰度，对上式进行泰勒展开可得：</p>
<script type="math/tex; mode=display">E(u,v) = \sum_{x,y}w(x,y)[I_xu+T_yv+O((u^2,v^2))]^2</script><p>略去$O$项，最终可得：</p>
<script type="math/tex; mode=display">E(x,y)\approx[u, v] [\begin{matrix}I_x^2 & I_xI_y \\ I_xI_y & I_y^2 \end{matrix}] [\begin{matrix} u \\ v \end{matrix}]  = [u, v] M [\begin{matrix} u \\ v \end{matrix}]</script><p>其中$M = \sum_{x,y}w(x,y) [\begin{matrix}I_x^2 &amp; I_xI_y \\ I_xI_y &amp; I_y^2 \end{matrix}]$</p>
<p>我们只需要讨论这个2×2的矩阵M，M的特征值记为$\lambda_1, \lambda_2$，这两个特征值分别代表沿$x,y$两个方向上灰度值的变化速度。由前面角点以及边缘特征的定义，我们可以得到：</p>
<ol>
<li>当两个特征值$λ_1$和$λ_2$都偏小的时候，表示窗口沿任意方向移动都会使灰度变化很细微，该点处于图像的平坦区域。</li>
<li>当$λ_1&gt;&gt;λ_2$或者$λ_1&lt;&lt;λ_2$时，说明该点向水平（垂直）方向移动时变化会很明显，而向垂直（水平）方向则变化不明显，该点处于图像的边缘区。</li>
<li>当两个特征值$λ_1$和$λ_2$都很大的时候，表示窗口沿任意方向移动都会使灰度变化很明显，该点位置就是图像角点的位置。</li>
</ol>
<p>也就得到了下面这张图所示：</p>
<p><img src="/posts/CV/harris2.png" alt></p>
<h2 id="角点响应函数CRF"><a href="#角点响应函数CRF" class="headerlink" title="角点响应函数CRF"></a>角点响应函数CRF</h2><p>实际中我们很少判断两个特征值，更多的是使用角点响应函数，计算方法如下：</p>
<script type="math/tex; mode=display">R = detM - k(traceM)^2</script><p>其中$detM = \lambda_1\lambda_2,traceM = \lambda_1+\lambda_2,k$为一个修正值，经验取值为$0.04-0.06$，算出响应值之后，根据$R$与阈值$T$的比较来判断是否为角点。</p>
<ol>
<li>当$|R|$很小时，$R\lt T$ , 认为该点处于图像的平坦区域。</li>
<li>当$R&lt;0$时，$R\lt T$ , 认为该点处于图像的边缘区。</li>
<li>当$R&gt;0$时，$R&gt;T$, 认为该点位置就是图像角点。</li>
</ol>
<h2 id="附注"><a href="#附注" class="headerlink" title="附注"></a>附注</h2><ol>
<li>自定义阈值T决定角点的数量</li>
<li>Harris角点的检测算子对亮度和对比度的变化不敏感（光照敏感性），也就是说，对亮度和对比度的仿射变换并不会改变Harris响应的极值点出现的位置。</li>
<li>Harris检测算子具有旋转不变性，即特征位置发生转动，特征值并不会发生变化。</li>
<li>Harris检测算子不具有尺度不变性，即尺度的变化，会将角点变为边缘，或者边缘变为角点。而将Harris角点检测算子与高斯尺度空间表示相结合，使Harris角点检测算子具有尺度的不变性。</li>
</ol>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>首先我导入最可爱的松鼠图，获得图片的尺寸信息：</p>
<p><img src="/posts/CV/animal.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># detector parameters</span></span><br><span class="line">block_size = <span class="number">3</span></span><br><span class="line">sobel_size = <span class="number">3</span></span><br><span class="line">k = <span class="number">0.04</span></span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">'Pic/animal.jpg'</span>)</span><br><span class="line"></span><br><span class="line">print(image.shape)</span><br><span class="line">height = image.shape[<span class="number">0</span>]</span><br><span class="line">width = image.shape[<span class="number">1</span>]</span><br><span class="line">channels = image.shape[<span class="number">2</span>]</span><br><span class="line">print(<span class="string">"width: %s  height: %s  channels: %s"</span> % (width, height, channels))</span><br></pre></td></tr></table></figure>
<p>然后将一个三维的BGR彩色图像转化为一张二维的数据类型为float32灰度图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">gray_img = np.float32(gray_img)</span><br></pre></td></tr></table></figure>
<p>下面一行代码运用Harris角点检测算法进行角点的检测<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corners_img = cv2.cornerHarris(gray_img, block_size, sobel_size, k)</span><br></pre></td></tr></table></figure></p>
<p>定义一个显示函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv_show</span><span class="params">(name, img)</span>:</span></span><br><span class="line">    cv2.imshow(name, img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>显示得到的图像为（好诡异）：</p>
<p><img src="/posts/CV/corner_img.png" alt></p>
<p>随后我们构造卷积核(kernel为3×2全1矩阵)，对图像进行膨胀操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">dst = cv2.dilate(corners_img, kernel)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">"dst"</span>,dst)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>由于膨胀后的图像与上一个比起来看不出什么变化就不进行展示了。我们使用膨胀的原因就是非极大值抑制，其原理是：在一个窗口内，如果有多个角点则用值最大的那个角点，其他的角点都删除，窗口大小这里我们用3*3，程序中通过图像的膨胀运算来达到检测极大值的目的，因为默认参数的膨胀运算就是用窗口内的最大值替代当前的灰度值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> range(height):</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="comment"># 拿出每一个位置的像素值。</span></span><br><span class="line">        pix = dst[r, c]</span><br><span class="line">        <span class="comment"># 这里选用的参数k是0.05</span></span><br><span class="line">        <span class="keyword">if</span> pix &gt; <span class="number">0.05</span> * dst.max():</span><br><span class="line">            <span class="comment"># cicle 参数：图像二维数组、中心位置坐标、半径、颜色、thickness：圆形轮廓的粗细</span></span><br><span class="line">            <span class="comment"># 其中，thickness 正值表示圆形轮廓的粗细，而负厚度表示要绘制实心圆。</span></span><br><span class="line">            <span class="comment"># 颜色 可以是三通道的元组模式，其中如果用cv显示将被解析为BGR,如果用plt则RGB</span></span><br><span class="line">            cv2.circle(image, (c, r), <span class="number">5</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>上述代码似乎跑的比较慢，我们也可以使用简化版的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'Pic/animal.jpg'</span></span><br><span class="line">img = cv.imread(filename)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">gray = np.float32(gray)</span><br><span class="line">dst = cv.cornerHarris(gray,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0.04</span>)</span><br><span class="line"><span class="comment">#result用于标记角点，并不重要</span></span><br><span class="line">dst = cv.dilate(dst,<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#最佳值的阈值，它可能因图像而异。</span></span><br><span class="line">img[dst&gt;<span class="number">0.01</span>*dst.max()]=[<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>]</span><br><span class="line">cv.imshow(<span class="string">'dst'</span>,img)</span><br><span class="line"><span class="keyword">if</span> cv.waitKey(<span class="number">0</span>) &amp; <span class="number">0xff</span> == <span class="number">27</span>:</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>得到的图像如下:</p>
<p><img src="/posts/CV/img1.png" alt></p>
<p>我们修改阈值参数(修改为0.03)，图像如下：</p>
<p><img src="/posts/CV/img2.png" alt></p>
<p>以上就是Harris特征点检测的的全部内容啦~</p>
<blockquote>
<p>参考文章：<br><a href="https://blog.csdn.net/bymar/article/details/104503450/" target="_blank" rel="noopener">https://blog.csdn.net/bymar/article/details/104503450/</a><br><a href="https://blog.csdn.net/m0_38052500/article/details/106877072?utm_source=blogxgwz7" target="_blank" rel="noopener">https://blog.csdn.net/m0_38052500/article/details/106877072?utm_source=blogxgwz7</a><br><a href="https://blog.csdn.net/pbymw8iwm/article/details/82624898" target="_blank" rel="noopener">https://blog.csdn.net/pbymw8iwm/article/details/82624898</a><br><a href="https://blog.csdn.net/weixin_41923000/article/details/88631944" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41923000/article/details/88631944</a><br><a href="http://www.woshicver.com/Sixth/5_2_%E5%93%88%E9%87%8C%E6%96%AF%E8%A7%92%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">http://www.woshicver.com/Sixth/5_2_%E5%93%88%E9%87%8C%E6%96%AF%E8%A7%92%E6%A3%80%E6%B5%8B/</a></p>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>受限玻尔兹曼机</title>
    <url>/posts/e20ce036.html</url>
    <content><![CDATA[<h1 id="受限玻尔兹曼机模型"><a href="#受限玻尔兹曼机模型" class="headerlink" title="受限玻尔兹曼机模型"></a>受限玻尔兹曼机模型</h1><p>Restricted Boltzmann machines are some of the most common building blocks of <strong>deep probabilistic models</strong>. They are undirected probabilistic graphical models containing a layer of observable variables and a single layer of latent variables.</p>
<p>RBM根据MLE原理来估计预定义分布中的参数，以便预定义分布能尽可能地逼近产生观测数据的未知分布。多个RBM分层堆叠而成的DBN(deep belief networks)构成深度学习的主要框架。如下图：</p>
<p><img src="/Pic/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA/1.webp" alt></p>
<p>具体模型为：</p>
<script type="math/tex; mode=display">p(v, h) = \frac{1}{Z}\text{exp}(-E(v,h))</script><p>其中$E(v, h)$是能量函数，定义如下：</p>
<script type="math/tex; mode=display">E(v, h) = -b^Tv - c^Th - v^TWh</script><p>$Z$为正规化函数，定义如下：</p>
<script type="math/tex; mode=display">Z = \sum_v\sum_h\text{exp}(-E(v, h))</script><p>目标函数是极大化概率似然函数$\prod{p(v, h)}$，因此我们希望$E(v, h)$尽可能小，将能量中的点积写成求和式：</p>
<script type="math/tex; mode=display">E(v, h) = -b^Tv-c^Th-v^TWh = -\sum_kb_kv_k - \sum_jc_jh_j - \sum_j\sum_kW_{jk}h_jv_k</script><p>如果$b_k&gt;0$，因为$v_k \in \{0,1\}$，那么要使得能量最小，我们更希望$v_k=1$；反之如果$b_k&lt;0$，我们更希望$v_k=0$。用同样的方式对$b_j,W_{jk}$进行分析，总结如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>系数的情形</th>
<th>我们倾向的选择</th>
</tr>
</thead>
<tbody>
<tr>
<td>$b_k&gt;0$</td>
<td>$v_k=1$</td>
</tr>
<tr>
<td>$b_k&lt;0$</td>
<td>$v_k=0$</td>
</tr>
<tr>
<td>$c_j&gt;0$</td>
<td>$h_j =1$</td>
</tr>
<tr>
<td>$c_j&lt;0$</td>
<td>$h_j =0$</td>
</tr>
<tr>
<td>$W_{jk} &gt; 0$</td>
<td>$h_jv_k =1$</td>
</tr>
<tr>
<td>$W_{jk} &lt; 0$</td>
<td>$h_jv_k =0$</td>
</tr>
</tbody>
</table>
</div>
<p>接着考虑如何最大化$\prod p(v,h)$，这个式子中有$Z$，而$Z$是指数的求和，不大好处理，所以我们不优化这个式子，考虑条件概率$p(h|v)$</p>
<script type="math/tex; mode=display">\begin{equation}\begin{split} p(h|v) &=\frac{p(h,v)}{p(v)}\\ &= \frac{ \frac 1 Z \exp\Big(-E (v,h)\Big)}{ \sum_h \frac 1 Z \exp\Big(-E (v,h)\Big)}\\ &=\frac{ \exp\Big(-E (v,h)\Big)}{ \sum_h \exp\Big(-E (v,h)\Big)}\\ &=\frac{ \exp\Big( b^Tv + c^Th +v^TWh \Big)}{ \sum_h \exp\Big( b^Tv + c^Th +v^TWh \Big)} \\ &= \frac{\exp \Big( b^Tv \Big)\exp\Big( c^Th +v^TWh \Big)}{\exp \Big( b^Tv \Big) \sum_h \exp\Big( c^Th +v^TWh \Big)}\\ &= \frac{\exp\Big( c^Th +v^TWh \Big)}{\sum_h \exp\Big( c^Th +v^TWh \Big)}\\ &= \frac{\exp\Big( \sum_j c_jh_j +\sum_j v^T W_{:j}h_j \Big)}{Z^{'}}\\ &= \frac{1}{Z^{'}} \prod_{j=1}^n \exp \Big( c_jh_j +v^T W_{:j}h_j \Big) \end{split}\end{equation}</script><p>这里$Z^{‘}=\sum_h \exp\Big( c^Th +v^TWh \Big),W_{:j}$是$W$的第$j$列</p>
<p>注意$Z^{‘}$是常数，上式将概率分解为有关$h_1,…,h_n$项的乘积，所以这个式子告诉我们$p(h_1|v),…,p(h_n|v)$条件独立：</p>
<script type="math/tex; mode=display">p(h|v) =\prod _{j=1}^n p(h_j|v)</script><p>并且</p>
<script type="math/tex; mode=display">p(h_j|v) \propto \exp \Big( c_jh_j +v^T W_{:j}h_j \Big)</script><p>我们利用上式计算$p(h_j=1|v),p(h_j=0|v)$</p>
<script type="math/tex; mode=display">\begin{aligned} p(h_j=1|v) &= \frac{p(h_j=1,v)}{p(v)}\\ &= \frac{p(h_j=1,v)}{p(h_j=0,v)+p(h_j=1,v)}\\ &= \frac{ \exp \Big( c_j +v^T W_{:j} \Big)}{\exp(0)+\exp \Big( c_j +v^T W_{:j} \Big) }\\ &= \text{sigmoid} (c_j +v^TW_{:j}) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} p(h_j=0|v) &= 1-p(h_j=1|v) \\ &= 1 - \text{sigmoid} (c_j +v^TW_{:j}) \end{aligned}</script><p>其中$\text{sigmoid}(z) = \frac{1}{1+e^{-z}}$<br>由$v,h$的对称性，同理可得</p>
<script type="math/tex; mode=display">p(v|h) = \prod _{i=1}^n p(v_i|h)\\ p(v_i=1|h) = \text{sigmoid} (b_i +W_{i:}h) \\ p(v_i=0|h) =1- \text{sigmoid} (b_i +W_{i:}h)</script><p>其中$W_{i:}$表示$W$的第$i$行</p>
<h1 id="RBM-Gibbs-Sampling"><a href="#RBM-Gibbs-Sampling" class="headerlink" title="RBM Gibbs Sampling"></a>RBM Gibbs Sampling</h1><p>根据条件独立性，可以得到如下取样的方法：</p>
<ul>
<li><p>取样$h^{(l)} ∼ P(h|v^{(l)} )$：我们可以在给定$v^{(l)}$的条件下同时并且独立的对$h^{(l)}$中的每个元素取样。</p>
</li>
<li><p>取样$v^{(l+1)} ∼ P(v|h^{(l)} )$由条件独立性，我们可以在给定$h^{(l)}$的条件下同时并且独立的对$v^{(l+1)}$中的每个元素取样。</p>
</li>
</ul>
<p>这种取样方法叫做<strong>Gibbs Sampling</strong></p>
<h1 id="训练受限玻尔兹曼机"><a href="#训练受限玻尔兹曼机" class="headerlink" title="训练受限玻尔兹曼机"></a>训练受限玻尔兹曼机</h1><p>这里考虑如何训练受限玻尔兹曼机，我们的目标肯定是极大化概率似然函数，或者等价地，极大化对数概率似然函数，我们进行如下处理：</p>
<script type="math/tex; mode=display">\begin{aligned} \ell (W,b,c) &= \sum_{t=1}^n \log P(v^{(t)})\\ &= \sum_{t=1}^n \log \sum _h P(v^{(t)},h)\\ &= \sum_{t=1}^n \log \sum _h \frac 1 Z \exp\Big(-E (v^{(t)},h)\Big)\\ &= \sum_{t=1}^n \log \frac 1 Z \sum _h \exp\Big(-E (v^{(t)},h)\Big)\\ &= \sum_{t=1}^n \log \sum _h \exp\Big(-E (v^{(t)},h)\Big) - \sum_{t=1}^n \log Z\\ &= \sum_{t=1}^n \log \sum _h \exp\Big(-E (v^{(t)},h)\Big) - n \log Z\\ &= \sum_{t=1}^n \log \sum _h \exp\Big(-E (v^{(t)},h)\Big) - n \log \sum _{v,h} \exp\Big(-E (v,h)\Big)\\ \end{aligned}</script><p>令$\theta =\{b,c,W\}$，我们来对上式关于$\theta$求梯度：</p>
<script type="math/tex; mode=display">\begin{aligned} \nabla_\theta \ell (W,b,c) &=\nabla_\theta\sum_{t=1}^n \log \sum _h \exp\Big(-E (v^{(t)},h)\Big) - n \nabla_\theta\log \sum _{v,h} \exp\Big(-E (v,h)\Big)\\ &=\sum_{t=1}^n \frac{\nabla_\theta \sum _h \exp\Big(-E (v^{(t)},h)\Big)}{ \sum _h \exp\Big(-E (v^{(t)},h)\Big)} - n\frac{\nabla_\theta \sum _{v,h} \exp\Big(-E (v,h)\Big)}{ \sum _{v,h} \exp\Big(-E (v,h)\Big)}\\ &=\sum_{t=1}^n \frac{ \sum _h \exp\Big(-E (v^{(t)},h)\Big) \nabla_\theta \Big( -E (v^{(t)},h)\Big)}{ \sum _h \exp\Big(-E (v^{(t)},h)\Big)}- n\frac{ \sum _{v,h}\exp\Big(-E (v,h)\Big)\nabla_\theta \Big( -E (v,h)\Big)}{ \sum _{v,h}\exp\Big(-E (v,h)\Big)}\ \end{aligned}</script><p>注意，所以上述两项都可以看成随机变量的期望，所以可以写成如下形式：</p>
<script type="math/tex; mode=display">\begin{aligned} \nabla_\theta \ell (W,b,c) &= \sum_{t=1}^n \mathbb E_{P(h|v^{(t)})}\Big[ \nabla_\theta \Big( -E (v^{(t)},h)\Big)\Big] - n \mathbb E_{P(h,v)}\Big[ \nabla_\theta \Big( -E (v,h)\Big)\Big] \end{aligned}</script><p>第一项可以理解关于data的，第二项可以理解为关于model的。</p>
<p>我们利用能量的定义分别对上式再进行处理</p>
<script type="math/tex; mode=display">\begin{aligned} \nabla_W \Big( -E (v,h)\Big) &= \frac{\partial}{\partial W} \Big(b^Tv + c^Th +v^TWh\Big)\\ &= hv^T \end{aligned}\\ \begin{aligned} \nabla_b \Big( -E (v,h)\Big) &= \frac{\partial}{\partial b} \Big(b^Tv + c^Th +v^TWh\Big)\\ &= v \end{aligned}\\ \begin{aligned} \nabla_c \Big( -E (v,h)\Big) &= \frac{\partial}{\partial c} \Big(b^Tv + c^Th +v^TWh\Big)\\ &=h \end{aligned}</script><p>记</p>
<script type="math/tex; mode=display">\hat h^{(t)} = \mathbb E_{P(h|v^{(t)})}[h] =P(h=1|v^{(t)})= \text{sigmoid}(c+{v^{(t)}}^TW)</script><p>带入上式可得</p>
<script type="math/tex; mode=display">\nabla_W \ell (W,b,c) = \sum_{t=1}^n \hat h^{(t)}{v^{(t)}}^T - n \mathbb E_{P(h,v)}[ hv^T] \\ \nabla_b \ell (W,b,c) = \sum_{t=1}^n {v^{(t)}}^T - n \mathbb E_{P(h,v)}[ v] \\ \nabla_c \ell (W,b,c) = \sum_{t=1}^n \hat h^{(t)} - n \mathbb E_{P(h,v)}[ h]</script><p>但是注意<script type="math/tex">\mathbb E_{P(h,v)}\Big[ \nabla_\theta \Big( -E (v,h)\Big)\Big]</script>依旧涉及到<script type="math/tex">Z = \sum_v \sum_h \exp\Big(-E (v,h)\Big)</script>非常难计算，所以实际上我们会用如下近似：</p>
<script type="math/tex; mode=display">\mathbb E_{P(h,v)}\Big[ \nabla_\theta \Big( -E (v,h)\Big)\Big] \approx \nabla_\theta \Big( -E (v,h)\Big) \Big| _{v=\hat v ,h=\hat h}</script><p>对上述式子运用此近似可得</p>
<script type="math/tex; mode=display">\nabla_W \ell (W,b,c) \approx \sum_{t=1}^n h(v^{(t)}){v^{(t)}}^T - n h(\tilde v ) \tilde v ^T \\ \nabla_b \ell (W,b,c) \approx \sum_{t=1}^n {v^{(t)}}^T - n \tilde v \\ \nabla_c \ell (W,b,c) \approx \sum_{t=1}^n h(v^{(t)}) - n h(\tilde v )</script><p>实际中我们会使用随机梯度，所以更新式如下</p>
<script type="math/tex; mode=display">W = W+ \alpha \Big( h(v^{(t)}){v^{(t)}}^T - h(\tilde v ) \tilde v ^T\Big) \\ b= b+ \alpha \Big( h(v^{(t)}) - h(\tilde v )\Big) \\ c = c+ \alpha \Big( v^{(t)} -\tilde v\Big)</script><p>注意这里之所以使用加号是因为这里要最大化目标和函数，所以使用梯度上升。</p>
<p>算法总结<br>把上述内容总结起来，就可以得到如下算法：</p>
<ul>
<li><p>对每个训练数据v^{(t)}</p>
<ul>
<li>从$v^{(t)}$开始使用$k$步Gibbs Sampling生成样本$\tilde v$</li>
<li>更新参数<script type="math/tex; mode=display">W = W+ \alpha \Big( h(v^{(t)}){v^{(t)}}^T - h(\tilde v ) \tilde v ^T\Big) \\ b= b+ \alpha \Big( h(v^{(t)}) - h(\tilde v )\Big) \\ c = c+ \alpha \Big( v^{(t)} -\tilde v\Big)</script></li>
</ul>
</li>
<li><p>返回至1直至停止条件满足</p>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li><a href="https://uwaterloo.ca/data-analytics/sites/ca.data-analytics/files/uploads/files/dbn2.pdf" target="_blank" rel="noopener">https://uwaterloo.ca/data-analytics/sites/ca.data-analytics/files/uploads/files/dbn2.pdf</a></li>
<li><a href="https://www.jianshu.com/p/378e4c93411a" target="_blank" rel="noopener">https://www.jianshu.com/p/378e4c93411a</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>GAN学习（一）</title>
    <url>/posts/ba35fcf2.html</url>
    <content><![CDATA[<h1 id="啥是GAN网络"><a href="#啥是GAN网络" class="headerlink" title="啥是GAN网络"></a>啥是GAN网络</h1><h2 id="通俗地理解GAN网络"><a href="#通俗地理解GAN网络" class="headerlink" title="通俗地理解GAN网络"></a>通俗地理解GAN网络</h2><p>GAN的全称是Generative adversarial network（对抗神经网络）：</p>
<ul>
<li>Gererative：生成模型</li>
<li>Adversarial：使用对抗的方法训练</li>
<li>Networks：使用神经网络</li>
</ul>
<p>由他的名字我们可以知道对抗神经网络是通过对抗的方法去学习数据分布的生成网络，它其实是两个网络的组合，可以理解为一个网络生成模拟数据（生成网络Generator），另一个网络判断生成的数据是真实的还是模拟的（判别网络Discriminator）。生成网络要不断优化自己生成的数据让判别网络判断不出来（纳什均衡），判别网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗神经网络。下面这段形象的比喻来自<a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">论文原文</a>：</p>
<blockquote>
<p>The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency.    </p>
</blockquote>
<p>常用的模型大多可以分为两类，生成模型（Generative Model）和判别模型（Discriminative Model）。前者的输出是“看起来很逼真”的虚假数据，例如合成人脸图片、撰写新闻报道，应当和真实的数据尽可能相似；相比之下，后者一般实现一个从复杂结构数据到简单判别结果的映射（Mapping），例如判断图片是猫还是狗、文本所表达的情绪为积极或者消极。</p>
<p>根据以上的定义，GAN显然属于生成模型，因为我们希望通过GAN生成一些以假乱真的合成数据。进一步而言，生成模型可以分为无条件（Unconditional）和有条件（Conditional）两类，其中前者的生成结果完全随机，后者的生成结果则是可控的，例如生成男性或女性的人脸图片、撰写符合某个话题的新闻报道。</p>
<p>总结<strong>对抗学习和监督学习的区别：</strong></p>
<ul>
<li>监督学习：有明确的标签信息，类似于教小朋友画画</li>
<li>对抗学习：小朋友自己模仿，由大人来鉴别好坏</li>
</ul>
<p><strong>GAN网络唯一需要记住的公式：</strong></p>
<script type="math/tex; mode=display">\text{min}_G \text{max}_DV(D, G)</script><p>其中</p>
<script type="math/tex; mode=display">V(D,G)=\mathbb{E}_{x\text{~}p(x)}[\text{log}D(x)]+\mathbb{E}_{z\text{~}q(z)}[\text{log}(1-D(G(z)))]</script><p>$G$的目的：$G$希望自己生成的数据越真实越好，即$G$希望$D(G(z))$尽可能大，此时$V(D,G)$会变小</p>
<p>$D$的目的：$D$的能力越强，$D(x)$应该越大，$D(G(x))$应该越小，此时$V(D,G)$应该越大，即$\text{max}D$</p>
<p><strong>由于两个网络都需要训练，因此训练其中一个网络时需要锁定另外一个网络，通过迭代的方式可以训练好网络</strong></p>
<h2 id="如何更通俗地理解GAN网络"><a href="#如何更通俗地理解GAN网络" class="headerlink" title="如何更通俗地理解GAN网络"></a>如何更通俗地理解GAN网络</h2><p>有人说GAN强大之处在于可以自动的学习原始真实样本集的数据分布，不管这个分布多么的复杂，只要训练的足够好就可以学出来。针对这一点，感觉有必要好好理解一下为什么别人会这么说。</p>
<p>我们知道，传统的机器学习方法，我们一般都会定义一个什么模型让数据去学习。比如说假设我们知道原始数据属于高斯分布呀，只是不知道高斯分布的参数，这个时候我们定义高斯分布，然后利用数据去学习高斯分布的参数得到我们最终的模型。再比如说我们定义一个分类器，比如SVM，然后强行让数据进行东变西变，进行各种高维映射，最后可以变成一个简单的分布，SVM可以很轻易的进行二分类分开，其实SVM已经放松了这种映射关系了，但是也是给了一个模型，这个模型就是核映射（什么径向基函数等等），说白了其实也好像是你事先知道让数据该怎么映射一样，只是核映射的参数可以学习罢了。</p>
<p>所有的这些方法都在直接或者间接的告诉数据你该怎么映射一样，只是不同的映射方法能力不一样。那么我们再来看看GAN，生成模型最后可以通过噪声生成一个完整的真实数据（比如人脸），说明生成模型已经掌握了从随机噪声到人脸数据的分布规律了，有了这个规律，想生成人脸还不容易。然而这个规律我们开始知道吗？显然不知道，如果让你说从随机噪声到人脸应该服从什么分布，你不可能知道。这是一层层映射之后组合起来的非常复杂的分布映射规律。然而GAN的机制可以学习到，也就是说GAN学习到了真实样本集的数据分布。<br>还有人说GAN强大之处在于可以自动的定义潜在损失函数。 什么意思呢，这应该说的是判别网络可以自动学习到一个好的判别方法，其实就是等效的理解为可以学习到好的损失函数，来比较好或者不好的判别出来结果。虽然大的loss函数还是我们人为定义的，基本上对于多数GAN也都这么定义就可以了，但是判别网络潜在学习到的损失函数隐藏在网络之中，不同的问题这个函数就不一样，所以说可以自动学习这个潜在的损失函数。</p>
<p>总而言之：<strong>GAN网络最强大的地方就是可以帮助我们建立模型，而不像传统的网络那样是在已有模型上帮我们更新参数而已。同时，GAN网络是一种无监督的学习方式，它的泛化性非常好。</strong></p>
<h2 id="GAN的应用场景"><a href="#GAN的应用场景" class="headerlink" title="GAN的应用场景"></a>GAN的应用场景</h2><ol>
<li>数据生成，主要指图像生成。常用的有DCGAN WGAN，BEGAN；</li>
<li>GAN本身也是一种无监督学习的典范，因此它在无监督学习，半监督学习领域都有广泛的应用；</li>
<li>不仅在生成领域，GAN在分类领域也占有一席之地，简单来说，就是替换判别器为一个分类器，做多分类任务，而生成器仍然做生成任务，辅助分类器训练；</li>
<li>GAN可以和强化学习结合，目前一个比较好的例子就是seq-GAN；可以与迁移学习结合（Mind2Mind）</li>
<li>目前比较有意思的应用就是GAN用在图像风格迁移，图像降噪修复，图像超分辨率了，都有比较好的结果；</li>
<li>图像数据增强。</li>
</ol>
<blockquote>
<p>GAN也可以用在NLP领域，但GAN在CV界所取得的进展和成果远远多于自然语言处理（Natural Language Processing，NLP），一个主要原因就是<strong>图片的表示是“连续”的，即当像素值发生微小变化时，视觉上并不会察觉出明显的区别</strong>。相比之下，NLP任务中一般会将字或词作为最基础的语义单元，而<strong>字和词的表示是“离散”的</strong>，即我们很难统一规定，当一个字或词发生微小变化时，对应的语义是哪一个其他的字或词。字或词的这种“离散跳变性”，无疑加大了GAN训练时的困难和不稳定性，从而导致GAN在NLP领域中的发展和应用相对较少</p>
</blockquote>
<h2 id="GAN的优点"><a href="#GAN的优点" class="headerlink" title="GAN的优点"></a>GAN的优点</h2><ul>
<li>GAN是一种生成式模型，相较于其他模型（玻尔兹曼机和GSNs）只用到了反向传播，不需要复杂的马尔可夫链</li>
<li>相较于其他所有模型，GAN可以生成更加清晰真实的样本</li>
<li>GAN采用的是一种无监督的学习方式训练，可以广泛用在无监督学习和半监督学习的领域</li>
<li>相对于变分自编码器，GANs没有引入任何决定性偏置，变分方法引入决定性偏置（需要先采样），因为它们优化对数似然的下界，而不是似然度本身，这看起来导致了VAEs生成的实例比GANs更模糊</li>
<li>相比VAEs，GANs没有变分下界，如果鉴别器训练良好，那么生成器可以完美的学习到训练样本的分布，换句话说，GANs是渐进一致的，而VAE是有偏差的</li>
<li>GAN应用在一些场景上，比如图片风格迁移，超分辨率 ，图像补全，去噪，避免了损失函数设计的困难，只要有一个基准，直接上判别器，剩下就交给对抗训练了</li>
</ul>
<h2 id="GAN存在的问题"><a href="#GAN存在的问题" class="headerlink" title="GAN存在的问题"></a>GAN存在的问题</h2><h3 id="Non-Convergence（不收敛）"><a href="#Non-Convergence（不收敛）" class="headerlink" title="Non-Convergence（不收敛）"></a>Non-Convergence（不收敛）</h3><p>训练GAN需要达到纳什均衡，有时候做不到（目前找不到一定可以做到的方法），训练GAN相比VAE不稳定。</p>
<h3 id="Mode-Collapse（模式坍塌）"><a href="#Mode-Collapse（模式坍塌）" class="headerlink" title="Mode-Collapse（模式坍塌）"></a>Mode-Collapse（模式坍塌）</h3><p>可以理解为生成的内容没有多样性，这种情况一般出现在GAN训练不稳定时，具体表现为生成出来的结果非常差，即使加长训练时间后也不能得到很好的改善。</p>
<p>原因：</p>
<ul>
<li>GAN采用的是对抗训练的方式，G的梯度更新来自D，所以G生成的好不好取决于D的评价</li>
<li>如果某一次G生成的样本并不是很好，但D给出了很好的评价，或者是G生成的结果中的一些特征得到了D的认可，这时候G就会认为自己输出的是正确的，那么接下来认为这样输出D仍会给出较高的评价（实际上G输出的并不好）</li>
<li>进入一种“死循环”，最终生成结果丢失一些信息，特征不全</li>
</ul>
<h1 id="具体咋做-代码"><a href="#具体咋做-代码" class="headerlink" title="具体咋做(代码)"></a>具体咋做(代码)</h1><p>我们康一康伪代码咋写：</p>
<p><img src="/Pic/GAN%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/1.png" alt></p>
<p>根据这个伪代码我们很容易可以完成我们的实验流程。首先以造小狗的假图片为例：首先需要一个生成小狗图片的模型，我们称之为generator，还有一个判断小狗图片是否是真假的判别模型discrimator。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Dense(input_dim=<span class="number">1000</span>, output_dim=<span class="number">1024</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(Dense(<span class="number">128</span> * <span class="number">8</span> * <span class="number">8</span>))</span><br><span class="line">    model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(Reshape((<span class="number">8</span>, <span class="number">8</span>, <span class="number">128</span>), input_shape=(<span class="number">8</span> * <span class="number">8</span> * <span class="number">128</span>,)))</span><br><span class="line">    model.add(UpSampling2D(size=(<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">'same'</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(UpSampling2D(size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">3</span>, (<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">'same'</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>生成器接受一个1000维的随机生成的数组，然后输出一个64×64×3通道的图片数据。判别器则输入64，64，3的图片，输出1或者0，代表图片是否是狗：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">'same'</span>, input_shape=(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">128</span>, (<span class="number">5</span>, <span class="number">5</span>)))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(<span class="number">1024</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>下面就是GAN网络的核心部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_containing_discriminator</span><span class="params">(g, d)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(g)</span><br><span class="line">    <span class="comment"># 判别器参数不进行修改</span></span><br><span class="line">    d.trainable = <span class="literal">False</span></span><br><span class="line">    model.add(d)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>这个模型由生成器与判别器组成：上半部分是生成网络，下半部分是判别网络，生成网络首先生成假图，然后送入判别网络中进行判断，这里有一个d.trainable=False，意思是，只调整生成器，判别的的参数不做更改。</p>
<p>下面对生成网络进行训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(BATCH_SIZE)</span>:</span></span><br><span class="line">    <span class="comment"># 载入训练参数</span></span><br><span class="line">    X_train = load_data(path)</span><br><span class="line">    X_train = (X_train.astype(np.float32) - <span class="number">127.5</span>)/<span class="number">127.5</span></span><br><span class="line">    d = discriminator_model()</span><br><span class="line">    g = generator_model()</span><br><span class="line">    d_on_g = generator_containing_discriminator(g, d)</span><br><span class="line">    d_optim = SGD(lr=<span class="number">0.0005</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">    g_optim = SGD(lr=<span class="number">0.0005</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">    g.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">"SGD"</span>)</span><br><span class="line">    d_on_g.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=g_optim)</span><br><span class="line">    d.trainable = <span class="literal">True</span></span><br><span class="line">    d.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=d_optim)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(g_epoch_num):</span><br><span class="line">        print(<span class="string">"Epoch is"</span>, epoch)</span><br><span class="line">        <span class="comment"># 训练一个batchsize里面的数据</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(int(X_train.shape[<span class="number">0</span>]/BATCH_SIZE)):</span><br><span class="line">            <span class="comment"># 产生随机噪声</span></span><br><span class="line">            noise = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=(BATCH_SIZE, <span class="number">1000</span>))</span><br><span class="line">            <span class="comment"># 这里面都是真图片</span></span><br><span class="line">            image_batch = X_train[index*BATCH_SIZE:(index+<span class="number">1</span>)*BATCH_SIZE]</span><br><span class="line">            <span class="comment"># 这里产生假图片</span></span><br><span class="line">            generated_images = g.predict(noise, verbose=<span class="number">0</span>)</span><br><span class="line">            cv2.imshow(<span class="string">'generator_dog_image'</span>, generated_images[<span class="number">0</span>, :, :, :])</span><br><span class="line">            cv2.waitKey(<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># 将真图片与假图片拼接在一起</span></span><br><span class="line">            X = np.concatenate((image_batch, generated_images))</span><br><span class="line">            <span class="comment"># 前64张图片标签为1,即真图，后64张照片为假图</span></span><br><span class="line">            y = [<span class="number">1</span>] * BATCH_SIZE + [<span class="number">0</span>] * BATCH_SIZE</span><br><span class="line">            <span class="comment"># 对于判别器进行训练，不断提高判别器的识别精度</span></span><br><span class="line">            d_loss = d.train_on_batch(X, y)</span><br><span class="line">            <span class="comment"># 再次产生随机噪声</span></span><br><span class="line">            noise = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, (BATCH_SIZE, <span class="number">1000</span>))</span><br><span class="line">            <span class="comment"># 设置判别器的参数不可调整</span></span><br><span class="line">            d.trainable = <span class="literal">False</span></span><br><span class="line">            <span class="comment"># 在此我们送入噪声，并认为这些噪声是真实的标签</span></span><br><span class="line">            g_loss = d_on_g.train_on_batch(noise, [<span class="number">1</span>] * BATCH_SIZE)</span><br><span class="line">            <span class="comment"># 此时设置判别器可以被训练，参数可以被修改</span></span><br><span class="line">            d.trainable = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 打印损失值</span></span><br><span class="line">            print(<span class="string">"Epoch is : %s, batch %d d_loss : %s, g_loss : %f"</span> % (epoch, index, d_loss, g_loss))</span><br><span class="line">            <span class="keyword">if</span> index % <span class="number">10</span> == <span class="number">9</span>:</span><br><span class="line">                g.save_weights(<span class="string">'generator'</span>, <span class="literal">True</span>)</span><br><span class="line">                d.save_weights(<span class="string">'discriminator'</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>训练完之后自然就要生成图像：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(BATCH_SIZE, nice=False)</span>:</span></span><br><span class="line">    g = generator_model()</span><br><span class="line">    g.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">"SGD"</span>)</span><br><span class="line">    g.load_weights(<span class="string">'generator'</span>)</span><br><span class="line">    <span class="keyword">if</span> nice:</span><br><span class="line">        d = discriminator_model()</span><br><span class="line">        d.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">"SGD"</span>)</span><br><span class="line">        d.load_weights(<span class="string">'discriminator'</span>)</span><br><span class="line">        noise = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, (BATCH_SIZE*<span class="number">20</span>, <span class="number">1000</span>))</span><br><span class="line">        generated_images = g.predict(noise, verbose=<span class="number">1</span>)</span><br><span class="line">        d_pret = d.predict(generated_images, verbose=<span class="number">1</span>)</span><br><span class="line">        index = np.arange(<span class="number">0</span>, BATCH_SIZE*<span class="number">20</span>)</span><br><span class="line">        index.resize((BATCH_SIZE*<span class="number">20</span>, <span class="number">1</span>))</span><br><span class="line">        pre_with_index = list(np.append(d_pret, index, axis=<span class="number">1</span>))</span><br><span class="line">        pre_with_index.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[<span class="number">1</span>:<span class="number">3</span>], dtype=np.float32)</span><br><span class="line">        nice_images = nice_images[:, :, :, <span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(BATCH_SIZE):</span><br><span class="line">            idx = int(pre_with_index[i][<span class="number">1</span>])</span><br><span class="line">            nice_images[i, :, :, <span class="number">0</span>] = generated_images[idx, :, :, <span class="number">0</span>]</span><br><span class="line">        image = combine_images(nice_images)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>):</span><br><span class="line">            noise = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, (BATCH_SIZE, <span class="number">1000</span>))</span><br><span class="line">            generated_images = g.predict(noise, verbose=<span class="number">1</span>)</span><br><span class="line">            img = np.zeros((generated_images.shape[<span class="number">0</span>], generated_images.shape[<span class="number">1</span>], <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">64</span>):</span><br><span class="line">                img = generated_images[j, :, :, :]</span><br><span class="line">                cv2.imshow(<span class="string">'test1'</span>, img)</span><br><span class="line">                cv2.waitKey(<span class="number">10</span>)</span><br><span class="line">                cv2.imwrite(<span class="string">"./dog_images/generated_image"</span> + str(i) + <span class="string">'_'</span> + str(j) + <span class="string">".png"</span>, img*<span class="number">127.5</span>+<span class="number">127.5</span>)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'i : i'</span>, i)</span><br></pre></td></tr></table></figure>
<p>参考链接：</p>
<ul>
<li><a href="https://www.sohu.com/a/325882199_114877" target="_blank" rel="noopener">https://www.sohu.com/a/325882199_114877</a></li>
<li><a href="https://www.leiphone.com/news/201907/Sv3AtCsT4w2W6roc.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201907/Sv3AtCsT4w2W6roc.html</a></li>
<li><a href="https://blog.csdn.net/LEE18254290736/article/details/97371930" target="_blank" rel="noopener">https://blog.csdn.net/LEE18254290736/article/details/97371930</a></li>
<li><a href="https://blog.csdn.net/LEEANG121/article/details/104113406" target="_blank" rel="noopener">https://blog.csdn.net/LEEANG121/article/details/104113406</a></li>
<li><a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.2661.pdf</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/114838349" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/114838349</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/education/lessonvideo/496803" target="_blank" rel="noopener">https://aistudio.baidu.com/aistudio/education/lessonvideo/496803</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（六）图像金字塔与轮廓检测</title>
    <url>/posts/511ffd8d.html</url>
    <content><![CDATA[<h1 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a>图像金字塔</h1><p>图像金字塔是<strong>图像中多尺度表达</strong>的一种，最主要用于<strong>图像的分割</strong>，是一种<strong>以多分辨率来解释图像的有效但概念简单的结构</strong>。</p>
<p>图像金字塔最初用于机器视觉和图像压缩，<strong>一幅图像的金字塔是一系列以金字塔形状排列的分辨率逐步降低，且来源于同一张原始图的图像集合</strong>。其通过梯次向下采样获得，直到达到某个终止条件才停止采样。金字塔的底部是待处理图像的高分辨率表示，而顶部是低分辨率的近似。我们将一层一层的图像比喻成金字塔，<strong>层级越高，则图像越小，分辨率越低</strong>。</p>
<p>首先我们看一下图像金字塔长啥样：</p>
<p><img src="/Pic/Opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%EF%BC%9A%EF%BC%88%E5%85%AD%EF%BC%89%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94%E4%B8%8E%E8%BD%AE%E5%BB%93%E6%A3%80%E6%B5%8B/Pyramid_1.png" alt></p>
<p>左边的图是高斯金字塔，右边的图是拉普拉斯金字塔。假设我们有一张$800<em>800$的图像，我们可以对其进行往上层变换，由于越离金字塔尖的图像会越小，可能就变成了$400</em>400$的图像了，然后依次往上走，越来越小，这样我们就把一张图像变成了一系列不同大小的图像。你可能就会问这样的图像金字塔有什么用呢？我们以后将做特征提取，这时候我们不一定只是对原图像进行特征提取，可能会对好几层金字塔进行特征提取，每一层提取出来的结果是不一样的，最后再综合在一起。</p>
<p>我们下面分别介绍两种图像金字塔——高斯金字塔与拉普拉斯金字塔：</p>
<h2 id="高斯金字塔"><a href="#高斯金字塔" class="headerlink" title="高斯金字塔"></a>高斯金字塔</h2><p>高斯金字塔的主要采样方法分为两种——向上采样和向下采样。这两个名称恰好和金字塔的图像相反，向下采样是指将图像变得越来越小，也就对应从金字塔塔底到塔尖的过程；反过来向上采样对应从金字塔塔尖到塔底的过程。</p>
<h3 id="向下采样"><a href="#向下采样" class="headerlink" title="向下采样"></a>向下采样</h3><p><img src="/posts/CV/Pyramid_2.png" alt></p>
<p>如上图所示，首先我们对内核做卷积操作，将内核与位置相乘后累加，除以总数归一化，随后我们将所有偶数行去掉。</p>
<h3 id="向上采样"><a href="#向上采样" class="headerlink" title="向上采样"></a>向上采样</h3><p><img src="/posts/CV/Pyramid_3.png" alt></p>
<p>我们的原图像为：</p>
<p><img src="/Pic/CV/AM.png" alt></p>
<p>首先我们直接对原像素矩阵扩充，然后用之前同样的内核对原图像进行卷积操作，得到近似放大图像，实现代码为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">up=cv2.pyrUp(img)</span><br><span class="line">cv_show(up,<span class="string">'up'</span>)</span><br><span class="line"><span class="keyword">print</span> (up.shape)</span><br></pre></td></tr></table></figure>
<p>这里关键就是pyrUp函数，这是做了一个向上采样，得到的图像为：</p>
<p><img src="/Pic/CV/upAM.png" alt></p>
<p>我们可以看到这张图片明显变大了，既然图片能变大，当然也能变小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">down=cv2.pyrDown(img)</span><br><span class="line">cv_show(down,<span class="string">'down'</span>)</span><br><span class="line"><span class="keyword">print</span> (down.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/CV/downAM.png" alt></p>
<p>我们看到该图片确实比原图小了一半。那么有一个问题，我们将图片放大之后再缩小，图片能恢复原状吗？显然是不行的，采样之后得到的图像只是一个近似值，我们可以通过对得到的两个图像相减得到两个图像的差值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">up=cv2.pyrUp(img)</span><br><span class="line">up_down=cv2.pyrDown(up)</span><br><span class="line">cv_show(img-up_down,<span class="string">'img-up_down'</span>)</span><br></pre></td></tr></table></figure>
<p>得到的结果蛮酷炫的：</p>
<p><img src="/Pic/CV/imgup_down.png" alt></p>
<h2 id="拉普拉斯金字塔"><a href="#拉普拉斯金字塔" class="headerlink" title="拉普拉斯金字塔"></a>拉普拉斯金字塔</h2><p>拉普拉斯金字塔用来从金字塔低层图像重建上层未采样图像，在数字图像处理中也即是预测残差，可以对图像进行最大程度的还原，配合高斯金字塔一起使用。其原理稍为复杂，我们直接看代码较为直观：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">laplacian</span><span class="params">(gaussian_pyramid, up_times=<span class="number">5</span>)</span>:</span></span><br><span class="line">    laplacian_pyramid = [gaussian_pyramid[<span class="number">-1</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(up_times, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># i的取值为5,4,3,2,1,0也就是拉普拉斯金字塔有6层</span></span><br><span class="line">        temp_pyrUp = cv2.pyrUp(gaussian_pyramid[i])</span><br><span class="line">        temp_lap = cv2.subtract(gaussian_pyramid[i<span class="number">-1</span>], temp_pyrUp)</span><br><span class="line">        laplacian_pyramid.append(temp_lap)</span><br><span class="line">    <span class="keyword">return</span> laplacian_pyramid</span><br></pre></td></tr></table></figure>
<p>简单地说拉普拉斯金字塔记录了高斯金字塔每一级下采样后再上采样与下采样前的差异，目的是为了能够完全地还原采样前的图像，差异记录过程为：</p>
<script type="math/tex; mode=display">L_i = G_i-Up(Down(G_i))</script><p>获取拉普拉斯金字塔第一层结果的方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">down=cv2.pyrDown(img)</span><br><span class="line">down_up=cv2.pyrUp(down)</span><br><span class="line">l_1=img-down_up</span><br><span class="line">cv_show(l_1,<span class="string">'l_1'</span>)</span><br></pre></td></tr></table></figure>
<p>当然也可以用同样 的方式获取其他层，这里不再对其进行更详细的介绍。</p>
<h1 id="轮廓检测"><a href="#轮廓检测" class="headerlink" title="轮廓检测"></a>轮廓检测</h1><h2 id="轮廓绘制"><a href="#轮廓绘制" class="headerlink" title="轮廓绘制"></a>轮廓绘制</h2><p>首先介绍我们的常用函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.findContours(img,mode,method)</span><br></pre></td></tr></table></figure>
<p><strong>mode：轮廓检索模式</strong></p>
<ul>
<li>RETR_EXTERNAL ：只检索最外面的轮廓；</li>
<li>RETR_LIST：检索所有的轮廓，并将其保存到一条链表当中；</li>
<li>RETR_CCOMP：检索所有的轮廓，并将他们组织为两层：顶层是各部分的外部边界，第二层是空洞的边界；</li>
<li>RETR_TREE：检索所有的轮廓，并重构嵌套轮廓的整个层次（一般直接使用这个就够了）；</li>
</ul>
<p><strong>method:轮廓逼近方法</strong></p>
<ul>
<li>CHAIN_APPROX_NONE：以Freeman链码的方式输出轮廓，所有其他方法输出多边形（顶点的序列）；</li>
<li>CHAIN_APPROX_SIMPLE:压缩水平的、垂直的和斜的部分，也就是，函数只保留他们的终点部分。</li>
</ul>
<p>我们在进行轮廓检测时，为了更高的准确率，常使用二值图像，因此我们的常用流程为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'car.png'</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv2.threshold(gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">cv_show(thresh,<span class="string">'thresh'</span>)</span><br></pre></td></tr></table></figure>
<p>上面代码中第一步是读入图像，第二步是将图像转换为灰度图，第三步将灰度图转化为二值图像（像素值大于127的像素点为1，小于127的像素点为0），我们传入的图像为：</p>
<p><img src="/Pic/CV/car.png" alt></p>
<p>转化后的图像为：</p>
<p><img src="/Pic/CV/thresh.png" alt></p>
<p>下面我们使用检测函数进行检测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">binary, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)</span><br></pre></td></tr></table></figure>
<p>binary实际上就是我们原来的二值图像，而contours则是边界信息，我们可以查看一下他的维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.array(contours).shape</span><br></pre></td></tr></table></figure>
<p>返回值为$(2579,)$，我们也可以#传入绘制图像，轮廓，轮廓索引，颜色模式，线条厚度，展示轮廓图像，注意需要copy，要不原图会变：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img, contours, <span class="number">-1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/CV/res23.png" alt></p>
<p>我们改变一下参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img, contours, <span class="number">0</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以看到第三个参数实际上是表示绘制轮廓的范围，参数为-1时默认全部绘制；第四个参数为BGR通道，即希望用什么颜色来绘制，第五个参数为线条宽度，具体大家可以自行尝试（建议换一张简单的几何图像对第三个参数进行尝试）。</p>
<p>下面有一个很有意思的地方，我们的轮廓由于是一个闭合曲线，因此我们可以计算他们的面积：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#面积</span></span><br><span class="line">cv2.contourArea(cnt)</span><br><span class="line"><span class="comment">#周长，True表示闭合的</span></span><br><span class="line">cv2.arcLength(cnt,<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="轮廓近似"><a href="#轮廓近似" class="headerlink" title="轮廓近似"></a>轮廓近似</h2><p>如下图，我们可以改变阈值对轮廓做不同层次的近似（轮廓真实点与直线距离小于某个阈值时可以认为归为这条直线内部），最终可以将轮廓划分成尽可能少的图像。</p>
<p><img src="/Pic/CV/contours3.png" alt></p>
<p>我们直接来看如何实现这样一个轮廓近似，首先我们给图像描出一个轮廓：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'contours2.png'</span>)</span><br><span class="line"></span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv2.threshold(gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">binary, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img, [cnt], <span class="number">-1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p>原图像为：</p>
<p><img src="/Pic/CV/contours2.png" alt></p>
<p>绘制的轮廓结果为：</p>
<p><img src="/Pic/CV/ctor.png" alt></p>
<p>我们近似轮廓的代码其实很简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">0.15</span>*cv2.arcLength(cnt,<span class="literal">True</span>) </span><br><span class="line">approx = cv2.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img, [approx], <span class="number">-1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p>其中第一个参数为传入待近似的轮廓，第二个参数传入阈值，这个阈值通常是通过轮廓的周长进行设定，这里取了0.15倍的周长，近似轮廓结果如下：</p>
<p><img src alt><img src="/Pic/CV/ctor2.png" alt="ctor2"></p>
<p>我们可以看到近似效果其实很不好，我们可以缩减阈值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">0.02</span>*cv2.arcLength(cnt,<span class="literal">True</span>) </span><br><span class="line">approx = cv2.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img, [approx], <span class="number">-1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br><span class="line">cv2.imwrite(<span class="string">"ctor2.png"</span>, res)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：</p>
<p><img src="/Pic/CV/ctor3.png" alt></p>
<p>结果比原来好了很多，我们还可以继续修改参数得到我们最满意的结果。</p>
<h2 id="外接图形"><a href="#外接图形" class="headerlink" title="外接图形"></a>外接图形</h2><p>我们也可以对图像的轮廓做外接矩形或外接圆等，具体做法为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'contours.png'</span>)</span><br><span class="line"></span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv2.threshold(gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">binary, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">x,y,w,h = cv2.boundingRect(cnt)</span><br><span class="line">img = cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">cv_show(img,<span class="string">'img'</span>)</span><br></pre></td></tr></table></figure>
<p>传入的图像为：</p>
<p><img src="/Pic/CV/contours.png" alt></p>
<p>绘制出来的结果长成这个样子：</p>
<p><img src="/Pic/CV/ctor4.png" alt></p>
<p>同样也可以绘制外接圆：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(x,y),radius = cv2.minEnclosingCircle(cnt) </span><br><span class="line">center = (int(x),int(y)) </span><br><span class="line">radius = int(radius) </span><br><span class="line">img = cv2.circle(img,center,radius,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">cv_show(img,<span class="string">'img'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模板匹配"><a href="#模板匹配" class="headerlink" title="模板匹配"></a>模板匹配</h1><p>模板匹配就是将模板在原图像上滑动，计算模板与原图像的差异程度，计算差异程度的方法有6种，计算结果将存放在矩阵中作为结果输出，假设原图像大小为$A×B$，而模板大小为$a×b$，则输出结果的矩阵为$(A-a+1)×(B-b+1)$。</p>
<p>首先传入模板图像和原图像：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"lena.jpg"</span>, <span class="number">0</span>)</span><br><span class="line">template = cv2.imread(<span class="string">"face.jpg"</span>, <span class="number">0</span>)</span><br><span class="line">h, w = template.shape[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>然后就调包进行模板匹配：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = cv2.matchTemplate(img, template, cv2.TM_SQDIFF)</span><br></pre></td></tr></table></figure>
<p>第三个参数为选择的计算差异程度的方法，建议选择归一化的结果，比如TM_CCORR_NORMED，得到的结果将更可靠。为了得到边框的位置，我们可以看左上角和右下角两个点的位置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)</span><br></pre></td></tr></table></figure>
<p>然后大家可以根据这两个点计算一波画个矩形框框。</p>
<p>若待匹配结果有多个图像，比如行人检测，我们自然也可以自己设定阈值，找到匹配程度大于阈值的位置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">threshold &#x3D; 0.6</span><br><span class="line">loc &#x3D; np.where(res&gt;&#x3D;threshold)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（五）Canny边缘检测 </title>
    <url>/posts/dcd951b.html</url>
    <content><![CDATA[<h1 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><ol>
<li><p>使用高斯滤波器，以平滑图像，滤除噪声。</p>
</li>
<li><p>计算图像中每个像素点的梯度强度和方向。</p>
</li>
<li><p>应用非极大值（Non-Maximum Suppression）抑制，以消除边缘检测带来的杂散响应。</p>
</li>
<li><p>应用双阈值（Double-Threshold）检测来确定真实的和潜在的边缘。</p>
</li>
<li><p>通过抑制孤立的弱边缘最终完成边缘检测。</p>
</li>
</ol>
<h2 id="具体来说"><a href="#具体来说" class="headerlink" title="具体来说"></a>具体来说</h2><p>首先我们有一个归一化后的<strong>高斯滤波器</strong>，然后完成我们前面说的<strong>平滑处理</strong>：</p>
<p><img src="/posts/CV/canny_1.png" alt></p>
<p>如前一篇文章我们进行<strong>图像梯度</strong>的计算：</p>
<p><img src="/posts/CV/canny_2.png" alt></p>
<p><strong>非极大值抑制</strong>可以用线性插值法来实现：</p>
<p><img src="/posts/CV/canny_3.png" alt></p>
<p>也可以简化操作：</p>
<p><img src="/posts/CV/canny_6.png" alt></p>
<p>其本质就是比较当前点与周围两点的梯度幅值大小，若他是最大的就保存下来，若不是就要抑制。</p>
<p>然后我们通过一个<strong>双阈值检测</strong>，检测过程如下：</p>
<p><img src="/posts/CV/canny_5.png" alt></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我们下面设置了两个不同的阈值（双阈值检测的min和max）进行对比：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img=cv2.imread(<span class="string">"lena.jpg"</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line">v1=cv2.Canny(img,<span class="number">80</span>,<span class="number">150</span>)</span><br><span class="line">v2=cv2.Canny(img,<span class="number">50</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">res = np.hstack((v1,v2))</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/cannycompare.png" alt></p>
<p>我们可以看到，两个阈值都比较小的一组边界更细致，更多边界信息被刻画到，但也可能 考虑进了更多噪音点。</p>
<p>我们可以从下面的例子看到更明显的对比：</p>
<p>我们导入一排汽车的图片：</p>
<p><img src="/posts/CV/car.png" alt></p>
<p>我们对其进行同上处理，结果为：</p>
<p><img src="/posts/CV/cannycompare2.png" alt></p>
<p>我们看到右边图中明显多出了很多信息。</p>
<h1 id="概念解释"><a href="#概念解释" class="headerlink" title="概念解释"></a>概念解释</h1><h2 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h2><p>非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素，可以理解为局部最大搜索。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法(参考论文《Efficient Non-Maximum Suppression》对1维和2维数据的NMS实现)，而是用于目标检测中提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。</p>
<p>简单地说，经过一个算法进行人脸识别，其中有多个窗口，有的窗口覆盖了一部分人脸，有的窗口覆盖了整个人脸，非极大值抑制可以筛选出含全部人脸的部分，剔除掉只含部分人脸的部分。</p>
<p><img src="/posts/CV/nms.png" alt></p>
<p>例如在上图中我们的目的就是要去除左边冗余的检测框，留下最好的那个（右边）。</p>
<h2 id="双阈值检测"><a href="#双阈值检测" class="headerlink" title="双阈值检测"></a>双阈值检测</h2><p><strong>目的：</strong>检测过程中有一些边界候选值，要对其进行过滤找真实边界。</p>
<p><strong>具体方法：</strong></p>
<ol>
<li>根据图像选取合适的高阈值和低阈值，通常高阈值是低阈值的2到3倍</li>
<li>如果某一像素的梯度值高于高阈值，则保留</li>
<li>如果某一像素的梯度值低于低阈值，则舍弃</li>
<li>如果某一像素的梯度值介于高低阈值之间，则从该像素的8邻域的寻找像素梯度值，如果存在像素梯度值高于高阈值，则保留，如果没有，则舍弃</li>
</ol>
<p>伪代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">st = zeros(row,col);%定义一个双阈值图像</span><br><span class="line">TL = <span class="number">0.1</span> * max(max(jd));%低阈值</span><br><span class="line">TH = <span class="number">0.12</span>* max(max(jd));%高阈值</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>  : row</span><br><span class="line">    <span class="keyword">for</span> j = <span class="number">1</span> : col</span><br><span class="line">        <span class="keyword">if</span> (jd(i, j) &lt; TL)</span><br><span class="line">            st(i,j) = <span class="number">0</span>;</span><br><span class="line">        elseif (jd(i, j) &gt; TH)</span><br><span class="line">            st(i,j) = <span class="number">1</span> ;</span><br><span class="line">        %对TL &lt; Nms(i, j) &lt; TH 使用<span class="number">8</span>连通区域确定</span><br><span class="line">        elseif (jd(i, j)&lt;TH&amp;&amp;jd(i, j)&gt;TL)</span><br><span class="line">            su =[jd(i<span class="number">-1</span>,j<span class="number">-1</span>), jd(i<span class="number">-1</span>,j), jd(i<span class="number">-1</span>,j+<span class="number">1</span>);</span><br><span class="line">                       jd(i,j<span class="number">-1</span>),    jd(i,j),   jd(i,j+<span class="number">1</span>);</span><br><span class="line">                       jd(i+<span class="number">1</span>,j<span class="number">-1</span>), jd(i+<span class="number">1</span>,j), jd(i+<span class="number">1</span>,j+<span class="number">1</span>)];</span><br><span class="line">            Max = max(su);</span><br><span class="line">            <span class="keyword">if</span> Max&gt;=TH</span><br><span class="line">                st(i,j) = <span class="number">1</span> ;</span><br><span class="line">            end</span><br><span class="line"></span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">imshow(st)%输出用prewitt模板处理后的梯度幅值图片</span><br><span class="line">title(<span class="string">'最终输出的图像'</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（四）图像梯度处理 </title>
    <url>/posts/33b97531.html</url>
    <content><![CDATA[<h1 id="Sobel算子"><a href="#Sobel算子" class="headerlink" title="Sobel算子"></a>Sobel算子</h1><p>梯度是图像中两个实体的边缘，我们可以算一算这个点的左边是什么，右边是什么，上面是什么，下面是什么，因此 我们主要考虑两个方向———上下、左右。</p>
<p><img src="/posts/CV/sobel_1.png" alt></p>
<p>在上面这张图中，A是一个3×3的矩阵，我们计算左右梯度也就是左乘上图左边的矩阵（卷积核），；同理我们计算上下梯度就是左乘上图右边的矩阵。我们可以看到相邻的像素值的权重会被增大，这不正是前面说的高斯滤波的感觉了吗。综上所述，就是下减上，右减左。</p>
<p>我们先导入要计算梯度的图片：</p>
<p><img src="/posts/CV/pie.png" alt></p>
<p>导入图片并定义show函数的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'pie.png'</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line">cv2.imshow(<span class="string">"img"</span>,img)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv_show</span><span class="params">(img,name)</span>:</span></span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey()</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>我们看一下Sobel算子函数的调用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dst = cv2.Sobel(src, depth, dx, dy, ksize)</span><br></pre></td></tr></table></figure>
<ul>
<li>depth:图像的深度，一般指定为-1即可</li>
<li>dx和dy分别表示水平和竖直方向</li>
<li>ksize是Sobel算子的大小</li>
</ul>
<p>我们使用一下Sobel函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sobelx = cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">3</span>)</span><br><span class="line">cv_show(sobelx,<span class="string">'sobelx'</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/sobel1.png" alt></p>
<p>我们看到传入了一个新的参数：cv2.CV_64F，这个参数是什么意思呢？我们在计算梯度的时候结果不一定是正数，假如结果是负数，图像的像素默认是0-255，一般情况下在图像展示时会被归为0，因此上如图右边本该是白色的框框就变成黑色了。后面对应dx，dy的位置分别为1和0，这意味着只看x轴不看y轴。</p>
<p>我们如果想让右边被抹掉的框框也变为白色，我们可以对相减的得到负数取绝对值，也就是把得到的结果做转换，具体操作为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sobelx = cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)</span><br><span class="line">cv_show(sobelx,<span class="string">'sobelx'</span>)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/posts/CV/sobel2.png" alt></p>
<p>同理我们可以对y方向的梯度也做同样的操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sobely = cv2.Sobel(img,cv2.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line">cv_show(sobely,<span class="string">'sobely'</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/sobel3.png" alt></p>
<p>现在我们得到了x方向的梯度和y方向的梯度，我们可以计算一个总和，我们可以计算平方和开根号，也可以取绝对值相加，我们在下面对得到的x和y进行求和：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sobelxy = cv2.addWeighted(sobelx,<span class="number">0.5</span>,sobely,<span class="number">0.5</span>,<span class="number">0</span>)</span><br><span class="line">cv_show(sobelxy,<span class="string">'sobelxy'</span>)</span><br></pre></td></tr></table></figure>
<p>下面是得到的图像的效果：</p>
<p><img src="/posts/CV/sobel4.png" alt></p>
<p>我们在这里不建议直接计算，直接计算方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sobelxy=cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">1</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobelxy = cv2.convertScaleAbs(sobelxy)</span><br><span class="line">cv_show(sobelxy,<span class="string">'sobelxy'</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/sobel5.png" alt></p>
<p>我们可以看到直接计算的效果不是很好，下面我们对Lena的图像进行操作，原始图像为：</p>
<p><img src="/posts/CV/lena.jpg" alt></p>
<p>转化后的图像为：</p>
<p><img src="/posts/CV/lenaSobel.jpg" alt></p>
<p>代码为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'lena.jpg'</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line">sobelx = cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)</span><br><span class="line">sobely = cv2.Sobel(img,cv2.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line">sobelxy = cv2.addWeighted(sobelx,<span class="number">0.5</span>,sobely,<span class="number">0.5</span>,<span class="number">0</span>)</span><br><span class="line">cv_show(sobelxy,<span class="string">'sobelxy'</span>)</span><br></pre></td></tr></table></figure>
<p>下面我们再介绍两个算子——Scharr算子和Laplacian算子</p>
<h1 id="其他算子"><a href="#其他算子" class="headerlink" title="其他算子"></a>其他算子</h1><p>我们先直观地看一下这两个算子的卷积核：</p>
<p><strong>Scharr算子</strong></p>
<p><img src="/posts/CV/scharr.png" alt></p>
<p><strong>Laplacian算子</strong></p>
<p><img src="/posts/CV/l.png" alt></p>
<p>Scharr算子对结果的差异更敏感一些，而Laplacian算子相比于Sobel算子以及Scharr算子会对一些变化更敏感，因为Laplacian算子使用了二阶导，也就是一阶导的变化率，但Laplacian的缺陷是对噪音点比较敏感，因此一般Laplacian算子并不会单独使用，一般是结合其他工具使用。</p>
<p>下面我们进行一个对比试验：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'lena.jpg'</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line">sobelx = cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobely = cv2.Sobel(img,cv2.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize=<span class="number">3</span>)</span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)</span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line">sobelxy =  cv2.addWeighted(sobelx,<span class="number">0.5</span>,sobely,<span class="number">0.5</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scharrx = cv2.Scharr(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">scharry = cv2.Scharr(img,cv2.CV_64F,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">scharrx = cv2.convertScaleAbs(scharrx)</span><br><span class="line">scharry = cv2.convertScaleAbs(scharry)</span><br><span class="line">scharrxy =  cv2.addWeighted(scharrx,<span class="number">0.5</span>,scharry,<span class="number">0.5</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">laplacian = cv2.Laplacian(img,cv2.CV_64F)</span><br><span class="line">laplacian = cv2.convertScaleAbs(laplacian)</span><br><span class="line"></span><br><span class="line">res = np.hstack((sobelxy,scharrxy,laplacian))</span><br><span class="line">cv_show(res,<span class="string">'res'</span>)</span><br></pre></td></tr></table></figure>
<p>对比结果为：</p>
<p><img src="/posts/CV/concat.jpg" alt></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（三）图像形态学处理</title>
    <url>/posts/1f4ed7b2.html</url>
    <content><![CDATA[<h1 id="腐蚀操作"><a href="#腐蚀操作" class="headerlink" title="腐蚀操作"></a>腐蚀操作</h1><p>图像腐蚀操作的前提（大多为）图像数据为二值，我们看下面这张图：</p>
<p><img src="/posts/CV/dige.png" alt></p>
<p>我们看到这两个字的很多地方有溢出来的白线，我们希望去掉这些白线，即转化后结果为：</p>
<p><img src="/posts/CV/fushi.png" alt></p>
<p>同时我们还观察到上图的线条都变细了，这就是腐蚀操作做的事，我们再来仔细看一下腐蚀操作：</p>
<p>原图如下：</p>
<p><img src="/posts/CV/pie.png" alt></p>
<p>腐蚀操作后如下：</p>
<p><img src="/posts/CV/pie2.png" alt></p>
<p>我们在上面进行了如下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">30</span>,<span class="number">30</span>),np.uint8)</span><br><span class="line">erosion_1 = cv2.erode(pie,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">erosion_2 = cv2.erode(pie,kernel,iterations = <span class="number">2</span>)</span><br><span class="line">erosion_3 = cv2.erode(pie,kernel,iterations = <span class="number">3</span>)</span><br><span class="line">res = np.hstack((erosion_1,erosion_2,erosion_3))</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'res'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>也就是进行了不同次数的腐蚀，第一张图循环次数是一次，第二张图两次，第三张图三次，依次展现出上面的图像，并且我们注意到我们的kernel是30×30的，如果我们指定更大的kernel值，比如50×50，是否预期会有更大的腐蚀面积（白色的区域更少），我们看一看实验结果：</p>
<p><img src="/posts/CV/pie3.png" alt></p>
<p>我们看到果然是符合预期的。</p>
<h1 id="膨胀操作"><a href="#膨胀操作" class="headerlink" title="膨胀操作"></a>膨胀操作</h1><p>腐蚀的逆操作不就是膨胀操作吗，比如我们最开始将字上面的刺儿都去掉了，但线条变细了，我们想让它变粗一些，我们可以执行膨胀操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.uint8)</span><br><span class="line">dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'dilate'</span>, dige_dilate)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>和前面的腐蚀操作一样，这里需要定义kernel和iterations，我们会用框框（由kernel大小定义）去框住图片中的区域，每框一次如果有白色的像素点我们就会将整个框框内都变为白色，这不正好和腐蚀操作相反吗。我们同样对前面的初始园执行膨胀操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pie = cv2.imread(<span class="string">'pie.png'</span>)</span><br><span class="line"></span><br><span class="line">kernel = np.ones((<span class="number">30</span>,<span class="number">30</span>),np.uint8) </span><br><span class="line">dilate_1 = cv2.dilate(pie,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">dilate_2 = cv2.dilate(pie,kernel,iterations = <span class="number">2</span>)</span><br><span class="line">dilate_3 = cv2.dilate(pie,kernel,iterations = <span class="number">3</span>)</span><br><span class="line">res = np.hstack((dilate_1,dilate_2,dilate_3))</span><br><span class="line">cv2.imshow(<span class="string">'res'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/pie4.png" alt></p>
<h1 id="开运算"><a href="#开运算" class="headerlink" title="开运算"></a>开运算</h1><p><strong>对于最开始的任务，我们希望能既去掉毛刺又不破坏原来的图像，我们会进行一个腐蚀后膨胀的操作，开运算正是帮我们省去了这两部，直接一步到位~</strong>，具体操作为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开：先腐蚀，再膨胀</span></span><br><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line"></span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8) </span><br><span class="line">opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'opening'</span>, opening)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>结果与我们做了两步操作后相同：</p>
<p><img src="/posts/CV/dege45.png" alt></p>
<h1 id="闭运算"><a href="#闭运算" class="headerlink" title="闭运算"></a>闭运算</h1><p>闭运算就和开运算相反，先做膨胀后做腐蚀：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 闭：先膨胀，再腐蚀</span></span><br><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line"></span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8) </span><br><span class="line">closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'closing'</span>, closing)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>在这张图上，旁边的线反而被加粗了。</p>
<p><img src="/posts/CV/dege12.png" alt></p>
<h1 id="梯度运算"><a href="#梯度运算" class="headerlink" title="梯度运算"></a>梯度运算</h1><p>当我们想要关注边界信息时，我们可以对原图像执行一次腐蚀得到p1，再对原图像执行一次膨胀操作得到p2，用p2-p1就得到了边界的信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'gradient'</span>, gradient)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>打印结果为：</p>
<p><img src="/posts/CV/tidu.png" alt></p>
<h1 id="礼帽和黑帽"><a href="#礼帽和黑帽" class="headerlink" title="礼帽和黑帽"></a>礼帽和黑帽</h1><ul>
<li>礼帽 = 原始输入-开运算结果</li>
<li>黑帽 = 闭运算-原始输入</li>
</ul>
<p>我们看到礼帽就是最开始剩下的那些刺的全体，而黑帽就是变粗之后的刺，具体调用如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#礼帽</span></span><br><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line">tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)</span><br><span class="line">cv2.imshow(<span class="string">'tophat'</span>, tophat)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（二）阈值与平滑处理</title>
    <url>/posts/5d7ec7e5.html</url>
    <content><![CDATA[<h1 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a>图像阈值</h1><p><strong>ret, dst = cv2.threshold(src, thresh, maxval, type)</strong></p>
<ul>
<li>src： 输入图，只能输入单通道图像，通常来说为灰度图</li>
<li>dst： 输出图</li>
<li>thresh： 阈值</li>
<li>maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值</li>
<li><p>type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；cv2.THRESH_TOZERO_INV</p>
</li>
<li><p>cv2.THRESH_BINARY           超过阈值部分取maxval（最大值），否则取0</p>
</li>
<li>cv2.THRESH_BINARY_INV    THRESH_BINARY的反转</li>
<li>cv2.THRESH_TRUNC            大于阈值部分设为阈值，否则不变</li>
<li>cv2.THRESH_TOZERO          大于阈值部分不改变，否则设为0</li>
<li>cv2.THRESH_TOZERO_INV  THRESH_TOZERO的反转</li>
</ul>
<p>原图像为：</p>
<p><img src="/posts/CV/cat.jpg" alt></p>
<p>对上述阈值处理的具体操作如下：</p>
<p><img src="/posts/CV/tc1.png" alt></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ret, thresh1 = cv2.threshold(img_gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">ret, thresh2 = cv2.threshold(img_gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY_INV)</span><br><span class="line">ret, thresh3 = cv2.threshold(img_gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_TRUNC)</span><br><span class="line">ret, thresh4 = cv2.threshold(img_gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_TOZERO)</span><br><span class="line">ret, thresh5 = cv2.threshold(img_gray, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_TOZERO_INV)</span><br><span class="line"></span><br><span class="line">titles = [<span class="string">'Original Image'</span>, <span class="string">'BINARY'</span>, <span class="string">'BINARY_INV'</span>, <span class="string">'TRUNC'</span>, <span class="string">'TOZERO'</span>, <span class="string">'TOZERO_INV'</span>]</span><br><span class="line">images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">3</span>, i + <span class="number">1</span>), plt.imshow(images[i], <span class="string">'gray'</span>)</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line">    plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a>图像平滑</h1><p>目的是通过滤波等操作尽可能去掉图像上的噪音点。我们首先看一张带有噪声的lena图片：</p>
<p><img src="/posts/CV/lenaNoise.png" alt></p>
<h2 id="均值滤波"><a href="#均值滤波" class="headerlink" title="均值滤波"></a>均值滤波</h2><p>我们要做平滑处理，就是需要对一定窗口大小的图片矩阵进行变化，我们可以做均值滤波，也就是将窗口内像素点取平均代替原有值，具体实现也十分简单，只需要传入图片以及滤波窗口大小即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blur = cv2.blur(img, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">cv2.imshow(<span class="string">'blur'</span>, blur)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/blur.jpg" alt></p>
<h2 id="方框滤波"><a href="#方框滤波" class="headerlink" title="方框滤波"></a>方框滤波</h2><p>除此之外，我们还可以构建方框滤波，方框滤波和均值滤波几乎相同 ，不过比之多了两个参数，颜色通道是否一致，若希望一致则填入-1，通常都不需要改动；另外还有normalize，也就是是否需要做归一化操作，当不适用normalize时可能会发生越界现象，而当normalize指定为True时方框滤波就与均值滤波完全一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">box = cv2.boxFilter(img,<span class="number">-1</span>,(<span class="number">3</span>,<span class="number">3</span>), normalize=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'box'</span>, box)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>若指定normalize=False，显示图像为：</p>
<p><img src="/posts/CV/blur2.jpg" alt></p>
<p>我们看到大多数地方是全白一片，我们可以感觉到，当值之和大于255时，会直接将值定为255，不再如前面所述会取模。</p>
<h2 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h2><p>下面是著名的高斯滤波，高斯滤波会对同个窗口的不同位置的值设置不同的权重，最终加权平均取得这个窗口的最终值，对于一个3×3的窗口而言，上下左右离中心的距离最近，会将权重设的稍微高些，而四个角落离得距离较远，因此就会将权重设的稍微低些，比之均值滤波更合理，也就相当于构建权重矩阵进行滤波。</p>
<p>下面是高斯滤波的调用方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 高斯滤波</span></span><br><span class="line"><span class="comment"># 高斯模糊的卷积核里的数值是满足高斯分布，相当于更重视中间的</span></span><br><span class="line">aussian = cv2.GaussianBlur(img, (<span class="number">5</span>, <span class="number">5</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'aussian'</span>, aussian)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p><img src="/posts/CV/aussian.jpg" alt></p>
<p>我们可以看出高斯滤波的噪音点没有那么明显了。</p>
<h2 id="中值滤波"><a href="#中值滤波" class="headerlink" title="中值滤波"></a>中值滤波</h2><p>中值就是将数从小到大排序取中位数，当我们数据有噪音点时，中值滤波可以立刻去除噪音点。</p>
<p>具体调用如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 中值滤波</span></span><br><span class="line"><span class="comment"># 相当于用中值代替</span></span><br><span class="line">median = cv2.medianBlur(img, <span class="number">5</span>)  <span class="comment"># 中值滤波</span></span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'median'</span>, median)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>滤波结果如下：</p>
<p><img src="/posts/CV/median.jpg" alt></p>
<p>最终我们可以通过下面操作将所有图像拼接在一起：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = np.hstack((blur,aussian,median))</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'median vs average'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/CV/concate.jpg" alt></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Opencv计算机视觉基础：（一）图像基本操作</title>
    <url>/posts/f4ba5069.html</url>
    <content><![CDATA[<p>首先展示以下实例原图像给大家萌一萌：</p>
<p><img src="/posts/CV/animal.jpg" alt></p>
<h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>下面直接列举需要搭建的环境：</p>
<ol>
<li>Anaconda+python大环境（相信大家都早已安装好了，若没有请前往百度搜索自行安装）</li>
<li>opencv以及扩展包的安装：<ul>
<li>直接在命令行输入以下两行分别安装即可：<blockquote>
<p>pip install opencv-python<br>pip install opencv-contrib-python</p>
</blockquote>
</li>
<li>若在第一个opencv-python中指定了版本号，比如（pip install opencv-python==4.2.0），需要注意在第二行的安装中也需要输入相同的版本号，opencv-contrib-python相当于给opencv-python添加了额外的扩展，比如特征提取的算法等。</li>
<li>若安装出错建议多安装几次（很可能是网络的问题），实在安装不了可以直接去官网下载，需要注意选择适当的python版本以及windows位数的安装包进行下载</li>
<li>安装完之后进入python，导入相应的包并查看版本号，若无异常就可以进入正题啦~<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">import</span> cv2</span><br><span class="line">cv2.__version__</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h1 id="图像读取"><a href="#图像读取" class="headerlink" title="图像读取"></a>图像读取</h1><p>我们看到的每一张彩色图片事实上都是由RGB三个颜色通道堆叠而成，每个通道实际上是一个矩阵，里面的每个元素大小从0-256不等，对于灰度图则只有一个通道。</p>
<p>我们很容易通过以下代码验证：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">"Pic/animal.jpg"</span>)</span><br><span class="line">print(img)</span><br><span class="line"></span><br><span class="line">img2 = cv2.imread(<span class="string">"Pic/animal.jpg"</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">print(img2)</span><br></pre></td></tr></table></figure>
<p>当我们想要查看图像，可以通过下面三行代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imshow(<span class="string">"image1"</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>上面第一行imshow是用于展示图像；第二行waitKey是控制图片显示时间，若设置为0则当鼠标单击图像会消失，若设置为一个&gt;0的数，该数会转化成毫秒显示图像，比如可以试试20000；第三行是控制窗口关闭的，要注意区分cv2.destroyAllWindows()和cv2.destroyWindow()的区别。为了便于调用，可以将这三行代码写成一个函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv_show</span><span class="params">(name, img)</span>:</span></span><br><span class="line">    cv2.imshow(name, img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>可以通过img.shape显示图像形状：</p>
<p><img src="/posts/CV/shape.png" alt></p>
<p>正如前例所述，图像转换的命令为：</p>
<ul>
<li><p>cv2.IMREAD_COLOR:彩色图像</p>
</li>
<li><p>cv2.IMREAD_GRAYSCACLE:灰度图像</p>
</li>
</ul>
<h1 id="视频读取"><a href="#视频读取" class="headerlink" title="视频读取"></a>视频读取</h1><p>读取视频的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vc = cv2.VideoCapture(<span class="string">"Video/turtle.mp4"</span>)</span><br></pre></td></tr></table></figure>
<p>可以通过下面代码读取视频每一帧：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> vc.isOpened():</span><br><span class="line">    open, frame = vc.read()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    open = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>可以通过下面完整代码调节视频速度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vc = cv2.VideoCapture(<span class="string">"Video/turtle.mp4"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> open:</span><br><span class="line">    ret, frame = vc.read()</span><br><span class="line">    <span class="keyword">if</span> frame <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> ret == <span class="literal">True</span>:</span><br><span class="line">        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        cv2.imshow(<span class="string">"result1"</span>, gray)</span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">100</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">vc.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>其主要思想是将视频转化为帧，然后定时一张张播放图片，可以修改waitKey控制播放进度。</p>
<h1 id="图片基本操作"><a href="#图片基本操作" class="headerlink" title="图片基本操作"></a>图片基本操作</h1><h2 id="截取图片及像素转换"><a href="#截取图片及像素转换" class="headerlink" title="截取图片及像素转换"></a>截取图片及像素转换</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"Pic/animal.jpg"</span>)</span><br><span class="line">cat = img[<span class="number">0</span>:<span class="number">200</span>,<span class="number">0</span>:<span class="number">200</span>]</span><br><span class="line">cv_show(<span class="string">"cat"</span>, cat)</span><br></pre></td></tr></table></figure>
<p>上述直接通过索引截取对应的部分矩阵（图像），可以通过以下代码实现分割RGB通道：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b, g, r = cv2.split(img)</span><br></pre></td></tr></table></figure>
<p>这其中需要注意的是，通道的顺序并不是RGB，而是BGR！</p>
<p>我们切分完之后，若希望看各个通道的图像，可以通过下面三块代码实现：</p>
<p><img src="/posts/CV/rgb.png" alt></p>
<p>其中红图的结果如下：</p>
<p><img src="/posts/CV/B.png" alt></p>
<p>若想要重新将bgr通道组合在一起，可以通过merge函数实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.merge((b, g, r))</span><br><span class="line">img.shape</span><br></pre></td></tr></table></figure>
<h2 id="边界填充"><a href="#边界填充" class="headerlink" title="边界填充"></a>边界填充</h2><p>我们做卷积时，常常会给图像做一个边界的填充，具体实现方式及效果如下：</p>
<p><img src="/posts/CV/border1.png" alt></p>
<p><img src="/posts/CV/border2.png" alt></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">top_size, bottom_size, left_size, right_size = (<span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">replicate = cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_REPLICATE)</span><br><span class="line">reflect = cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_REFLECT)</span><br><span class="line">reflect101 = cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_REFLECT_101)</span><br><span class="line">wrap = cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_WRAP)</span><br><span class="line">constant = cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_CONSTANT, value=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.subplot(<span class="number">231</span>), plt.imshow(img, <span class="string">"gray"</span>), plt.title(<span class="string">"ORIGINAL"</span>)</span><br><span class="line">plt.subplot(<span class="number">232</span>), plt.imshow(replicate, <span class="string">"gray"</span>), plt.title(<span class="string">"REPLICATE"</span>)</span><br><span class="line">plt.subplot(<span class="number">233</span>), plt.imshow(reflect, <span class="string">"gray"</span>), plt.title(<span class="string">"REFLECT"</span>)</span><br><span class="line">plt.subplot(<span class="number">234</span>), plt.imshow(reflect101, <span class="string">"gray"</span>), plt.title(<span class="string">"REFLECT_101"</span>)</span><br><span class="line">plt.subplot(<span class="number">235</span>), plt.imshow(wrap, <span class="string">"gray"</span>), plt.title(<span class="string">"WRAP"</span>)</span><br><span class="line">plt.subplot(<span class="number">236</span>), plt.imshow(constant, <span class="string">"gray"</span>), plt.title(<span class="string">"CONSTANT"</span>)</span><br><span class="line">plt.savefig(<span class="string">"border2.png"</span>)</span><br></pre></td></tr></table></figure>
<p>下面解释一下这几个填充原理：</p>
<ul>
<li>第一个是BORDER_REPLICATE，顾名思义就是直接复制边界像素；</li>
<li>第二个BORDER_REFLECT是反射法，也就是对感兴趣的图像中的像素子两边进行复制，比如fedcba|abcdefgh|hgfedcb；</li>
<li>第三个BORDER_REFLECT_101也是反射法，但是以最边缘像素为轴对称反射；</li>
<li>第四个BORDER_WRAP是外包装法：cdefgh|abcdefgh|abcdefg</li>
<li>第五个为常量法，使用常数值填充（在图中很明显为黑色边框）</li>
</ul>
<h2 id="数值计算"><a href="#数值计算" class="headerlink" title="数值计算"></a>数值计算</h2><p>我们可以直接把图片所有像素点做同一运算：比如下图所示的给所有像素加10：</p>
<p><img src="/posts/CV/add.png" alt></p>
<p>若两图像维度一致也可以直接相加：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_1 = img_ + img</span><br><span class="line">cv2.imwrite(<span class="string">"add.png"</span>, img_1)</span><br><span class="line">cv_show(<span class="string">"Add"</span>, img_1)</span><br></pre></td></tr></table></figure>
<p>得到的图像也挺有趣的：</p>
<p><img src="/posts/CV/add1.png" alt></p>
<p>我们可以感觉到有些像素点变化有些大，这是由于当两个图像的像素点直接相加时，由于像素最高为256，若越界就会自动对256取余，因此图像有些部分比较怪异。</p>
<h2 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h2><p>我们可以将两张同大小的图片按比例融合到一张图片中，当我们发现这两张图片大小不一致，我们需要将图片resize成大小一致的图片方能进行融合，操作如下：</p>
<p><img src="/posts/CV/merge.png" alt></p>
<p>结果如下：</p>
<p><img src="/posts/CV/merge1.png" alt></p>
<p>具体操作为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = cv2.addWeighted(img1, <span class="number">0.4</span>, img3, <span class="number">0.6</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>其中0.4与0.6分别为这两张图片的权重，最后的0为偏置项，可以设置为其他值调节亮度。</p>
<p>另外，resize操作还可以不指定数值，而只是写出图像倍数关系，比如 ：</p>
<p><img src="/posts/CV/resize1.png" alt></p>
<p>具体操作如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res1 = cv2.resize(res, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">3</span>,fy=<span class="number">1</span>)</span><br><span class="line">plt.imshow(res1)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Keras深度学习(二）深度学习用于计算机视觉</title>
    <url>/posts/c8c4a7ed.html</url>
    <content><![CDATA[<h1 id="mnist-数据集"><a href="#mnist-数据集" class="headerlink" title="mnist 数据集"></a>mnist 数据集</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 3, 3, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 576)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                36928     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
_________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(train_data, train_labels), (test_data, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = train_data.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_data = train_data.astype(<span class="string">"float32"</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_data = test_data.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_data = test_data.astype(<span class="string">"float32"</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">             loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_data, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/5
60000/60000 [==============================] - 23s 385us/step - loss: 0.1701 - accuracy: 0.9471
Epoch 2/5
60000/60000 [==============================] - 22s 366us/step - loss: 0.0471 - accuracy: 0.9857
Epoch 3/5
60000/60000 [==============================] - 22s 372us/step - loss: 0.0325 - accuracy: 0.9895
Epoch 4/5
60000/60000 [==============================] - 23s 391us/step - loss: 0.0249 - accuracy: 0.9924
Epoch 5/5
60000/60000 [==============================] - 23s 383us/step - loss: 0.0199 - accuracy: 0.9941





&lt;keras.callbacks.callbacks.History at 0x186eaf2d898&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(test_data, test_labels)</span><br><span class="line">print(test_loss, test_acc)</span><br></pre></td></tr></table></figure>
<pre><code>10000/10000 [==============================] - 1s 99us/step
0.027024587894159596 0.991599977016449
</code></pre><h1 id="猫狗分类数据集"><a href="#猫狗分类数据集" class="headerlink" title="猫狗分类数据集"></a>猫狗分类数据集</h1><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line">original_dataset_dir = <span class="string">'data/cat_dog/train/'</span></span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">os.mkdir(base_dir)</span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">os.mkdir(train_dir)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">os.mkdir(validation_dir)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">os.mkdir(test_dir)</span><br><span class="line"></span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(train_cats_dir)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(train_dogs_dir)</span><br><span class="line"></span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(validation_cats_dir)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(validation_dogs_dir)</span><br><span class="line"></span><br><span class="line">test_cats_dir = os.path.join(test_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(test_cats_dir)</span><br><span class="line">test_dogs_dir = os.path.join(test_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(test_dogs_dir)</span><br><span class="line"></span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(train_cats_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(validation_cats_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(test_cats_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">    </span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(train_dogs_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(validation_dogs_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">    src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">    dst = os.path.join(test_dogs_dir, fname)</span><br><span class="line">    shutil.copyfile(src, dst)</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'total training cat images:'</span>, len(os.listdir(train_cats_dir)))</span><br><span class="line">print(<span class="string">'total training dog images:'</span>, len(os.listdir(train_dogs_dir)))</span><br><span class="line">print(<span class="string">'total validation cat images:'</span>, len(os.listdir(validation_cats_dir)))</span><br><span class="line">print(<span class="string">'total validation dog images:'</span>, len(os.listdir(validation_dogs_dir)))</span><br><span class="line">print(<span class="string">'total test cat images:'</span>, len(os.listdir(test_cats_dir)))</span><br><span class="line">print(<span class="string">'total test dog images:'</span>, len(os.listdir(test_dogs_dir)))</span><br></pre></td></tr></table></figure>
<pre><code>total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
total test cat images: 500
total test dog images: 500
</code></pre><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用ImageDataGenerator做数据增强</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line">fnames = [os.path.join(train_cats_dir, fname) <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(train_cats_dir)]</span><br><span class="line">img_path = fnames[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = x.reshape((<span class="number">1</span>, )+x.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> datagen.flow(x, batch_size=<span class="number">1</span>):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    imgplot = plt.imshow(image.array_to_img(batch[<span class="number">0</span>]))</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">4</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_14_0-1596467952803.png" alt="png"></p>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_14_1.png" alt="png"></p>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_14_2.png" alt="png"></p>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_14_3.png" alt="png"></p>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">"binary_crossentropy"</span>,</span><br><span class="line">             optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),</span><br><span class="line">             metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用数据增强生成器训练卷积神经网络</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,)</span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    validation_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/100
100/100 [==============================] - 74s 740ms/step - loss: 0.6927 - acc: 0.5167 - val_loss: 0.6583 - val_acc: 0.5933
Epoch 2/100
100/100 [==============================] - 68s 678ms/step - loss: 0.6757 - acc: 0.5707 - val_loss: 0.6161 - val_acc: 0.5670
Epoch 3/100
100/100 [==============================] - 65s 645ms/step - loss: 0.6539 - acc: 0.6074 - val_loss: 0.6832 - val_acc: 0.6237
Epoch 4/100
100/100 [==============================] - 66s 656ms/step - loss: 0.6368 - acc: 0.6222 - val_loss: 0.6641 - val_acc: 0.6553
Epoch 5/100
100/100 [==============================] - 66s 658ms/step - loss: 0.6161 - acc: 0.6648 - val_loss: 0.5204 - val_acc: 0.6789
Epoch 6/100
100/100 [==============================] - 69s 693ms/step - loss: 0.6063 - acc: 0.6734 - val_loss: 0.5975 - val_acc: 0.6740
Epoch 7/100
100/100 [==============================] - 70s 702ms/step - loss: 0.6009 - acc: 0.6692 - val_loss: 0.4627 - val_acc: 0.7005
Epoch 8/100
100/100 [==============================] - 77s 768ms/step - loss: 0.5921 - acc: 0.6831 - val_loss: 0.8519 - val_acc: 0.6321
Epoch 9/100
100/100 [==============================] - 88s 881ms/step - loss: 0.5940 - acc: 0.6783 - val_loss: 0.4899 - val_acc: 0.7049
Epoch 10/100
100/100 [==============================] - 83s 832ms/step - loss: 0.5801 - acc: 0.6913 - val_loss: 0.4796 - val_acc: 0.6980
Epoch 11/100
100/100 [==============================] - 70s 697ms/step - loss: 0.5684 - acc: 0.7015 - val_loss: 0.5900 - val_acc: 0.7255
Epoch 12/100
100/100 [==============================] - 70s 702ms/step - loss: 0.5599 - acc: 0.7092 - val_loss: 0.5051 - val_acc: 0.7030
Epoch 13/100
100/100 [==============================] - 70s 698ms/step - loss: 0.5526 - acc: 0.7181 - val_loss: 0.5009 - val_acc: 0.7101
Epoch 14/100
100/100 [==============================] - 71s 708ms/step - loss: 0.5602 - acc: 0.7129 - val_loss: 0.6177 - val_acc: 0.7088
Epoch 15/100
100/100 [==============================] - 74s 738ms/step - loss: 0.5401 - acc: 0.7177 - val_loss: 0.7140 - val_acc: 0.6591
Epoch 16/100
100/100 [==============================] - 72s 725ms/step - loss: 0.5472 - acc: 0.7186 - val_loss: 0.8219 - val_acc: 0.7384
Epoch 17/100
100/100 [==============================] - 70s 704ms/step - loss: 0.5407 - acc: 0.7180 - val_loss: 0.4693 - val_acc: 0.7367
Epoch 18/100
100/100 [==============================] - 70s 704ms/step - loss: 0.5300 - acc: 0.7301 - val_loss: 0.6182 - val_acc: 0.7191
Epoch 19/100
100/100 [==============================] - 71s 710ms/step - loss: 0.5297 - acc: 0.7271 - val_loss: 0.5767 - val_acc: 0.7379
Epoch 20/100
100/100 [==============================] - 70s 700ms/step - loss: 0.5320 - acc: 0.7345 - val_loss: 0.4792 - val_acc: 0.7616
Epoch 21/100
100/100 [==============================] - 70s 702ms/step - loss: 0.5228 - acc: 0.7393 - val_loss: 0.6806 - val_acc: 0.7240
Epoch 22/100
100/100 [==============================] - 69s 690ms/step - loss: 0.5190 - acc: 0.7421 - val_loss: 0.5098 - val_acc: 0.7610
Epoch 23/100
100/100 [==============================] - 68s 679ms/step - loss: 0.5184 - acc: 0.7481 - val_loss: 0.6571 - val_acc: 0.7665
Epoch 24/100
100/100 [==============================] - 68s 677ms/step - loss: 0.5054 - acc: 0.7484 - val_loss: 0.4710 - val_acc: 0.7506
Epoch 25/100
100/100 [==============================] - 69s 686ms/step - loss: 0.5080 - acc: 0.7412 - val_loss: 0.5399 - val_acc: 0.7468
Epoch 26/100
100/100 [==============================] - 68s 679ms/step - loss: 0.4945 - acc: 0.7644 - val_loss: 0.2892 - val_acc: 0.7602
Epoch 27/100
100/100 [==============================] - 67s 671ms/step - loss: 0.5005 - acc: 0.7449 - val_loss: 0.5151 - val_acc: 0.7552
Epoch 28/100
100/100 [==============================] - 68s 677ms/step - loss: 0.5062 - acc: 0.7465 - val_loss: 0.3623 - val_acc: 0.7659
Epoch 29/100
100/100 [==============================] - 68s 678ms/step - loss: 0.4914 - acc: 0.7595 - val_loss: 0.3824 - val_acc: 0.7964
Epoch 30/100
100/100 [==============================] - 81s 807ms/step - loss: 0.4937 - acc: 0.7566 - val_loss: 0.4665 - val_acc: 0.6942
Epoch 31/100
100/100 [==============================] - 80s 796ms/step - loss: 0.4852 - acc: 0.7641 - val_loss: 0.5380 - val_acc: 0.7506
Epoch 32/100
100/100 [==============================] - 78s 782ms/step - loss: 0.4774 - acc: 0.7698 - val_loss: 0.3291 - val_acc: 0.7668
Epoch 33/100
100/100 [==============================] - 75s 752ms/step - loss: 0.4834 - acc: 0.7636 - val_loss: 0.3217 - val_acc: 0.7805
Epoch 34/100
100/100 [==============================] - 72s 722ms/step - loss: 0.4784 - acc: 0.7677 - val_loss: 0.4656 - val_acc: 0.7397
Epoch 35/100
100/100 [==============================] - 77s 772ms/step - loss: 0.4717 - acc: 0.7737 - val_loss: 0.3206 - val_acc: 0.7887
Epoch 36/100
100/100 [==============================] - 70s 695ms/step - loss: 0.4781 - acc: 0.7582 - val_loss: 0.5194 - val_acc: 0.7932
Epoch 37/100
100/100 [==============================] - 69s 686ms/step - loss: 0.4530 - acc: 0.7828 - val_loss: 0.2837 - val_acc: 0.7786
Epoch 38/100
100/100 [==============================] - 68s 684ms/step - loss: 0.4778 - acc: 0.7692 - val_loss: 0.4444 - val_acc: 0.7674
Epoch 39/100
100/100 [==============================] - 78s 780ms/step - loss: 0.4629 - acc: 0.7753 - val_loss: 0.5315 - val_acc: 0.7627
Epoch 40/100
100/100 [==============================] - 83s 831ms/step - loss: 0.4606 - acc: 0.7792 - val_loss: 0.5177 - val_acc: 0.7564
Epoch 41/100
100/100 [==============================] - 84s 838ms/step - loss: 0.4597 - acc: 0.7917 - val_loss: 0.4885 - val_acc: 0.7680
Epoch 42/100
100/100 [==============================] - 76s 764ms/step - loss: 0.4475 - acc: 0.7820 - val_loss: 0.3850 - val_acc: 0.7868
Epoch 43/100
100/100 [==============================] - 85s 849ms/step - loss: 0.4532 - acc: 0.7891 - val_loss: 0.4080 - val_acc: 0.7957
Epoch 44/100
100/100 [==============================] - 73s 733ms/step - loss: 0.4655 - acc: 0.7715 - val_loss: 0.5390 - val_acc: 0.7779
Epoch 45/100
100/100 [==============================] - 68s 684ms/step - loss: 0.4479 - acc: 0.7871 - val_loss: 0.3279 - val_acc: 0.7751
Epoch 46/100
100/100 [==============================] - 68s 680ms/step - loss: 0.4452 - acc: 0.7948 - val_loss: 0.4211 - val_acc: 0.7506
Epoch 47/100
100/100 [==============================] - 69s 689ms/step - loss: 0.4403 - acc: 0.7945 - val_loss: 0.4955 - val_acc: 0.7809
Epoch 48/100
100/100 [==============================] - 82s 824ms/step - loss: 0.4535 - acc: 0.7885 - val_loss: 0.2140 - val_acc: 0.7951
Epoch 49/100
100/100 [==============================] - 86s 861ms/step - loss: 0.4466 - acc: 0.7899 - val_loss: 0.3635 - val_acc: 0.7786
Epoch 50/100
100/100 [==============================] - 84s 844ms/step - loss: 0.4287 - acc: 0.8075 - val_loss: 0.5638 - val_acc: 0.7210
Epoch 51/100
100/100 [==============================] - 92s 916ms/step - loss: 0.4411 - acc: 0.7904 - val_loss: 0.5665 - val_acc: 0.7779
Epoch 52/100
100/100 [==============================] - 91s 908ms/step - loss: 0.4360 - acc: 0.7904 - val_loss: 0.3423 - val_acc: 0.8009
Epoch 53/100
100/100 [==============================] - 72s 717ms/step - loss: 0.4210 - acc: 0.8015 - val_loss: 0.9407 - val_acc: 0.7151
Epoch 54/100
100/100 [==============================] - 70s 702ms/step - loss: 0.4283 - acc: 0.7999 - val_loss: 0.5640 - val_acc: 0.7726
Epoch 55/100
100/100 [==============================] - 71s 708ms/step - loss: 0.4223 - acc: 0.8040 - val_loss: 0.7437 - val_acc: 0.7855
Epoch 56/100
100/100 [==============================] - 75s 745ms/step - loss: 0.4262 - acc: 0.7970 - val_loss: 0.3689 - val_acc: 0.7796
Epoch 57/100
100/100 [==============================] - 93s 928ms/step - loss: 0.4006 - acc: 0.8197 - val_loss: 0.7411 - val_acc: 0.7558
Epoch 58/100
100/100 [==============================] - 85s 849ms/step - loss: 0.4284 - acc: 0.7961 - val_loss: 0.4485 - val_acc: 0.7621
Epoch 59/100
100/100 [==============================] - 84s 835ms/step - loss: 0.4225 - acc: 0.8043 - val_loss: 0.4462 - val_acc: 0.7249
Epoch 60/100
100/100 [==============================] - 96s 963ms/step - loss: 0.4270 - acc: 0.7977 - val_loss: 0.3348 - val_acc: 0.7836
Epoch 61/100
100/100 [==============================] - 86s 856ms/step - loss: 0.4080 - acc: 0.8150 - val_loss: 0.5779 - val_acc: 0.7700
Epoch 62/100
100/100 [==============================] - 83s 830ms/step - loss: 0.4228 - acc: 0.8009 - val_loss: 0.2566 - val_acc: 0.7919
Epoch 63/100
100/100 [==============================] - 83s 831ms/step - loss: 0.4020 - acc: 0.8131 - val_loss: 0.5130 - val_acc: 0.7809
Epoch 64/100
100/100 [==============================] - 73s 729ms/step - loss: 0.4190 - acc: 0.8052 - val_loss: 0.2442 - val_acc: 0.7687
Epoch 65/100
100/100 [==============================] - 76s 759ms/step - loss: 0.4110 - acc: 0.8112 - val_loss: 0.4209 - val_acc: 0.7843
Epoch 66/100
100/100 [==============================] - 73s 728ms/step - loss: 0.3941 - acc: 0.8169 - val_loss: 0.4747 - val_acc: 0.7700
Epoch 67/100
100/100 [==============================] - 70s 697ms/step - loss: 0.4040 - acc: 0.8201 - val_loss: 0.3064 - val_acc: 0.8077
Epoch 68/100
100/100 [==============================] - 73s 727ms/step - loss: 0.3913 - acc: 0.8228 - val_loss: 0.4265 - val_acc: 0.7790
Epoch 69/100
100/100 [==============================] - 72s 719ms/step - loss: 0.3935 - acc: 0.8236 - val_loss: 0.2245 - val_acc: 0.8179
Epoch 70/100
100/100 [==============================] - 73s 728ms/step - loss: 0.4004 - acc: 0.8131 - val_loss: 0.4465 - val_acc: 0.7796
Epoch 71/100
100/100 [==============================] - 71s 712ms/step - loss: 0.3935 - acc: 0.8248 - val_loss: 0.5309 - val_acc: 0.7766
Epoch 72/100
100/100 [==============================] - 73s 733ms/step - loss: 0.3998 - acc: 0.8141 - val_loss: 0.4126 - val_acc: 0.8048
Epoch 73/100
100/100 [==============================] - 71s 711ms/step - loss: 0.3866 - acc: 0.8109 - val_loss: 0.3455 - val_acc: 0.7970
Epoch 74/100
100/100 [==============================] - 74s 742ms/step - loss: 0.3847 - acc: 0.8291 - val_loss: 0.4794 - val_acc: 0.7893
Epoch 75/100
100/100 [==============================] - 72s 717ms/step - loss: 0.3831 - acc: 0.8223 - val_loss: 0.3068 - val_acc: 0.8073
Epoch 76/100
100/100 [==============================] - 73s 732ms/step - loss: 0.3780 - acc: 0.8333 - val_loss: 0.3512 - val_acc: 0.7938
Epoch 77/100
100/100 [==============================] - 71s 711ms/step - loss: 0.3803 - acc: 0.8298 - val_loss: 0.4373 - val_acc: 0.8235
Epoch 78/100
100/100 [==============================] - 73s 733ms/step - loss: 0.3746 - acc: 0.8333 - val_loss: 0.3070 - val_acc: 0.8077
Epoch 79/100
100/100 [==============================] - 69s 694ms/step - loss: 0.3815 - acc: 0.8286 - val_loss: 0.5839 - val_acc: 0.7932
Epoch 80/100
100/100 [==============================] - 74s 735ms/step - loss: 0.3700 - acc: 0.8392 - val_loss: 0.4024 - val_acc: 0.8177
Epoch 81/100
100/100 [==============================] - 90s 903ms/step - loss: 0.3702 - acc: 0.8374 - val_loss: 0.3240 - val_acc: 0.8115
Epoch 82/100
100/100 [==============================] - 90s 898ms/step - loss: 0.3768 - acc: 0.8313 - val_loss: 0.6445 - val_acc: 0.8073
Epoch 83/100
100/100 [==============================] - 90s 898ms/step - loss: 0.3578 - acc: 0.8349 - val_loss: 0.2373 - val_acc: 0.7893
Epoch 84/100
100/100 [==============================] - 72s 717ms/step - loss: 0.3692 - acc: 0.8357 - val_loss: 0.3050 - val_acc: 0.8222
Epoch 85/100
100/100 [==============================] - 70s 703ms/step - loss: 0.3587 - acc: 0.8365 - val_loss: 0.2624 - val_acc: 0.8128
Epoch 86/100
100/100 [==============================] - 70s 702ms/step - loss: 0.3559 - acc: 0.8405 - val_loss: 0.3040 - val_acc: 0.7932
Epoch 87/100
100/100 [==============================] - 68s 683ms/step - loss: 0.3672 - acc: 0.8390 - val_loss: 0.6529 - val_acc: 0.8166
Epoch 88/100
100/100 [==============================] - 69s 685ms/step - loss: 0.3653 - acc: 0.8425 - val_loss: 0.6271 - val_acc: 0.7758
Epoch 89/100
100/100 [==============================] - 68s 684ms/step - loss: 0.3461 - acc: 0.8439 - val_loss: 0.3287 - val_acc: 0.8196
Epoch 90/100
100/100 [==============================] - 68s 680ms/step - loss: 0.3671 - acc: 0.8387 - val_loss: 0.6344 - val_acc: 0.8185
Epoch 91/100
100/100 [==============================] - 68s 685ms/step - loss: 0.3509 - acc: 0.8458 - val_loss: 0.5421 - val_acc: 0.7925
Epoch 92/100
100/100 [==============================] - 68s 683ms/step - loss: 0.3430 - acc: 0.8494 - val_loss: 0.3669 - val_acc: 0.8052
Epoch 93/100
100/100 [==============================] - 68s 684ms/step - loss: 0.3536 - acc: 0.8464 - val_loss: 0.4218 - val_acc: 0.8003
Epoch 94/100
100/100 [==============================] - 68s 683ms/step - loss: 0.3424 - acc: 0.8510 - val_loss: 0.3568 - val_acc: 0.8166
Epoch 95/100
100/100 [==============================] - 68s 681ms/step - loss: 0.3368 - acc: 0.8510 - val_loss: 0.2573 - val_acc: 0.8112
Epoch 96/100
100/100 [==============================] - 68s 684ms/step - loss: 0.3384 - acc: 0.8464 - val_loss: 0.1202 - val_acc: 0.8138
Epoch 97/100
100/100 [==============================] - 68s 680ms/step - loss: 0.3614 - acc: 0.8384 - val_loss: 0.2557 - val_acc: 0.8071
Epoch 98/100
100/100 [==============================] - 69s 689ms/step - loss: 0.3374 - acc: 0.8548 - val_loss: 0.5183 - val_acc: 0.7668
Epoch 99/100
100/100 [==============================] - 69s 691ms/step - loss: 0.3432 - acc: 0.8452 - val_loss: 0.6175 - val_acc: 0.8249
Epoch 100/100
100/100 [==============================] - 69s 685ms/step - loss: 0.3448 - acc: 0.8489 - val_loss: 0.3623 - val_acc: 0.7951
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">'model/cats_and_dogs_small_2.h5'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line">conv_base = VGG16(weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>))</span><br><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">base_dir = <span class="string">"data/cat_dog/cats_and_dogs_small"</span></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(directory, sample_count)</span>:</span></span><br><span class="line">    features = np.zeros(shape=(sample_count, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</span><br><span class="line">    labels = np.zeros(shape=(sample_count))</span><br><span class="line">    generator = datagen.flow_from_directory(</span><br><span class="line">        directory,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs_batch, labels_batch <span class="keyword">in</span> generator:</span><br><span class="line">        features_batch = conv_base.predict(inputs_batch)</span><br><span class="line">        features[i * batch_size : (i + <span class="number">1</span>) * batch_size] = features_batch</span><br><span class="line">        labels[i * batch_size : (i + <span class="number">1</span>) * batch_size] = labels_batch</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i * batch_size &gt;= sample_count:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = extract_features(train_dir, <span class="number">2000</span>)</span><br><span class="line">validation_features, validation_labels = extract_features(validation_dir, <span class="number">1000</span>)</span><br><span class="line">test_features, test_labels = extract_features(test_dir, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><p>目前，提取的特征形状为(samples, 4, 4, 512)。我们要将其输入到密集连接分类器中，<br>所以首先必须将其形状展平为(samples, 8192)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_features = np.reshape(train_features, (<span class="number">2000</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">512</span>))</span><br><span class="line">validation_features = np.reshape(validation_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">test_features = np.reshape(test_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers </span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>, input_dim=<span class="number">4</span>*<span class="number">4</span>*<span class="number">512</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">             loss = <span class="string">"binary_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"acc"</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(train_features, train_labels,</span><br><span class="line">                   epochs=<span class="number">30</span>, batch_size=<span class="number">20</span>,</span><br><span class="line">                   validation_data=(validation_features, validation_labels))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 2000 samples, validate on 1000 samples
Epoch 1/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.6142 - acc: 0.6485 - val_loss: 0.4569 - val_acc: 0.8210
Epoch 2/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.4278 - acc: 0.8115 - val_loss: 0.3716 - val_acc: 0.8650
Epoch 3/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.3626 - acc: 0.8430 - val_loss: 0.3342 - val_acc: 0.8720
Epoch 4/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.3246 - acc: 0.8605 - val_loss: 0.3049 - val_acc: 0.8850
Epoch 5/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2933 - acc: 0.8855 - val_loss: 0.2946 - val_acc: 0.8730
Epoch 6/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2679 - acc: 0.8950 - val_loss: 0.2773 - val_acc: 0.8950
Epoch 7/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2581 - acc: 0.8950 - val_loss: 0.2805 - val_acc: 0.8830
Epoch 8/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2352 - acc: 0.9075 - val_loss: 0.2612 - val_acc: 0.9010
Epoch 9/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2227 - acc: 0.9120 - val_loss: 0.2591 - val_acc: 0.8950
Epoch 10/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2081 - acc: 0.9235 - val_loss: 0.2521 - val_acc: 0.9010
Epoch 11/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2105 - acc: 0.9125 - val_loss: 0.2492 - val_acc: 0.8990
Epoch 12/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1917 - acc: 0.9270 - val_loss: 0.2482 - val_acc: 0.8980
Epoch 13/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1835 - acc: 0.9350 - val_loss: 0.2447 - val_acc: 0.8990
Epoch 14/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1756 - acc: 0.9345 - val_loss: 0.2462 - val_acc: 0.8980
Epoch 15/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1648 - acc: 0.9355 - val_loss: 0.2414 - val_acc: 0.9000
Epoch 16/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1600 - acc: 0.9435 - val_loss: 0.2449 - val_acc: 0.8980
Epoch 17/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1566 - acc: 0.9460 - val_loss: 0.2410 - val_acc: 0.9010
Epoch 18/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1430 - acc: 0.9510 - val_loss: 0.2404 - val_acc: 0.9020
Epoch 19/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1451 - acc: 0.9470 - val_loss: 0.2422 - val_acc: 0.9010
Epoch 20/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1368 - acc: 0.9565 - val_loss: 0.2387 - val_acc: 0.9010
Epoch 21/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1293 - acc: 0.9565 - val_loss: 0.2389 - val_acc: 0.9010
Epoch 22/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1194 - acc: 0.9610 - val_loss: 0.2390 - val_acc: 0.9010
Epoch 23/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1206 - acc: 0.9605 - val_loss: 0.2396 - val_acc: 0.8990
Epoch 24/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1176 - acc: 0.9530 - val_loss: 0.2442 - val_acc: 0.8990
Epoch 25/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1117 - acc: 0.9630 - val_loss: 0.2392 - val_acc: 0.8990
Epoch 26/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1072 - acc: 0.9650 - val_loss: 0.2480 - val_acc: 0.9040
Epoch 27/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1028 - acc: 0.9685 - val_loss: 0.2466 - val_acc: 0.9030
Epoch 28/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0949 - acc: 0.9745 - val_loss: 0.2453 - val_acc: 0.8980
Epoch 29/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0946 - acc: 0.9715 - val_loss: 0.2618 - val_acc: 0.8960
Epoch 30/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0891 - acc: 0.9740 - val_loss: 0.2428 - val_acc: 0.9000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_24_0.png" alt="png"></p>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_24_1.png" alt="png"></p>
<h1 id="可视化中间激活"><a href="#可视化中间激活" class="headerlink" title="可视化中间激活"></a>可视化中间激活</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">model = load_model(<span class="string">'model/cats_and_dogs_small_2.h5'</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>接下来，我们需要一张输入图像，即一张猫的图像，它不属于网络的训练图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">'data/cat_dog/train/cat.1700.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型的输入数据都用这种方法预处理</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">img_tensor = image.img_to_array(img)</span><br><span class="line">img_tensor = np.expand_dims(img_tensor, axis=<span class="number">0</span>)</span><br><span class="line">img_tensor /= <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">print(img_tensor.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 150, 150, 3)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(img_tensor[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_29_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">layer_outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">8</span>]]</span><br><span class="line">activation_model = models.Model(inputs=model.input, outputs=layer_outputs)</span><br><span class="line">activations = activation_model.predict(img_tensor)</span><br><span class="line"></span><br><span class="line">first_layer_activation = activations[<span class="number">0</span>]</span><br><span class="line">print(first_layer_activation.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 148, 148, 32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.matshow(first_layer_activation[<span class="number">0</span>, :, :, <span class="number">4</span>], cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x18690050be0&gt;
</code></pre><p><img src="/Pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/output_31_1.png" alt="png"></p>
<p>当然，还有更多的可视化方法，但不是我们关注的主线，这里就不一一阐述了。</p>
<blockquote>
<p>注：以上内容均直接搬运自个人的jupyter notebook，参考书籍为《Python深度学习》，注释未到位之处请多谅解，有需要对哪些地方补充注释的可以在下方留言</p>
</blockquote>
]]></content>
      <categories>
        <category>Keras</category>
      </categories>
  </entry>
  <entry>
    <title>Keras深度学习（一）神经网络入门</title>
    <url>/posts/990a7167.html</url>
    <content><![CDATA[<h1 id="神经网络剖析"><a href="#神经网络剖析" class="headerlink" title="神经网络剖析"></a>神经网络剖析</h1><p>训练神经网络主要分为以下四个方面：</p>
<ol>
<li>层：多个层组合成网络（或模型）</li>
<li>输入数据和相应的目标</li>
<li>损失函数：即用于学习的返回信号</li>
<li>优化器：决定学习进程如何进行</li>
</ol>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>定义模型的两种不同方法：</p>
<ol>
<li>使用Sequential类（仅用于层的线性堆叠）</li>
<li>函数式API（可以构建任何形式的架构）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br></pre></td></tr></table></figure>
<p><strong>Sequential类定义的模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
</code></pre><p><strong>函数式API定义的模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_tensor = layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">x = layers.Dense(<span class="number">32</span>, activation=<span class="string">"relu"</span>)(input_tensor)</span><br><span class="line">output_tensor = layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line">model = models.Model(inputs=input_tensor, outputs=output_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="损失函数与优化器"><a href="#损失函数与优化器" class="headerlink" title="损失函数与优化器"></a>损失函数与优化器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.01</span>), loss=<span class="string">"mse"</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>最后，学习过程就是通过fit() 方法将输入数据的Numpy 数组（和对应的目标数据）传入模型，这一做法与Scikit-Learn 及其他机器学习库类似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(input_tensor, target_tensor, batch_size=<span class="number">128</span>, epoches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h1 id="电影评论分类：二分类问题"><a href="#电影评论分类：二分类问题" class="headerlink" title="电影评论分类：二分类问题"></a>电影评论分类：二分类问题</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\datasets\imdb.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\datasets\imdb.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(train_data[<span class="number">0</span>])</span><br><span class="line">print(train_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
1
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">reverse_word_index = dict([(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_review = <span class="string">" "</span>.join([reverse_word_index.get(i<span class="number">-3</span>, <span class="string">"?"</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">import</span> keras.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">import</span> keras.layers <span class="keyword">as</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">              loss=<span class="string">"binary_crossentropy"</span>,</span><br><span class="line">              metrics=[<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 15000 samples, validate on 10000 samples
Epoch 1/20
15000/15000 [==============================] - 5s 304us/step - loss: 0.5254 - accuracy: 0.7822 - val_loss: 0.3898 - val_accuracy: 0.8633
Epoch 2/20
15000/15000 [==============================] - 2s 110us/step - loss: 0.3074 - accuracy: 0.9003 - val_loss: 0.3077 - val_accuracy: 0.8848
Epoch 3/20
15000/15000 [==============================] - 2s 102us/step - loss: 0.2278 - accuracy: 0.9264 - val_loss: 0.2798 - val_accuracy: 0.8914
Epoch 4/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.1805 - accuracy: 0.9415 - val_loss: 0.2928 - val_accuracy: 0.8823
Epoch 5/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.1486 - accuracy: 0.9529 - val_loss: 0.2839 - val_accuracy: 0.8875
Epoch 6/20
15000/15000 [==============================] - 2s 101us/step - loss: 0.1194 - accuracy: 0.9642 - val_loss: 0.2957 - val_accuracy: 0.8872
Epoch 7/20
15000/15000 [==============================] - 1s 99us/step - loss: 0.1030 - accuracy: 0.9678 - val_loss: 0.3137 - val_accuracy: 0.8836
Epoch 8/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0846 - accuracy: 0.9773 - val_loss: 0.3224 - val_accuracy: 0.8827
Epoch 9/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0735 - accuracy: 0.9803 - val_loss: 0.3628 - val_accuracy: 0.8725
Epoch 10/20
15000/15000 [==============================] - 1s 99us/step - loss: 0.0582 - accuracy: 0.9861 - val_loss: 0.3789 - val_accuracy: 0.8783
Epoch 11/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0500 - accuracy: 0.9883 - val_loss: 0.3892 - val_accuracy: 0.8786
Epoch 12/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0397 - accuracy: 0.9917 - val_loss: 0.4213 - val_accuracy: 0.8718
Epoch 13/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0338 - accuracy: 0.9931 - val_loss: 0.4453 - val_accuracy: 0.8744
Epoch 14/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0273 - accuracy: 0.9948 - val_loss: 0.4754 - val_accuracy: 0.8745
Epoch 15/20
15000/15000 [==============================] - 2s 101us/step - loss: 0.0224 - accuracy: 0.9961 - val_loss: 0.5247 - val_accuracy: 0.8717
Epoch 16/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0155 - accuracy: 0.9981 - val_loss: 0.5341 - val_accuracy: 0.8711
Epoch 17/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0139 - accuracy: 0.9983 - val_loss: 0.5686 - val_accuracy: 0.8695
Epoch 18/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.6013 - val_accuracy: 0.8672
Epoch 19/20
15000/15000 [==============================] - 2s 100us/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.6324 - val_accuracy: 0.8652
Epoch 20/20
15000/15000 [==============================] - 1s 98us/step - loss: 0.0079 - accuracy: 0.9989 - val_loss: 0.6615 - val_accuracy: 0.8668
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_dict = history.history</span><br><span class="line">history_dict.keys()</span><br></pre></td></tr></table></figure>
<pre><code>dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">"loss"</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">"val_loss"</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">"bo"</span>, label=<span class="string">"Training loss"</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">"b"</span>, label=<span class="string">"Validation loss"</span>)</span><br><span class="line">plt.title(<span class="string">"Training and validation loss"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/posts/keras/output_24_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.clf()</span><br><span class="line">acc = history_dict[<span class="string">'accuracy'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_accuracy'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/keras/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">             loss=<span class="string">"binary_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"acc"</span>])</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/4
25000/25000 [==============================] - 2s 65us/step - loss: 0.4611 - acc: 0.8215
Epoch 2/4
25000/25000 [==============================] - 2s 62us/step - loss: 0.2626 - acc: 0.9069
Epoch 3/4
25000/25000 [==============================] - 2s 65us/step - loss: 0.2007 - acc: 0.9264
Epoch 4/4
25000/25000 [==============================] - 2s 67us/step - loss: 0.1659 - acc: 0.9414
25000/25000 [==============================] - 15s 586us/step
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>
<pre><code>[0.2943145182275772, 0.8847600221633911]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.20976487],
       [0.999882  ],
       [0.89416105],
       ...,
       [0.12636474],
       [0.05909678],
       [0.44751233]], dtype=float32)
</code></pre><h1 id="新闻分类：多分类问题"><a href="#新闻分类：多分类问题" class="headerlink" title="新闻分类：多分类问题"></a>新闻分类：多分类问题</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
C:\Users\16402\Anaconda3\envs\python36\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(len(train_data))</span><br><span class="line">print(len(train_labels))</span><br></pre></td></tr></table></figure>
<pre><code>8982
8982
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict([(value, key) <span class="keyword">for</span> (key,value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_newwire = <span class="string">" "</span>.join([reverse_word_index.get(i<span class="number">-3</span>, <span class="string">"?"</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br><span class="line">decoded_newwire</span><br></pre></td></tr></table></figure>
<pre><code>&#39;? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(labels, dimension=<span class="number">46</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(labels), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        results[i, label] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_one_hot(train_labels)</span><br><span class="line">one_hot_test_labels = to_one_hot(test_labels)</span><br></pre></td></tr></table></figure>
<p>注意，Keras 内置方法可以实现One-hot操作:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">import</span> keras.layers <span class="keyword">as</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">"softmax"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">             loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                   partial_y_train,</span><br><span class="line">                   epochs=<span class="number">20</span>,</span><br><span class="line">                   batch_size=<span class="number">512</span>,</span><br><span class="line">                   validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 2s 272us/step - loss: 1.8693 - accuracy: 0.5712 - val_loss: 1.2307 - val_accuracy: 0.7020
Epoch 2/20
7982/7982 [==============================] - 2s 258us/step - loss: 0.8578 - accuracy: 0.8029 - val_loss: 0.9550 - val_accuracy: 0.7770
Epoch 3/20
7982/7982 [==============================] - 2s 257us/step - loss: 0.4959 - accuracy: 0.8861 - val_loss: 0.8886 - val_accuracy: 0.8010
Epoch 4/20
7982/7982 [==============================] - 2s 269us/step - loss: 0.3254 - accuracy: 0.9247 - val_loss: 0.8109 - val_accuracy: 0.8330
Epoch 5/20
7982/7982 [==============================] - 2s 266us/step - loss: 0.2040 - accuracy: 0.9491 - val_loss: 0.8889 - val_accuracy: 0.8050
Epoch 6/20
7982/7982 [==============================] - 2s 257us/step - loss: 0.1730 - accuracy: 0.9520 - val_loss: 0.8479 - val_accuracy: 0.8200
Epoch 7/20
7982/7982 [==============================] - 2s 247us/step - loss: 0.1677 - accuracy: 0.9520 - val_loss: 0.9323 - val_accuracy: 0.8040
Epoch 8/20
7982/7982 [==============================] - 2s 245us/step - loss: 0.1376 - accuracy: 0.9546 - val_loss: 0.8941 - val_accuracy: 0.8190
Epoch 9/20
7982/7982 [==============================] - 2s 256us/step - loss: 0.1273 - accuracy: 0.9564 - val_loss: 0.9124 - val_accuracy: 0.8190
Epoch 10/20
7982/7982 [==============================] - 2s 252us/step - loss: 0.1188 - accuracy: 0.9559 - val_loss: 1.0038 - val_accuracy: 0.7970
Epoch 11/20
7982/7982 [==============================] - 2s 264us/step - loss: 0.1116 - accuracy: 0.9555 - val_loss: 0.9859 - val_accuracy: 0.8120
Epoch 12/20
7982/7982 [==============================] - 2s 256us/step - loss: 0.1014 - accuracy: 0.9578 - val_loss: 1.1145 - val_accuracy: 0.7910
Epoch 13/20
7982/7982 [==============================] - 2s 251us/step - loss: 0.1018 - accuracy: 0.9554 - val_loss: 1.0680 - val_accuracy: 0.8020
Epoch 14/20
7982/7982 [==============================] - 2s 258us/step - loss: 0.0950 - accuracy: 0.9568 - val_loss: 1.1566 - val_accuracy: 0.8000
Epoch 15/20
7982/7982 [==============================] - 2s 284us/step - loss: 0.0906 - accuracy: 0.9578 - val_loss: 1.1965 - val_accuracy: 0.8040
Epoch 16/20
7982/7982 [==============================] - 2s 289us/step - loss: 0.0843 - accuracy: 0.9562 - val_loss: 1.1994 - val_accuracy: 0.8020
Epoch 17/20
7982/7982 [==============================] - 2s 263us/step - loss: 0.0834 - accuracy: 0.9575 - val_loss: 1.2736 - val_accuracy: 0.8060
Epoch 18/20
7982/7982 [==============================] - 2s 249us/step - loss: 0.0786 - accuracy: 0.9559 - val_loss: 1.3329 - val_accuracy: 0.8050
Epoch 19/20
7982/7982 [==============================] - 2s 255us/step - loss: 0.0771 - accuracy: 0.9592 - val_loss: 1.7559 - val_accuracy: 0.7720
Epoch 20/20
7982/7982 [==============================] - 2s 253us/step - loss: 0.0768 - accuracy: 0.9569 - val_loss: 1.4354 - val_accuracy: 0.8040
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss = history_dict[<span class="string">"loss"</span>]</span><br><span class="line">val_loss = history_dict[<span class="string">"val_loss"</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/keras/output_42_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc = history.history[<span class="string">'accuracy'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_accuracy'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/keras/output_43_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据在第五轮左右开始过拟合</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">"softmax"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">             loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">model.fit(x_train,</span><br><span class="line">         one_hot_train_labels,</span><br><span class="line">         epochs=<span class="number">5</span>,</span><br><span class="line">         batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
8982/8982 [==============================] - 1s 84us/step - loss: 2.4658 - accuracy: 0.5529
Epoch 2/5
8982/8982 [==============================] - 1s 77us/step - loss: 1.3174 - accuracy: 0.7286
Epoch 3/5
8982/8982 [==============================] - 1s 79us/step - loss: 0.9746 - accuracy: 0.7960
Epoch 4/5
8982/8982 [==============================] - 1s 77us/step - loss: 0.7625 - accuracy: 0.8398
Epoch 5/5
8982/8982 [==============================] - 1s 77us/step - loss: 0.6018 - accuracy: 0.8720
2246/2246 [==============================] - 0s 118us/step
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>
<pre><code>[0.9859163759015865, 0.7822796106338501]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 完全随机的分类精度</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">test_labels_copy = copy.copy(test_labels)</span><br><span class="line">np.random.shuffle(test_labels_copy)</span><br><span class="line">hits_array = np.array(test_labels) == np.array(test_labels_copy)</span><br><span class="line">print(float(np.sum(hits_array))/len(test_labels))</span><br></pre></td></tr></table></figure>
<pre><code>0.1861086375779163
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>(46,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>0.9999999
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.argmax(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>3
</code></pre><h2 id="处理标签的另一种方法"><a href="#处理标签的另一种方法" class="headerlink" title="处理标签的另一种方法"></a>处理标签的另一种方法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train = np.array(train_labels)</span><br><span class="line">y_test = np.array(test_labels)</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">"softmax"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>,</span><br><span class="line">             loss=<span class="string">"sparse_categorical_crossentropy"</span>,</span><br><span class="line">             metrics=[<span class="string">"acc"</span>])</span><br><span class="line">model.fit(x_train,</span><br><span class="line">         y_train,</span><br><span class="line">         epochs=<span class="number">5</span>,</span><br><span class="line">         batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, y_test)</span><br><span class="line">print(results)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
8982/8982 [==============================] - 1s 85us/step - loss: 2.6402 - acc: 0.5324
Epoch 2/5
8982/8982 [==============================] - 1s 80us/step - loss: 1.3306 - acc: 0.7237
Epoch 3/5
8982/8982 [==============================] - 1s 75us/step - loss: 0.9743 - acc: 0.7951
Epoch 4/5
8982/8982 [==============================] - 1s 76us/step - loss: 0.7556 - acc: 0.8447
Epoch 5/5
8982/8982 [==============================] - 1s 76us/step - loss: 0.5990 - acc: 0.8785
2246/2246 [==============================] - 0s 104us/step
[0.9927649695429543, 0.7724844217300415]
</code></pre><h1 id="预测房价：回归问题"><a href="#预测房价：回归问题" class="headerlink" title="预测房价：回归问题"></a>预测房价：回归问题</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(train_data.shape)</span><br><span class="line">print(test_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(404, 13)
(102, 13)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_targets[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4])
</code></pre><p>将取值范围差异很大的数据输入到神经网络中，这是有问题的。网络可能会自动适应这种<br>取值范围不同的数据，但学习肯定变得更加困难。对于这种数据，普遍采用的最佳实践是对每<br>个特征做标准化，即对于输入数据的每个特征（输入数据矩阵中的列），减去特征平均值，再除<br>以标准差，这样得到的特征平均值为0，标准差为1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure>
<p>注意，用于测试数据标准化的均值和标准差都是在训练数据上计算得到的。在工作流程中，<br>你不能使用在测试数据上计算得到的任何结果，即使是像数据标准化这么简单的事情也不行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>, input_shape=(train_data.shape[<span class="number">1</span>],)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">"rmsprop"</span>, loss=<span class="string">"mse"</span>, metrics=[<span class="string">"mae"</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="K折验证"><a href="#K折验证" class="headerlink" title="K折验证"></a>K折验证</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">all_scores = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">"Processing fold #"</span>, i)</span><br><span class="line">    val_data = train_data[i*num_val_samples: (i+<span class="number">1</span>)*num_val_samples]</span><br><span class="line">    val_targets = train_targets[i*num_val_samples:(i+<span class="number">1</span>)*num_val_samples]</span><br><span class="line">    partial_train_data = np.concatenate([train_data[:i*num_val_samples], train_data[(i+<span class="number">1</span>)*num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate([train_targets[:i*num_val_samples], train_targets[(i+<span class="number">1</span>)*num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line">    model = build_model()</span><br><span class="line">    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mae)</span><br><span class="line">print(all_scores)</span><br><span class="line">print(np.mean(all_scores))</span><br></pre></td></tr></table></figure>
<pre><code>Processing fold # 0
Processing fold # 1
Processing fold # 2
Processing fold # 3
[2.0739996433258057, 2.4122469425201416, 2.454885959625244, 2.5617122650146484]
2.37571120262146
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'processing fold #'</span>, i)</span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:i * num_val_samples],</span><br><span class="line">         train_data[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">         axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:i * num_val_samples],</span><br><span class="line">         train_targets[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    model = build_model()</span><br><span class="line">    history = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">    validation_data=(val_data, val_targets),</span><br><span class="line">    epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    mae_history = history.history[<span class="string">'mae'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br><span class="line"></span><br><span class="line">average_mae_history = [np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> all_mae_histories]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)]</span><br></pre></td></tr></table></figure>
<pre><code>processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(average_mae_history) + <span class="number">1</span>), average_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/output_64_0.png" alt="png"></p>
<p>因为纵轴的范围较大，且数据方差相对较大，所以难以看清这张图的规律。我们来重新绘制一张图。</p>
<ul>
<li>删除前 10 个数据点，因为它们的取值范围与曲线上的其他点不同。</li>
<li>将每个数据点替换为前面数据点的指数移动平均值，以得到光滑的曲线。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line">smooth_mae_history = smooth_curve(average_mae_history[<span class="number">10</span>:])</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(smooth_mae_history) + <span class="number">1</span>), smooth_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/output_66_0.png" alt="png"></p>
<blockquote>
<p>注：以上内容均直接搬运自个人的jupyter notebook，参考书籍为《Python深度学习》，注释未到位之处请多谅解，有需要对哪些地方补充注释的可以在下方留言</p>
</blockquote>
]]></content>
      <categories>
        <category>Keras</category>
      </categories>
  </entry>
  <entry>
    <title>Tensorflow 神经网络搭建（一）—— 组件构建与基础使用</title>
    <url>/posts/acf1f2c0.html</url>
    <content><![CDATA[<p>这一节我们对tensorflow的一些最基本的函数进行简单调用举例，当然调用函数前大家有必要自己了解神经网络的一些基本常识，这里给大家推荐邱锡鹏老师的一本书《神经网络与深度学习》，以及Raul Rojas的《Neural Networks》，这两本书足够对神经网络与深度学习的基础知识有一个完整清晰的认识，下面直接进入tensorflow的基础语法尝试。</p>
<h1 id="第一个例子"><a href="#第一个例子" class="headerlink" title="第一个例子"></a>第一个例子</h1><p>下面看第一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line">y_data = x_data*<span class="number">0.1</span> + <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">Weights = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">y = Weights*x_data + biases</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(Weights), sess.run(biases))</span><br></pre></td></tr></table></figure>
<ul>
<li>代码中第1行导入tensorflow包</li>
<li>第3行和第4行定义x和y变量，其中x变量是根据标准正态分布随机生成的100个数</li>
<li>第7、8行分别定义变量Weights和biases，分别声明为tf.Variable()</li>
<li>第9行根据这两个变量生成一个预测的y值</li>
<li>第11行至第13行分别定义损失函数形式，优化器和训练器</li>
<li>第15行的initialize_all_variables()函数十分重要，因为前面定义的所有变量都只是定义，而没有初始化（C++中的实例化）</li>
<li>第17-18行分别定义了Session()并执行初始化</li>
<li>最后四行则是打印出运行结果，每隔20轮打印出权重参数和偏置参数</li>
</ul>
<p>至此结束，我们看看打印结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 [0.52235436] [0.0773498]</span><br><span class="line">20 [0.2182928] [0.23877653]</span><br><span class="line">40 [0.133002] [0.28291953]</span><br><span class="line">60 [0.1092071] [0.2952348]</span><br><span class="line">80 [0.10256864] [0.2986706]</span><br><span class="line">100 [0.10071664] [0.29962912]</span><br><span class="line">120 [0.10019994] [0.29989654]</span><br><span class="line">140 [0.1000558] [0.29997113]</span><br><span class="line">160 [0.10001557] [0.29999197]</span><br><span class="line">180 [0.10000437] [0.29999775]</span><br></pre></td></tr></table></figure>
<h1 id="Session的打开方式"><a href="#Session的打开方式" class="headerlink" title="Session的打开方式"></a>Session的打开方式</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">matrix1 = tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],[<span class="number">2</span>]])</span><br><span class="line">product = tf.multiply(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式一</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[6 6]</span></span><br><span class="line"><span class="comment"># [6 6]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式二</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result2)</span><br></pre></td></tr></table></figure>
<p>由于Session()打开之后就一定要关闭，和平时操作文件时使用到的with open语句类似，方式二的语句可以避免close()操作，使代码更美观。</p>
<h1 id="Variable变量"><a href="#Variable变量" class="headerlink" title="Variable变量"></a>Variable变量</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line">print(state.name)</span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>变量一定要initialize！</p>
<h1 id="placeholder"><a href="#placeholder" class="headerlink" title="placeholder"></a>placeholder</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">output = tf.multiply(input1,input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(output, feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.4</span>]&#125;))</span><br><span class="line"><span class="comment"># [16.800001]</span></span><br></pre></td></tr></table></figure>
<p>placeholder一共有三个参数，含义分别是：</p>
<ul>
<li>dtype：数据类型。常用的是tf.float32,tf.float64等数值类型</li>
<li>shape：数据形状。默认是None，就是一维值，也可以是多维，比如[2,3], [None, 3]表示列是3，行不定</li>
<li>name：名称。</li>
</ul>
<p>在训练神经网络时需要每次提供一个批量的训练样本，如果每次迭代选取的数据要通过常量表示，那么TensorFlow 的计算图会非常大。因为每增加一个常量，TensorFlow 都会在计算图中增加一个结点，所以说拥有几百万次迭代的神经网络会拥有极其庞大的计算图，而占位符却可以解决这一点，它只会拥有占位符这一个结点，Placeholder机制的出现就是为了解决这个问题，我们在编程的时候只需要把数据通过placeholder传入tensorflow计算图即可。另外，若变量维度不确定，可以填入None。</p>
<h1 id="Activation实例"><a href="#Activation实例" class="headerlink" title="Activation实例"></a>Activation实例</h1><p>下面介绍Activation函数的使用：</p>
<h2 id="定义添加层函数"><a href="#定义添加层函数" class="headerlink" title="定义添加层函数"></a>定义添加层函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络结构"><a href="#建立神经网络结构" class="headerlink" title="建立神经网络结构"></a>建立神经网络结构</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:x_data, ys:y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span>:</span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs:x_data, ys:y_data&#125;))</span><br></pre></td></tr></table></figure>
<p>上面这些代码代价应能看懂，与第一个例子类似，只是在这里我们定义了一个新的Activation Layer，读者自行分析总结即可。</p>
<h1 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()   <span class="comment"># (*)</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)   <span class="comment"># (*)</span></span><br><span class="line">ax.scatter(x_data, y_data)   <span class="comment"># (*)</span></span><br><span class="line">plt.ion()   <span class="comment"># (*)</span></span><br><span class="line">plt.show()   <span class="comment"># (*)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:x_data, ys:y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            ax.lines.remove(lines[<span class="number">0</span>])   <span class="comment"># (*)</span></span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs:x_data&#125;)   <span class="comment"># (*)</span></span><br><span class="line">        lines = ax.plot(x_data, prediction_value, <span class="string">"r-"</span>, lw=<span class="number">5</span>)   <span class="comment"># (*)</span></span><br><span class="line">        plt.pause(<span class="number">0.1</span>)   <span class="comment"># (*)</span></span><br></pre></td></tr></table></figure>
<p>重点关注有(*)标记的代码，这些是新增的与画图相关的代码。这些代码能够显示神经网络一步步接近真实值的过程，代码具体含义大家应当都能看懂，其中plt.ion()表示拟合曲线可以自动变化，plt.pause()是控制拟合曲线变化时间间隔的函数，ax.lines.remove是为了使图形界面不要太杂乱，抹掉上一次的拟合曲线。</p>
<h1 id="Tensorboard可视化"><a href="#Tensorboard可视化" class="headerlink" title="Tensorboard可视化"></a>Tensorboard可视化</h1><p>我们基于前面的add_layer函数及神经网络的实现代码进行部分修改，定义一些名称与包含关系，并在终端执行 tensorboard —logdir=’logs/‘ 命令，在浏览器中输入获得的对应网址，即而已获得我们需要的Tensorboard，下面是修改后的代码，注意with tf.name_scope()的用法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"inputs"</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"weights"</span>):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name=<span class="string">"W"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"biases"</span>):</span><br><span class="line">            biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"Wx_plus_b"</span>):</span><br><span class="line">            Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">        <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            outputs = Wx_plus_b</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = activation_function(Wx_plus_b)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"inputs"</span>):</span><br><span class="line">    xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">"x_input"</span>)</span><br><span class="line">    ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name = <span class="string">"y_input"</span>)</span><br><span class="line"></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p>下面是可视化结果（输入网址后），大家如果想看更清晰的图片可以滑动鼠标滚轮查看。</p>
<p><img src="/posts/Tensorflow/tb1.png" alt></p>
<p>事实上，我们还可以定义直方图，误差折线图等有趣的Tensorboard可视化，运行以下代码，然后试试删除第一行，多运行几次，看看会有什么问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, n_layer, activation_function=None)</span>:</span></span><br><span class="line">    layer_name = <span class="string">'layer%s'</span> % n_layer</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"weights"</span>):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name=<span class="string">"W"</span>)</span><br><span class="line">            tf.summary.histogram(layer_name+<span class="string">'/Weights'</span>, Weights)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"biases"</span>):</span><br><span class="line">            biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, name=<span class="string">"b"</span>)</span><br><span class="line">            tf.summary.histogram(layer_name+<span class="string">'/biases'</span>, Weights)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"Wx_plus_b"</span>):</span><br><span class="line">            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)</span><br><span class="line">        <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            outputs = Wx_plus_b</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = activation_function(Wx_plus_b)</span><br><span class="line">            tf.summary.histogram(layer_name+<span class="string">'/outputs'</span>, outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"inputs"</span>):</span><br><span class="line">    xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">"x_input"</span>)</span><br><span class="line">    ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name = <span class="string">"y_input"</span>)</span><br><span class="line"></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, n_layer=<span class="number">1</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, n_layer=<span class="number">2</span>, activation_function=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 会在Events显示</span></span><br><span class="line">    tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step,feed_dict=&#123;xs:x_data, ys:y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        result = sess.run(merged, feed_dict=&#123;xs:x_data, ys:y_data&#125;)</span><br><span class="line">        writer.add_summary(result, i)</span><br></pre></td></tr></table></figure>
<p>同样的，我们运行tensorboard —logdir=’logs/‘，进入给出的网址，我们可以在scalars栏看到下图：</p>
<p><img src="/Pic/Tensorflow/tb2.png" alt></p>
<p>这正是我们定义的tf.summary.scalar(“loss”, loss)，找到上面源代码看看是如何使用的。</p>
<p>然后我们看到Histograms栏，我们会发现前面使用tf.summary.histogram()定义的一些图都在这里体现出来了：</p>
<p><img src="/Pic/Tensorflow/tb3.png" alt></p>
<h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h1><p>我们看一看MNIST手写体分类的数据集实验，大家如果接触过sklearn就会有一些这个数据集的印象，大概就是从手写体的1到9中训练出能够分辨出这些手写数字的实际数字的算法，下面我们直接用tensorflow实现之：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs, v_ys)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> predictions</span><br><span class="line">    y__pre = sess.run(prediction, feed_dict=&#123;xs:v_xs&#125;)</span><br><span class="line">    <span class="comment"># 对比预测和实际是否相等</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y__pre, <span class="number">1</span>), tf.argmax(v_ys, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    <span class="comment"># result是百分比,越高越准确</span></span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs:v_xs, ys:v_ys&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) <span class="comment"># 28*28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">prediction = add_layer(xs, <span class="number">784</span>, <span class="number">10</span>, activation_function=tf.nn.softmax)</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:batch_xs, ys:batch_ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(compute_accuracy(mnist.test.images, mnist.test.labels))</span><br></pre></td></tr></table></figure>
<p>运行这段程序时会下载MNIST数据集，并与前面类似定义输入、训练集测试集、损失函数等等，这里使用的loss function：cross_entropy是适用于分类算法中的常用损失函数，感兴趣的话大家可以另行查找相关资料学习。</p>
<h1 id="避免overfitting"><a href="#避免overfitting" class="headerlink" title="避免overfitting"></a>避免overfitting</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">y = LabelBinarizer().fit_transform(y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, layer_name, activation_function=None, )</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, )</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="comment"># here to dropout</span></span><br><span class="line">    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b, )</span><br><span class="line">    tf.summary.histogram(layer_name + <span class="string">'/outputs'</span>, outputs)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">64</span>])  <span class="comment"># 8x8</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">64</span>, <span class="number">50</span>, <span class="string">'l1'</span>, activation_function=tf.nn.tanh)</span><br><span class="line">prediction = add_layer(l1, <span class="number">50</span>, <span class="number">10</span>, <span class="string">'l2'</span>, activation_function=tf.nn.softmax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the loss between prediction and real data</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),</span><br><span class="line">                                              reduction_indices=[<span class="number">1</span>]))  <span class="comment"># loss</span></span><br><span class="line">tf.summary.scalar(<span class="string">'loss'</span>, cross_entropy)</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"><span class="comment"># summary writer goes in here</span></span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="string">"logs/train"</span>, sess.graph)</span><br><span class="line">test_writer = tf.summary.FileWriter(<span class="string">"logs/test"</span>, sess.graph)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.initialize_all_variables() no long valid from</span></span><br><span class="line"><span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># here to determine the keeping probability</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># record loss</span></span><br><span class="line">        train_result = sess.run(merged, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">        test_result = sess.run(merged, feed_dict=&#123;xs: X_test, ys: y_test, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">        train_writer.add_summary(train_result, i)</span><br><span class="line">        test_writer.add_summary(test_result, i)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上部分代码参考于MovanZhou的github：<a href="https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT" target="_blank" rel="noopener">https://github.com/MorvanZhou/tutorials/tree/master/tensorflowTUT</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>CS224n 13_Contextual Representations</title>
    <url>/posts/f2fb8471.html</url>
    <content><![CDATA[<p>前面介绍了glove,word2vec,fasttext，这几个模型解决了词的上下文信息，但是不能解决多义词问题，基于语义环境的词嵌入模型由此被提出解决这一问题。语义环境词嵌入模型的核心在于通过具体的上下文语义环境确定词向量，“一句一词一向量”，其比较典型的代表有ELMo, GPT, Bert等，以下将分别进行介绍：</p>
<h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><p>ELMo(Embeddings from Language Models)利用语言模型通过上下文来确定词向量。在ELMo被提出之前，TagLM被提出了进行语音词嵌入，模型比ELMo简单。其先使用无监督数据预训练词向量和语言模型，接着将词向量和由语言模型得到的单词编码组合作为有标记的序列模型的输入，然后训练有标记的序列模型。其示意图如下：</p>
<p><img src="/posts/NLP/24.jpg" alt></p>
<p>在ELMo中，他们使用的是一个双向的LSTM语言模型，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然：</p>
<p><img src="/posts/NLP/23.jpg" alt></p>
<ul>
<li><strong>A Forward LM</strong></li>
</ul>
<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod_{k=1}^N p(t_k|t_1,t_2,...,t_{k-1})</script><ul>
<li><strong>A Backward LM</strong></li>
</ul>
<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod_{k=1}^N p(t_k|t_{k+1},t_{k+2},...,t_{N})</script><ul>
<li><strong>Jointly maximize the log likelihood of the forward and backward directions</strong></li>
</ul>
<script type="math/tex; mode=display">\sum_{k=1}^N (log p(t_k|t_1,t_2,...,t_{k-1};\theta_x,\theta_{LSTM}^{forward},\theta_s) + log p(t_k|t_{k+1},t_{k+2},...,t_{N};\theta_x,\theta_{LSTM}^{backward},\theta_s))</script><p>总结一下，不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），再当成特征加入到具体的NLP有监督模型里。</p>
<h1 id="Open-AI-GPT"><a href="#Open-AI-GPT" class="headerlink" title="Open AI GPT"></a>Open AI GPT</h1><p>他们利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。</p>
<p>首先我们来看一下他们无监督预训练时的语言模型。他们仍然使用的是标准的语言模型目标函数，即通过前k个词预测当前词，但是在语言模型网络上他们使用了google团队在《Attention is all your need》论文中提出的Transformer解码器作为语言模型。Transformer模型主要是利用自注意力（self-attention）机制的模型（参看我另一篇Bert介绍的博客）。</p>
<p>然后再具体NLP任务有监督微调时，与ELMo当成特征的做法不同，OpenAI GPT不需要再重新对任务构建新的模型结构，而是直接在transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调。他们额外发现，如果使用语言模型作为辅助任务，能够提升有监督模型的泛化能力，并且能够加速收敛。</p>
<p>由于不同NLP任务的输入有所不同，在transformer模型的输入上针对不同NLP任务也有所不同。具体如下图，对于分类任务直接讲文本输入即可；对于文本蕴涵任务，需要将前提和假设用一个Delim分割向量拼接后进行输入；对于文本相似度任务，在两个方向上都使用Delim拼接后，进行输入；对于像问答多选择的任务，就是将每个答案和上下文进行拼接进行输入。</p>
<p><img src="/posts/NLP/25.jpg" alt></p>
<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><p>这篇论文中，作者们证明了使用双向的预训练效果更好。其实这篇论文方法的整体框架和GPT类似，是进一步的发展。具体的，他们BERT是使用Transformer的编码器来作为语言模型，在语言模型预训练的时候，提出了两个新的目标任务（即遮挡语言模型MLM和预测下一个句子的任务），最后在11个NLP任务上取得了SOTA。</p>
<p>在语言模型上，BERT使用的是Transformer编码器，并且设计了一个小一点Base结构和一个更大的Large网络结构。步骤大体如下：</p>
<ul>
<li><p>Pre-training Task 1#: Masked LM</p>
<ul>
<li>第一步预训练的目标就是做语言模型，即bidirectional。</li>
<li>意思就是如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，而是左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲人们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”。</li>
<li>在训练过程中随机mask 15%的token，而不是把像CBOW一样把每个词都预测一遍。从结构上看输入输出是长度一样的sequence，这样模型实际上在做sequence-level的LM。</li>
<li>Mask如何做也是有技巧的，如果一直用标记[MASK]代替会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。</li>
</ul>
</li>
<li><p>Pre-training Task 2#: Next Sentence Prediction</p>
<ul>
<li>因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。</li>
</ul>
</li>
<li><p>Fine-tunning</p>
<ul>
<li>分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state  ，加一层权重后softmax预测label proba： </li>
<li>其他预测任务需要进行一些调整，如图：</li>
</ul>
</li>
</ul>
<p><img src="/posts/NLP/28.jpg" alt></p>
<p>因为大部分参数都和预训练时一样，精调会快一些，所以推荐多试一些参数。</p>
<p>具体见我的另一篇博客：<a href="https://chenk.tech/posts/1424e830.html" target="_blank" rel="noopener">Bert模型解析</a></p>
<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>对比一下三种语言模型结构，BERT使用的是Transformer编码器，由于self-attention机制，所以模型上下层直接全部互相连接的。而OpenAI GPT使用的是Transformer解码器，它是一个需要从左到右的受限制的Transformer，而ELMo使用的是双向LSTM，虽然是双向的，但是也只是在两个单向的LSTM的最高层进行简单的拼接。所以作者们任务只有BERT是真正在模型所有层中是双向的。</p>
<p><img src="/posts/NLP/27.jpg" alt></p>
<p>而在模型的输入方面，BERT做了更多的细节，如下图。他们使用了WordPiece embedding作为词向量，并加入了位置向量和句子切分向量。并在每一个文本输入前加入了一个CLS向量，后面会有这个向量作为具体的分类向量。</p>
<p><img src="/posts/NLP/26.jpg" alt></p>
<p>近日，百度提出知识增强的语义表示模型 ERNIE（Enhanced Representation from kNowledge IntEgration），并发布了基于 PaddlePaddle 的开源代码与模型，在语言推断、语义相似度、命名实体识别、情感分析、问答匹配等自然语言处理（NLP）各类中文任务上的验证显示，模型效果全面超越 BERT。由此可以看出，预训练模型已成为近来NLP领域的潮流。</p>
<p><em>参考链接</em>：</p>
<blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1902.06006.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.06006.pdf</a></li>
<li><a href="https://blog.csdn.net/skyseezhang/article/details/106951103" target="_blank" rel="noopener">https://blog.csdn.net/skyseezhang/article/details/106951103</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/152471599" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/152471599</a></li>
<li><a href="https://baike.sogou.com/historylemma?lId=177732926" target="_blank" rel="noopener">https://baike.sogou.com/historylemma?lId=177732926</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>CS224n 12_Subword Models</title>
    <url>/posts/44cf3868.html</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h2 id="语音学"><a href="#语音学" class="headerlink" title="语音学"></a>语音学</h2><p>Phonology假定了一组或多组独特的分类单元：音素（作为独特的特征存在）。声音本身在语言中没有意义，Subword是音素的下一级的形态学，是具有意义的最低级别</p>
<p><img src="/posts/NLP/15.png" alt></p>
<p>传统上，（morphemes）词素是最小的语义单位（semantic unit），长得像下面这样：</p>
<script type="math/tex; mode=display">\left[\left[\text {un}\left[[\text { fortun }(\mathrm{e})]_{\text { Root }} \text { ate }\right]_{\text { STEM }}\right]_{\text { STEM }} \text {ly}\right]_{\text { WORD }}</script><h2 id="进击的词汇处理"><a href="#进击的词汇处理" class="headerlink" title="进击的词汇处理"></a>进击的词汇处理</h2><p><strong>深度学习</strong>中的递归神经网络是一种尝试处理更大词汇量的一种方法，更大的词汇量比如一些网络词汇，或者是字符与数字的组合（火星文），例如下面这样：</p>
<p><img src="/posts/NLP/16.png" alt></p>
<p><strong>Character-Level Models</strong>：我们可以拓展词嵌入为字符嵌入，这样我们可以为未知单词生成词嵌入，相似的拼写单词将共享相似的词嵌入，同时能够解决OOV问题。也就是说：连续语言可以作为字符来处理，我们将所有的语言处理均建立在字符序列上，不考虑 word-level。上面两种方法事实上都是可行的（深度学习方法与字符嵌入方法）。</p>
<p>下面是一个简单地Character-Level Model:</p>
<p><img src="/posts/NLP/17.png" alt></p>
<p>从上图可以看出，字符级别的机器翻译效果较单词级别的效果有所提升，但是其有一个很显著的问题是随着模型层数的增大，其运行时间较单词级别的模型增长要快得多，如下图所示：</p>
<p><img src="/posts/NLP/18.png" alt></p>
<h1 id="Character-Level-model"><a href="#Character-Level-model" class="headerlink" title="Character-Level model"></a>Character-Level model</h1><p>Character-Level model大体可以分为两个趋势：<strong>一种是与word-level model完全相同，只不过是输入不同，另一种是hybrid模型，其输入主要为word，需要时再用character。</strong>下面分别介绍这两个方向：</p>
<h2 id="Byte-Pair-Encoding（BPE）"><a href="#Byte-Pair-Encoding（BPE）" class="headerlink" title="Byte Pair Encoding（BPE）"></a>Byte Pair Encoding（BPE）</h2><p>BPE可以看作是pure Character-Level model，属于上述两种趋势的第一种，其核心思想是将经常出现的Byte pair作为一个新的Byte并加入到字典中。它先将文本中所有的character加入字典，然后将经常出现的character对添加到字典中，下面是一个例子：</p>
<p><img src="/posts/NLP/19.png" alt></p>
<p>该模型事先设定好了character对的大小以及字典的大小，当字典达到数量后便停止。另外一个基于character的模型是Character-based LSTM，其整体结构如下：</p>
<p><img src="/posts/NLP/20.png" alt></p>
<p>该结构表明，基于character-level的CNNs + Highway Network能更好地提取语意和结构信息，从而帮助提升机器翻译的效果。</p>
<h2 id="Hybrid-NMT"><a href="#Hybrid-NMT" class="headerlink" title="Hybrid NMT"></a>Hybrid NMT</h2><p>第二种趋势是混合模型，其核心思想是大部分情况用word-level，只在需要时，如出现字典外的词时才使用character-level。如下面这个模型：</p>
<p><img src="/posts/NLP/21.png" alt></p>
<p>该模型当遇到OOV（out of vocabulary）词时，采用vocabulary-level的beam search，寻找最适合的结果。实验结果表明，该模型可以提高机器翻译的效果，尤其是对于一些字典之外的词或者需要音译的词。</p>
<h2 id="FastText-embeddings"><a href="#FastText-embeddings" class="headerlink" title="FastText embeddings"></a>FastText embeddings</h2><p>FastText embeddings的目的在于采用类似于word2vec的模型去生成character-level的词向量，这些词向量对于生僻词或者拥有复杂语法的语言有很好的适应能力。其思路是将一个词拆分成若干个N-grams，经过训练后将这些N-gram的词向量和这个词的词向量进行平均作为最终这个词的词向量，比如where可以拆分成：</p>
<p><img src="/posts/NLP/22.png" alt></p>
<p>最终实验表明，该模型的Word similarity效果要好于原始的word2vec，而且对于一些生僻词有更好的处理能力。</p>
<h1 id="缺点分析"><a href="#缺点分析" class="headerlink" title="缺点分析"></a>缺点分析</h1><ol>
<li>语义上理解可能没那么好</li>
<li>样本多的时候效果略微下降</li>
<li>有一些语言不太适合用形态学</li>
</ol>
<blockquote>
<p>参考链接：</p>
</blockquote>
<ul>
<li><a href="https://www.dazhuanlan.com/2020/03/09/5e65d24e1acfb/" target="_blank" rel="noopener">https://www.dazhuanlan.com/2020/03/09/5e65d24e1acfb/</a></li>
<li><a href="http://web.stanford.edu/class/cs224n/index.html#coursework" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/index.html#coursework</a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>CS224n 02_Word Vectors and Word Senses</title>
    <url>/posts/73ac7324.html</url>
    <content><![CDATA[<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h2 id="模型训练步骤"><a href="#模型训练步骤" class="headerlink" title="模型训练步骤"></a>模型训练步骤</h2><p>首先我们回顾一下前面说的Word2vec的main idea：</p>
<p><img src="/posts/NLP/11.png" alt></p>
<p>我们训练这个模型的主要方式是以下四步：</p>
<ol>
<li>给每个单词一个初始向量</li>
<li>遍历整个语料库的所有单词</li>
<li>使用中心单词预测周围的单词</li>
<li>更新单词向量以便更好地预测</li>
</ol>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>那么我们是用什么方法来预测单词呢？我们要降低loss，也就等价于概率最大化，在这里我们利用极大似然函数求解，首先我们定义预测单词的概率：</p>
<script type="math/tex; mode=display">P(o|c) = \frac{exp(u_0^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}</script><p>我们的目标函数是：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J_{t}(\theta)</script><p>其中</p>
<script type="math/tex; mode=display">J_{t}(\theta)=\log \sigma\left(u_{o}^{T} v_{c}\right)+\sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)}\left[\log \sigma\left(-u_{j}^{T} v_{c}\right)\right]</script><h2 id="想一想"><a href="#想一想" class="headerlink" title="想一想"></a>想一想</h2><p>我们思考两个问题：</p>
<ol>
<li>我们把所有单词拿出来计算预测单词的概率$P(0|c)$，会给计算机带来很大的负担。<ul>
<li>我们会采用两个方式缓解这个问题，也就是上一节课末尾提出的negative sampling和hierarchical softmax。</li>
</ul>
</li>
<li>词向量有什么局限性呢？<ul>
<li>一个单词只有唯一一种向量表征方式，无法用一个向量表征两个意思（ElMo解决）</li>
<li>滑窗体现不出语序的关系</li>
</ul>
</li>
</ol>
<p>因此我们以后会用Transformer来解决这两问题（Transformer和Word2Vec没什么关系，和RNN、LSTM一脉相承，因此以后再讲）。</p>
<p>Word2Vec模型是一种基于local context window的direct prediction预测模型，对于学习word vector，还有另一类模型是<strong>count based global matrix factorization</strong></p>
<h1 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h1><p>SVD主要基于count based global matrix factorization，也就是共现矩阵，因此下面首先介绍共现矩阵的概念：</p>
<h2 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h2><h3 id="主要内涵："><a href="#主要内涵：" class="headerlink" title="主要内涵："></a>主要内涵：</h3><p>共现矩阵主要用于<strong>发现主题，解决词向量相近关系的表示</strong>。首先将共现矩阵行(列)作为词向量</p>
<p>例如：语料库如下：</p>
<ul>
<li>I like deep learning.</li>
<li>I like NLP.</li>
<li>I enjoy flying.</li>
</ul>
<p>若使用对称的窗函数（左右window length都为1），则共现矩阵表示如下：</p>
<p><img src="/posts/NLP/12.png" alt></p>
<p>解释一下这个表：”I like”出现在第1，2句话中，一共出现2次，所以对应矩阵位置$A[0,1]=2$。对称的窗口指的是，“like I”也是2次，所以对应矩阵位置$A[1,0]=2$</p>
<p>我们将共现矩阵行(列)作为词向量表示得到上面的表后，可以知道”like”，”enjoy”都是在”I”附近且统计数目大约相等，可以认为他们意思相近。</p>
<h3 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h3><ul>
<li>面临稀疏性问题</li>
<li>向量维数随着词典大小线性增长</li>
</ul>
<p>解决方法：SVD、PCA降维（但是计算量大）</p>
<h2 id="降维方法"><a href="#降维方法" class="headerlink" title="降维方法"></a>降维方法</h2><h3 id="Method-1-Dimensionality-Reduction-on-X-HW1"><a href="#Method-1-Dimensionality-Reduction-on-X-HW1" class="headerlink" title="Method 1: Dimensionality Reduction on X (HW1)"></a>Method 1: Dimensionality Reduction on X (HW1)</h3><p>使用SVD方法将共现矩阵$X$分解为$U \Sigma V^{\top}, \Sigma$是对角矩阵，对角线上的值是矩阵的奇异值，$U,V$是对应于行和列的正交基。</p>
<p>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的$k$个值，并将矩阵$U,V$的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</p>
<p><img src="/posts/NLP/13.png" alt></p>
<h3 id="Method-2-Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Method-2-Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Method 2: Hacks to X (several used in Rohde et al. 2005)"></a>Method 2: Hacks to X (several used in Rohde et al. 2005)</h3><p>有很多高频词比如the, this等是我们不太需要关注的，为了应付这些词汇，我们可以用下面的方法降维：</p>
<ul>
<li>对高频词进行缩放<ul>
<li>使用log进行缩放</li>
<li>min(X,t),t≈100</li>
</ul>
</li>
<li>直接全部忽视</li>
<li>在基于window的计数中，提高更加接近的单词的计数</li>
<li>使用Person相关系数</li>
</ul>
<h1 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h1><h2 id="现有方法的问题与GloVe的目标"><a href="#现有方法的问题与GloVe的目标" class="headerlink" title="现有方法的问题与GloVe的目标"></a>现有方法的问题与GloVe的目标</h2><p>比较SVD这种<strong>count based</strong>模型与Word2Vec这种<strong>direct prediction</strong>模型，它们各有优缺点：</p>
<ul>
<li><p>Count based模型优点是:</p>
<ul>
<li>训练快速</li>
<li>有效的利用了统计信息</li>
</ul>
</li>
<li><p>Count based模型缺点是：</p>
<ul>
<li>对于高频词汇较为偏向</li>
<li>仅能概括词组的相关性</li>
<li>有的时候产生的word vector对于解释词的含义如word analogy等任务效果不好</li>
</ul>
</li>
<li><p>Direct Prediction模型优点是：</p>
<ul>
<li>可以概括比相关性更为复杂的信息</li>
<li>进行word analogy等任务时效果较好</li>
</ul>
</li>
<li><p>Direct Prediction模型缺点是：</p>
<ul>
<li>对统计信息利用的不够充分</li>
</ul>
</li>
</ul>
<p>所以Manning教授他们想采取一种方法可以结合两者的优势，并将这种算法命名为GloVe（Global Vectors的缩写），表示他们可以有效的利用全局的统计信息。</p>
<p>我们理解GloVe，重点便是：<strong>GloVe利用全局统计量，以最小二乘为目标，预测单词$j$出现在单词$i$上下文中的概率。</strong></p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="计算共现矩阵"><a href="#计算共现矩阵" class="headerlink" title="计算共现矩阵"></a>计算共现矩阵</h3><p>$X$表示word-word共现矩阵，其中$X_{ij}$表示词$j$出现在词$i$的上下文的次数。令$X_{i}=\sum_{k}X_{ik}$为任意词$k$出现在词$i$的上下文的次数。最后，令$P_{ij}=P(w_{j}\mid w_{i})=\frac{X_{ij}}{X_{i}}$是词$j$出现在词$i$的上下文的概率。</p>
<p>计算这个矩阵需要遍历一次整个语料库获得统计信息。对庞大的语料库，这样的遍历会产生非常大的计算量，但是这只是一次性的前期投入成本。</p>
<h3 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a>计算损失函数</h3><p>回想一下Skip-Gram模型，我们使用softmax来计算词$j$出现在词$i$的上下文的概率：</p>
<script type="math/tex; mode=display">Q_{ij}=\frac{exp(u_{j}^{T}v_{i})}{\sum_{w=1}^{W}exp(u_{w}^{T}v_{i})}</script><p>隐含的全局交叉熵损失可以如下计算：</p>
<script type="math/tex; mode=display">J=-\sum_{i\in corpus} \sum_{j\in context(i)}log\;Q_{ij}</script><p>或者可以写成：</p>
<script type="math/tex; mode=display">J=-\sum_{i=1}^{W}\sum_{j=1}^{W}X_{ij}log\;Q_{ij}</script><p>其中$X_{ij}$由共现矩阵相应位置的值给定。</p>
<p>交叉熵损失的一个显着缺点是要求分布$Q$被正确归一化，因为对整个词汇的求和的计算量是非常大的。因此，我们使用一个最小二乘的目标函数，其中$P$和$Q$的归一化因子被丢弃了：</p>
<script type="math/tex; mode=display">\widehat{J}=\sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(\widehat{P}_{ij}-\widehat{Q}_{ij})^{2}</script><p>其中$\hat{P}_{i j}=X_{i j}$和 $\hat{Q}_{i j}=\exp \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)$是未归一化分布。这个公式带来了一个新的问题，$X_{ij}$经常会是很大的值，从而难以优化。一个有效的改变是最小化$\widehat{P}$和$\widehat{Q}$对数的平方误差：</p>
<script type="math/tex; mode=display">\begin{eqnarray}  \widehat{J} &amp;=&amp;\sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(log(\widehat{P}_{ij})-log(\widehat{Q}_{ij}))^{2} \nonumber \\ &amp;=&amp; \sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(u_{j}^{T}v_{i}-log\; X_{ij})^{2} \nonumber \end{eqnarray}</script><p>另外一个问题是权值因子$X_i$不能保证是最优的。因此，我们引入更一般化的权值函数，我们可以自由地依赖于上下文单词：</p>
<script type="math/tex; mode=display">\widehat{J}=\sum_{i=1}^{W}\sum_{j=1}^{W}f(X_{ij})(u_{j}^{T}v_{i}-log\; X_{ij})^{2}</script><p>或者我们从另一个角度来看这个损失函数公式的结果：</p>
<p><img src="/posts/NLP/14.jpg" alt></p>
<p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词$x$的co-occurrence probability的比值来描述，例如对于solid固态，虽然$P(solid|ice)$与$P(solid|stream)$本身很小，不能透露有效的信息，但是它们的比值$\frac{P(solid|ice)}{P(solid|stream)}$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大，对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义。</p>
<p>基于对于以上概率比值的观察，我们假设模型的函数有如下形式：</p>
<script type="math/tex; mode=display">F(w_i,w_j,w_k) = \frac{P_{ik}}{P_{jk}}</script><p>其中，$w$代表了context vector，如上例中的solid，gas，water，fashion等。$w_i, w_j$则是我们要比较的两个词汇，如上例中的ice，steam。</p>
<p>$F$的可选的形式过多，我们希望有所限定。首先我们希望的是$F$能有效的在单词向量空间内表示概率比值，由于向量空间是线性空间，一个自然的假设是$F$是关于向量$w_i,w_j$的差的形式：</p>
<script type="math/tex; mode=display">F(w_i-w_j,w_k) = \frac{P_{ik}}{P_{jk}}</script><p>我们可以直接通过矢量的点乘将左边化为标量形式</p>
<script type="math/tex; mode=display">F((w_i-w_j)^T,w_k) = \frac{P_{ik}}{P_{jk}}</script><p>在此，作者又对其进行了对称性分析，即对于word-word co-occurrence，将向量划分为center word还是context word的选择是不重要的，即我们在交换 $w\leftrightarrow \bar w$ 与$X\leftrightarrow\bar X$的时候该式仍然成立。如何保证这种对称性呢？</p>
<p><img src="/posts/NLP/14.png" alt></p>
<p>作者最终尝试的比较好的权重参数是：</p>
<script type="math/tex; mode=display">f(x)=
\begin{cases}
(x/x_{max})^{\alpha}& \text{if x} \lt \text{}x_{max}\\
1& \text{otherwise}
\end{cases}</script><blockquote>
<p>部分引用自：</p>
</blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/60208480" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60208480</a></li>
<li><a href="https://looperxx.github.io/CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" target="_blank" rel="noopener">https://looperxx.github.io/CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59016893" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59016893</a></li>
<li><a href="https://www.aclweb.org/anthology/Q16-1028.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/Q16-1028.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>CS224n 01_Introduction and Word Vectors</title>
    <url>/posts/fb70fd3e.html</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h2><p>理解意义的最普遍的语言方式(linguistic way) : 语言符号与语言符号的意义的转化</p>
<script type="math/tex; mode=display">\boxed{\text{signifier(symbol)}\Leftrightarrow \text{signified(idea or thing)}} \ = \textbf{denotational semantics}</script><p>在所有的NLP任务中，第一个也是可以说是最重要的共同点是我们如何将单词表示为任何模型的输入（编码）。</p>
<h2 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h2><p>我们思考一下：我们如何让计算机学习到语言的意义？</p>
<h3 id="1、WordNet"><a href="#1、WordNet" class="headerlink" title="1、WordNet"></a>1、WordNet</h3><p>Wordnet指的是一个包含同义词集和上位词列表的辞典，我们很容易通过nltk获取到这种词典：</p>
<p><img src="/posts/NLP/1.png" alt></p>
<p>这种方式的缺点：</p>
<ul>
<li>需要人工操作，主观性强</li>
<li>忽略了细微差别</li>
<li>无法计算相似度</li>
</ul>
<h3 id="2、One-hot编码"><a href="#2、One-hot编码" class="headerlink" title="2、One-hot编码"></a>2、One-hot编码</h3><blockquote>
<p>motel = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0] \\ hotel = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0]</p>
</blockquote>
<p>这种方式的缺点：</p>
<ul>
<li>无法衡量词语相似性</li>
<li>向量维度过大，训练难</li>
</ul>
<p>解决方式：</p>
<ul>
<li>使用类似 WordNet 的工具中的列表，获得相似度，但会因不够完整而失败</li>
<li>学习在向量本身中编码相似性</li>
</ul>
<h3 id="3、Representing-words-by-their-context"><a href="#3、Representing-words-by-their-context" class="headerlink" title="3、Representing words by their context"></a>3、Representing words by their context</h3><p>这种方式的思想在于：一个单词的意思是由经常出现在它附近的单词给出的，我们可以在单词$A$出现的上下文（固定大小的窗口）附近找到一组单词$a_k$来表示$A$：</p>
<p><img src="/posts/NLP/2.png" alt></p>
<h1 id="Word2vec-introduction"><a href="#Word2vec-introduction" class="headerlink" title="Word2vec introduction"></a>Word2vec introduction</h1><p><strong>思想</strong>：我们为每个单词构建一个密集的向量，使其与出现在相似上下文中的单词向量相似</p>
<p><img src="/posts/NLP/3.png" alt></p>
<p><strong>主要思路</strong>：给定大量的文本，给定一个初始词向量（低维度），对于文本中的每个位置$t$，其中有一个中心词$c$和上下文(“外部”)单词$o$。使用$c$和$o$的词向量的相似性，来计算给定$c$的$o$的概率(反之亦然)，然后我们不断调整词向量来最大化这个概率。</p>
<p>下图为窗口大小$j=2$时的$P\left(w_{t+j} | w_{t}\right)$计算过程，center word分别为into和banking：</p>
<p><img src="/posts/NLP/4.png" alt></p>
<p><img src="/posts/NLP/5.png" alt></p>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>对于每个位置$t=1, \ldots, T$，在大小为$m$的固定窗口内预测上下文单词，给定中心词$w_j$</p>
<script type="math/tex; mode=display">Likelihoood = L(\theta) = \prod^{T}_{t=1} \prod_{-m \leq j \leq m \atop j \neq 0} P(w_{t+j} | w_{t} ; \theta)</script><p>其中，$\theta$为所有需要优化的变量。<br>目标函数$J(\theta)$(有时被称为代价函数或损失函数) 是(平均)负对数似然</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)</script><p>其中$log$形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。</p>
<h2 id="最小化目标函数-Leftrightarrow-最大化预测精度"><a href="#最小化目标函数-Leftrightarrow-最大化预测精度" class="headerlink" title="最小化目标函数$\Leftrightarrow$最大化预测精度"></a>最小化目标函数$\Leftrightarrow$最大化预测精度</h2><p><strong>问题：如何计算$P(w_{t+j} | w_{t} ; \theta)$？</strong></p>
<p>对于每个单词$w$都是用两个向量表示：</p>
<p><em>$v_w$当$w$是中心词时
</em>$u_w$当$w$是上下文词时</p>
<p>于是对于一个中心词$c$和一个上下文词$o$，就得到了Word2vec prediction function：</p>
<script type="math/tex; mode=display">P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}</script><blockquote>
<p>公式中，向量$u_o$和向量$v_c$进行点乘。向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。</p>
</blockquote>
<p>在公式中分母对整个词汇表进行标准化，从而给出概率分布。</p>
<p>附注：$softmax function：\mathbb{R}^{n} \rightarrow \mathbb{R}^{1}$</p>
<script type="math/tex; mode=display">\operatorname{softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}</script><p>将任意值$x_i$映射到概率分布$p_i$</p>
<h2 id="梯度下降法求解"><a href="#梯度下降法求解" class="headerlink" title="梯度下降法求解"></a>梯度下降法求解</h2><p>首先我们随机初始化$u_{w}\in\mathbb{R}^d$和$v_{w}\in\mathbb{R}^d$，而后使用梯度下降法进行更新：</p>
<script type="math/tex; mode=display">\begin{align} \frac{\partial}{\partial v_c}\log P(o|c) &=\frac{\partial}{\partial v_c}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ &=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\ &=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\ &=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)} \end{align}</script><p>我们可以对上述结果重新排列如下，第一项是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。</p>
<script type="math/tex; mode=display">\begin{align} \frac{\partial}{\partial v_c}\log P(o|c) &=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ &=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\ &=u_o-\sum_{w\in V}P(w|c)u_w \end{align}</script><p>再对$u_o$进行偏微分计算，注意这里的$u_o$是$u_{w=o}$的简写，故可知</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial u_o}\sum_{w \in V } u_w^T v_c = \frac{\partial}{\partial u_o} u_o^T v_c = \frac{\partial u_o}{\partial u_o}v_c + \frac{\partial v_c}{\partial u_o}u_o= v_c</script><p>因此有：</p>
<script type="math/tex; mode=display">\begin{align} \frac{\partial}{\partial u_o}\log P(o|c) &=\frac{\partial}{\partial u_o}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ &=\frac{\partial}{\partial u_o}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\ &=\frac{\partial}{\partial u_o}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\ &=v_c-\frac{\sum\frac{\partial}{\partial u_o}\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ &=v_c - \frac{\exp(u_o^Tv_c)v_c}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ &=v_c - \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}v_c\\ &=v_c - P(o|c)v_c\\ &=(1-P(o|c))v_c \end{align}</script><p>可以理解，当$P(o|c) \to 1$，即通过中心词$c$我们可以正确预测上下文词$o$，此时我们不需要调整$u_o$，反之，则相应调整$u_o$。</p>
<h1 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h1><p>与WordVector为词向量编码不同，SVD Based Methods为词嵌入的方法，首先遍历一个很大的数据集和统计词的共现计数矩阵$X$，然后对矩阵$X$进行SVD分解得到$USV^{T}$。然后我们使用$U$的行来作为字典中所有词的词向量。我们来讨论一下矩阵$X$的几种选择。</p>
<h2 id="矩阵-X-的选择"><a href="#矩阵-X-的选择" class="headerlink" title="矩阵$X$的选择"></a>矩阵$X$的选择</h2><h3 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h3><p>我们最初的尝试，我们猜想相关连的单词在同一个文档中会经常出现。例如，“banks” “bonds” “stocks” “moneys”等等，出现在一起的概率会比较高。但是“banks” “octopus” “banana” “hockey”不大可能会连续地出现。我们根据这个情况来建立一个Word-Document矩阵，$X$是按照以下方式构建：遍历数亿的文档和当词$i$出现在文档$j$，我们对$X_{ij}$加一。这显然是一个很大的矩阵$\mathbb{R}^{|V|\times M}$，它的规模是和文档数量$M$成正比关系。因此我们可以尝试更好的方法。</p>
<h3 id="Window-based-Co-occurrence-Matrix"><a href="#Window-based-Co-occurrence-Matrix" class="headerlink" title="Window based Co-occurrence Matrix"></a>Window based Co-occurrence Matrix</h3><p>与上一个不同的是，矩阵$X$存储单词的共现，从而成为一个关联矩阵。在此方法中，我们计算每个单词在特定大小的窗口中出现的次数。我们按照这个方法对语料库中的所有单词进行统计。</p>
<ul>
<li>生成维度为$|V| \times|V|$的共现矩阵$X$</li>
<li>在$X$上应用 SVD 从而得到$X = {USV}^T$</li>
<li>选择$U$前$k$行得到$k$维的词向量<br>*$\frac{\sum_{i=1}^{k} \sigma_{i}}{\sum_{i=1}^{|V|} \sigma_{i}}$表示第一个k维捕获的方差量</li>
</ul>
<h2 id="将SVD应用于共现矩阵"><a href="#将SVD应用于共现矩阵" class="headerlink" title="将SVD应用于共现矩阵"></a>将SVD应用于共现矩阵</h2><p>我们对矩阵$X$使用SVD，观察奇异值（矩阵 S 上对角线上元素），根据期望的捕获方差百分比截断，留下前$k$个元素：</p>
<p>然后取子矩阵$U_{1:|V|, 1:k}$作为词嵌入矩阵。这就给出了词汇表中每个词的$k$维表示，如下图所示：</p>
<p><img src="/posts/NLP/6.png" alt></p>
<p>通过选择前$k$个奇异向量来降低维度：</p>
<p><img src="/posts/NLP/7.png" alt></p>
<p>这两种方法都给我们提供了足够的词向量来编码语义和句法(part of speech)信息，但也有许多其他问题：</p>
<ul>
<li>矩阵的维度会经常发生改变（经常增加新的单词和语料库的大小会改变）。</li>
<li>矩阵会非常的稀疏，因为很多词不会共现。</li>
<li>矩阵维度一般会非常高$\approx 10^{6}\times 10^{6}$</li>
<li>基于 SVD 的方法的计算复杂度很高$(m×n$矩阵的计算成本是$O({mn}^2))$，并且很难合并新单词或文档</li>
<li>需要在$X$上加入一些技巧处理来解决词频的极剧的不平衡</li>
</ul>
<p>对上述讨论中存在的问题存在以下的解决方法：</p>
<ul>
<li>忽略功能词，例如 “the”，“he”，“has” 等等，构建停用词库。</li>
<li>使用ramp window，即根据文档中单词之间的距离对共现计数进行加权</li>
<li>使用皮尔逊相关系数并将负计数设置为0，而不是只使用原始计数</li>
</ul>
<h1 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h1><p>为了避免上述弊端（数据集过于庞大等），我们考虑设计一个模型，该模型的参数就是词向量。然后根据一个目标函数训练模型，在每次模型的迭代计算误差，并遵循一些更新规则，该规则具有惩罚造成错误的模型参数的作用，从而可以学习到词向量，我们称这个方法为“反向传播”，模型和任务越简单，反向传播训练速度就越快。</p>
<p>Word2vec作为一个概率模型，定义了两个算法和两个训练方法：</p>
<ul>
<li>两个算法：continuous bag-of-words（CBOW）和 skip-gram。CBOW 是根据中心词周围的上下文单词来预测该词的词向量。skip-gram 则相反，是根据中心词预测周围上下文的词的概率分布。</li>
<li>两个训练方法：negative sampling 和 hierarchical softmax。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。</li>
</ul>
<p>Language Models (Unigrams, Bigrams, etc.)</p>
<p>首先，我们需要创建一个模型来为一系列的单词分配概率。我们从一个例子开始：</p>
<blockquote>
<p>“The cat jumped over the puddle”</p>
</blockquote>
<p>一个好的语言模型会给这个句子很高的概率，因为在句法和语义上这是一个完全有效的句子。相似地，句子“stock boil fish is toy”会得到一个很低的概率，因为这是一个无意义的句子。在数学上，我们可以称为对给定 n 个词的序列的概率是：</p>
<script type="math/tex; mode=display">P(w_{1}, w_{2}, \ldots, w_{n})</script><p>我们可以采用一元语言模型方法(Unigram model)，假设单词的出现是完全独立的，从而分解概率</p>
<script type="math/tex; mode=display">P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i}\right)</script><p>但是我们知道这是不大合理的，因为下一个单词是高度依赖于前面的单词序列的。如果使用上述的语言模型，可能会让一个无意义的句子具有很高的概率。所以我们让序列的概率取决于序列中的单词和其旁边的单词的成对概率。我们称之为 bigram 模型：</p>
<script type="math/tex; mode=display">P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} | w_{i-1}\right)</script><p>但是这个方法还是有点简单，因为我们只考虑了一对邻近的单词而不是整个句子，但是这个方法已经能获得不错的效果了。现在我们根据概率模型讨论两个常用模型 —— CBOW和SkipGram模型。</p>
<h2 id="两个模型"><a href="#两个模型" class="headerlink" title="两个模型"></a>两个模型</h2><h3 id="1、CBOW"><a href="#1、CBOW" class="headerlink" title="1、CBOW"></a>1、CBOW</h3><p>CBOW的原句是Continuous Bag of Words Model，目标是通过给定中心词周围的其他词来预测中心词，首先我们设定已知参数。令我们模型的已知参数是 one-hot形式的词向量表示。输入的one-hot向量或者上下文我们用$x^{(c)}$表示，输出用$y^{(c)}$表示。在CBOW模型中，因为我们只有一个输出，因此我们把$y$称为是中心词的的one-hot向量。现在让我们定义模型的未知参数。</p>
<p>$w_{i}$：词汇表$V$中的单词$i$<br>$\mathcal{V}\in \mathbb{R}^{n\times |V|}$：输入词矩阵<br>$v_{i} ： \mathcal{V}$的第$i$列，单词$w_{i}$的输入向量表示<br>$\mathcal{U}\in \mathbb{R}^{|V|\times n}$：输出词矩阵<br>$u_{i}： \mathcal{U}$的第$i$行，单词$w_{i}$的输出向量表示</p>
<p>我们创建两个矩阵，$\mathcal{V}\in \mathbb{R}^{n\times |V|}$和$\mathcal{U}\in \mathbb{R}^{|V|\times n}$。其中$n$是嵌入空间的任意维度大小。$\mathcal{V}$是输入词矩阵，使得当其为模型的输入时，$\mathcal{V}$的第$i$列是词$w_{i}$的$n$维嵌入向量。我们定义这个$n \times 1$的向量为$v_{i}$。相似地，$\mathcal{U}$是输出词矩阵。当其为模型的输入时，$\mathcal{U}$的第$j$行是词$w_{j}$的$n$维嵌入向量。我们定义$\mathcal{U}$的这行为$u_{j}$。注意实际上对每个词$w_{i}$我们需要学习两个词向量（即输入词向量$v_{i}$和输出词向量$u_{i} ）。</p>
<p>我们将这个模型的训练分解为以下步骤</p>
<ol>
<li>为大小为$m$的输入上下文生成 one-hot 词向量<script type="math/tex; mode=display">(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in \mathbb{R}^{|V|})</script></li>
<li>从上下文得到嵌入词向量<script type="math/tex; mode=display">(v_{c-m}=\mathcal{V}x^{(c-m)},v_{c-m+1}=\mathcal{V}x^{(c-m+1)},...,v_{c+m}=\mathcal{V}x^{(c+m)}\in \mathbb{R}^{n})</script></li>
<li>对上述向量求平均值<script type="math/tex; mode=display">\widehat{v}=\frac{v_{c-m}+v_{c-m+1+...+v_{c+m}}}{2m}\in \mathbb{R}^{n}</script></li>
<li>生成一个分数向量$z = \mathcal{U}\widehat{v}\in \mathbb{R}^{|V|}$。相似向量的点积越高两个词越相似，从而获得更高的分数。将分数转换为概率：<script type="math/tex; mode=display">\widehat{y}=softmax(z)\in \mathbb{R}^{|V|}</script><ul>
<li>这里 softmax 是一个常用的函数。它将一个向量转换为另外一个向量，其中转换后的向量的第$i$个元素是<script type="math/tex; mode=display">\frac{e^{\widehat{y}_i}}{\sum_{k=1}^{|V|}e^{\widehat{y}_k}}</script>因为该函数是一个指数函数，所以值一定为正数；通过除以$\sum_{k=1}^{|V|}e^{\widehat{y}_k}$来归一化向量得到概率。</li>
</ul>
</li>
<li>我们希望生成的概率$\widehat{y} \in \mathbb{R}^{|V|}$与实际的概率$y \in \mathbb{R}^{|V|}$匹配。使得其刚好是实际的词，也就是这个     one-hot 向量。</li>
</ol>
<p><img src="/posts/NLP/8.png" alt></p>
<p>为了计算损失函数，我们首先给出度量两个概率分布的距离的方法 —— <strong>交叉熵$H(\widehat{y}, y)$</strong></p>
<script type="math/tex; mode=display">H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right)</script><p>上面的公式中，$y$是 one-hot 向量。因此上面的损失函数可以简化为：</p>
<script type="math/tex; mode=display">H(\widehat{y}, y)= - y_{j}\,log(\widehat{y}_{j})</script><p>c 是正确词的 one-hot 向量的索引。我们现在可以考虑我们的预测正确并且$\widehat{y}_{c}=1$的情况，可以计算得$H(\widehat{y}, y)=- log(1)=0$；若预测非常差并且$\widehat{y}_{c}=0.01$。和前面类似，我们可以计算损失$H(\widehat{y}, y)=-log(0.01)=4.605$。下面定义优化目标函数：</p>
<script type="math/tex; mode=display">\begin{aligned} \text { minimize } J &=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right)\\ &=-\log P\left(u_{c} | \hat{v}\right) \\ &=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\ &=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right) \end{aligned}</script><p>我们使用 SGD 来更新所有相关的词向量$u_{c} 和 v_{j}$。SGD对一个窗口计算梯度和更新参数：</p>
<script type="math/tex; mode=display">\begin{array}{l}{\mathcal{U}_{\text {new}} \leftarrow \mathcal{U}_{\text {old}}-\alpha \nabla_{\mathcal{U}} J} \\ {\mathcal{V}_{\text {old}} \leftarrow \mathcal{V}_{\text {old}}-\alpha \nabla_{\mathcal{V}} J}\end{array}</script><h3 id="2、Skip-Gram-Model"><a href="#2、Skip-Gram-Model" class="headerlink" title="2、Skip-Gram Model"></a>2、Skip-Gram Model</h3><p>在这里的预测任务和CBOW的正好相反，在CBOW中我们已知附近词预测中心词，在这里我们已知中心词预测附近词。</p>
<ul>
<li>生成中心词的one-hot向量$x\in \mathbb{R}^{|V|}$</li>
<li>我们对中心词$v_{c}=\mathcal{V}x\in \mathbb{R}^{|V|}$得到词嵌入向量</li>
<li>生成分数向量$z = \mathcal{U}v_{c}$</li>
<li>将分数向量转化为概率，$\widehat{y}=softmax(z)$，注意$\widehat{y}_{c-m},…,\widehat{y}_{c-1},\widehat{y}_{c+1},…,\widehat{y}_{c+m}$是每个上下文词观察到的概率</li>
<li>我们希望我们生成的概率向量匹配真实概率$y^{(c-m)},…,y^{(c-1)},y^{(c+1)},…,y^{(c+m)}$，实际的输出的是one-hot向量。</li>
</ul>
<p>大致如下：</p>
<p><img src="/posts/NLP/9.png" alt></p>
<p>我们要定义目标函数，首先假设给定中心词，所有输出的词是完全独立的（不甚严谨），于是可以写出目标函数：</p>
<script type="math/tex; mode=display">\begin{aligned} \text { minimize } J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)} \\ &=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \end{aligned}</script><p>通过这个目标函数，我们可以计算出与未知参数相关的梯度，并且在每次迭代中通过 SGD 来更新它们。另外需要注意到</p>
<script type="math/tex; mode=display">\begin{aligned} J &=-\sum_{j=0, j \neq m}^{2 m} \log P\left(u_{c-m+j} | v_{c}\right) \\ &=\sum_{j=0, j \neq m}^{2 m} H\left(\hat{y}, y_{c-m+j}\right) \end{aligned}</script><p>是向量$\widehat{y}$的概率和 one-hot 向量$y_{c-m+j}$之间的交叉熵。</p>
<p>以下为Skip-Gram的代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span>  pprint</span><br><span class="line">obj_path = <span class="string">r'E:\back_up\NLP\process_train.txt'</span></span><br><span class="line">stop_word_file = <span class="string">r"E:\back_up\NLP\pro1\data\stop_word.txt"</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stop_word</span><span class="params">(stop_word_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    加载停用词</span></span><br><span class="line"><span class="string">    :param stop_word_file: </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    stopwords = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> open(stop_word_file, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>).readlines()]</span><br><span class="line">    <span class="keyword">return</span> stopwords + list(<span class="string">"0123456789"</span>) + [<span class="string">r'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path, windows_len, lines_number)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param file_path:   数据的路径</span></span><br><span class="line"><span class="string">    :param windows_len:   窗口长度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words = set()     <span class="comment"># 保存词</span></span><br><span class="line">    sentences = []    <span class="comment"># 保存句子</span></span><br><span class="line">    stopwords = get_stop_word(stop_word_file)</span><br><span class="line">    stopwords = set(stopwords)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            line = fp.readline()</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line <span class="keyword">or</span> count &gt; lines_number:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># print(line)</span></span><br><span class="line">            out_str = []</span><br><span class="line">            result = jieba.cut(line, cut_all=<span class="literal">False</span>)    <span class="comment"># 精确模式</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> result:</span><br><span class="line">                <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> stopwords <span class="keyword">and</span> len(c) &gt; <span class="number">1</span>:</span><br><span class="line">                    out_str.append(c)</span><br><span class="line">                    words.add(c)     <span class="comment"># 保存所有的词</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">            out_str.append(<span class="string">"EOS"</span>)</span><br><span class="line">            sentences.append(out_str)</span><br><span class="line">    word2id = &#123;&#125;</span><br><span class="line">    words = list(words)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">        word2id[words[i]] = i + <span class="number">1</span></span><br><span class="line">    word2id[<span class="string">"EOS"</span>] = len(words) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 构造输入input和输出labels</span></span><br><span class="line">    input = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="comment"># 构造训练数据和标签</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">for</span> word_index <span class="keyword">in</span> range(len(sentence)):</span><br><span class="line">            start = max(<span class="number">0</span>, word_index - windows_len)</span><br><span class="line">            end = min(word_index + windows_len + <span class="number">1</span>, len(sentence))</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> range(start, end):</span><br><span class="line">                <span class="keyword">if</span> index == word_index:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    input_word_id = word2id.get(sentence[word_index], <span class="literal">None</span>)</span><br><span class="line">                    label_word_id = word2id.get(sentence[index], <span class="literal">None</span>)</span><br><span class="line">                    <span class="keyword">if</span> input_word_id <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> label_word_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    input.append(int(input_word_id))</span><br><span class="line">                    labels.append(int(label_word_id))</span><br><span class="line">    <span class="keyword">return</span> words, word2id, sentences, input, labels, len(words)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainData</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, labels, words, vocab_size, batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param inputs:   输入</span></span><br><span class="line"><span class="string">        :param labels:   输出</span></span><br><span class="line"><span class="string">        :param words:    所有单词</span></span><br><span class="line"><span class="string">        :param vocab_size:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.words = words</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.input_length = len(inputs)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch_data</span><span class="params">(self, batch_count)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param batch_count:  batch计数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 确定选取的batch大小</span></span><br><span class="line">        start_position = batch_count * self.batch_size</span><br><span class="line">        end_position = min((batch_count + <span class="number">1</span>) * self.batch_size, self.input_length)</span><br><span class="line">        batch_input = self.inputs[start_position: end_position]</span><br><span class="line">        batch_labels = self.labels[start_position: end_position]</span><br><span class="line">        batch_input = np.array(batch_input, dtype=np.int32)</span><br><span class="line">        batch_labels = np.array(batch_labels, dtype=np.int32)</span><br><span class="line">        batch_labels = np.reshape(batch_labels, [len(batch_labels), <span class="number">1</span>])   <span class="comment"># 转置</span></span><br><span class="line">        <span class="keyword">return</span> batch_input, batch_labels</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch_nums</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取数据的batch数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.input_length // self.batch_size + <span class="number">1</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, batch_nums, num_sampled, learning_rate)</span>:</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.batch_nums = batch_nums</span><br><span class="line">        self.num_sampled = num_sampled</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.batch_size = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># self.graph = tf.Graph()</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 创建placeholder</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"placeholders"</span>):</span><br><span class="line">            self.inputs = tf.placeholder(dtype=tf.int32, shape=[self.batch_size], name=<span class="string">"train_inputs"</span>)   <span class="comment"># 输入</span></span><br><span class="line">            self.labels = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, <span class="number">1</span>], name=<span class="string">"train_labels"</span>)</span><br><span class="line">            self.test_word_id = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">"test_word_id"</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 创建词向量</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"word_embedding"</span>):</span><br><span class="line">            self.embedding_dict = tf.get_variable(name=<span class="string">"embedding_dict"</span>, shape=[self.vocab_size, self.embedding_size],</span><br><span class="line">                                                  initializer=tf.random_uniform_initializer(<span class="number">-1</span>, <span class="number">1</span>, seed=<span class="number">1</span>)</span><br><span class="line">                                                  )</span><br><span class="line">            self.nce_weight = tf.get_variable(name=<span class="string">"nce_weight"</span>, shape=[self.vocab_size, self.embedding_size],</span><br><span class="line">                                              initializer=tf.random_uniform_initializer(<span class="number">-1</span>, <span class="number">1</span>, seed=<span class="number">1</span>)</span><br><span class="line">                                              )</span><br><span class="line">            self.nce_bias = tf.get_variable(name=<span class="string">"nce_bias"</span>, initializer=tf.zeros([self.vocab_size]))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 定义误差</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"creating_embedding"</span>):</span><br><span class="line">            embeded = tf.nn.embedding_lookup(self.embedding_dict, self.inputs)</span><br><span class="line">            self.embeded = tf.layers.dense(inputs=embeded, units=self.embedding_size, activation=tf.nn.relu)  <span class="comment"># 激活函数</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 定义误差</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"creating_loss"</span>):</span><br><span class="line">            self.loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.nce_loss(weights=self.nce_weight,</span><br><span class="line">                               biases=self.nce_bias,</span><br><span class="line">                               labels=self.labels,</span><br><span class="line">                               inputs=self.embeded,</span><br><span class="line">                               num_sampled=self.num_sampled,</span><br><span class="line">                               num_classes=self.vocab_size,</span><br><span class="line">                               <span class="comment"># remove_accidental_hits=True</span></span><br><span class="line">                               )</span><br><span class="line">             )</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 定义测试函数</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"evaluation"</span>):</span><br><span class="line">            norm = tf.sqrt(tf.reduce_sum(tf.square(self.embedding_dict), <span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">            self.normed_embedding_dict = self.embedding_dict / norm</span><br><span class="line">            test_embed = tf.nn.embedding_lookup(self.normed_embedding_dict, self.test_word_id)</span><br><span class="line">            self.similarity = tf.matmul(test_embed, tf.transpose(self.normed_embedding_dict), name=<span class="string">'similarity'</span>)</span><br><span class="line"> </span><br><span class="line">        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#  tensorboard 显示数据</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"summaries"</span>):</span><br><span class="line">            tf.summary.scalar(<span class="string">'loss'</span>, self.loss)   <span class="comment"># 在 tensorboard中显示信息</span></span><br><span class="line">            self.summary_op = tf.summary.merge_all()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, train_steps=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            <span class="comment"># 初始化变量</span></span><br><span class="line">            sess.run(tf.group(tf.local_variables_initializer(), tf.global_variables_initializer()))</span><br><span class="line">            writer = tf.summary.FileWriter(<span class="string">r'E:\back_up\NLP\graph'</span>, sess.graph)   </span><br><span class="line">            initial_step = <span class="number">0</span>                <span class="comment"># self.global_step.eval(session=sess)</span></span><br><span class="line">            step = <span class="number">0</span>   <span class="comment"># 记录总的训练次数</span></span><br><span class="line">            saver = tf.train.Saver(tf.global_variables(), max_to_keep=<span class="number">2</span>)   <span class="comment"># 保存模型</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> range(initial_step, train_steps):</span><br><span class="line">                total_loss = <span class="number">0.0</span>     <span class="comment"># 总的loss</span></span><br><span class="line">                <span class="keyword">for</span> batch_count <span class="keyword">in</span> tqdm(range(self.batch_nums)):</span><br><span class="line">                    batch_inputs, batch_labels = train_data.get_batch_data(batch_count)</span><br><span class="line">                    feed_dict = &#123;self.inputs: batch_inputs,</span><br><span class="line">                                 self.labels: batch_labels&#125;</span><br><span class="line"> </span><br><span class="line">                    sess.run(self.optimizer, feed_dict=feed_dict)</span><br><span class="line">                    batch_loss = sess.run(self.loss, feed_dict=feed_dict)</span><br><span class="line">                    summary = sess.run(self.summary_op, feed_dict=feed_dict)</span><br><span class="line">                    <span class="comment"># batch_loss, summary = sess.run([self.loss, self.summary_op])</span></span><br><span class="line">                    total_loss += batch_loss</span><br><span class="line">                    step += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> step % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                        saver.save(sess=sess, save_path=<span class="string">r'E:\back_up\NLP\global_variables\global_variables'</span>, global_step=step)</span><br><span class="line">                        writer.add_summary(summary, global_step=step)</span><br><span class="line">                print(<span class="string">'Train Loss at step &#123;&#125;: &#123;:5.6f&#125;'</span>.format(index, total_loss/self.batch_nums))</span><br><span class="line">            word_embedding = sess.run(self.embedding_dict)</span><br><span class="line">            np.save(<span class="string">r"E:\back_up\NLP\word_embedding\word_embedding"</span>, word_embedding)   <span class="comment"># 保存词向量</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(test_word, word2id, top_k=<span class="number">4</span>)</span>:</span>     <span class="comment"># 测试训练的词向量</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param test_word: </span></span><br><span class="line"><span class="string">    :param word2id: </span></span><br><span class="line"><span class="string">    :param top_k:   与testword最相近的k个词 </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    check_point_file = tf.train.latest_checkpoint(<span class="string">r'E:\back_up\NLP\global_variables'</span>)   <span class="comment"># 加载模型</span></span><br><span class="line">    saver = tf.train.import_meta_graph(<span class="string">"&#123;&#125;.meta"</span>.format(check_point_file), clear_devices=<span class="literal">True</span>)</span><br><span class="line">    saver.restore(sess, check_point_file)</span><br><span class="line">    graph = sess.graph    </span><br><span class="line">    graph_test_word_id = graph.get_operation_by_name(<span class="string">"placeholders/test_word_id"</span>).outputs[<span class="number">0</span>]</span><br><span class="line">    graph_similarity = graph.get_operation_by_name(<span class="string">"evaluation/similarity"</span>).outputs[<span class="number">0</span>]</span><br><span class="line">    test_word_id = [word2id.get(x) <span class="keyword">for</span> x <span class="keyword">in</span> test_word]</span><br><span class="line">    feed_dict = &#123;graph_test_word_id: test_word_id&#125;</span><br><span class="line">    similarity = sess.run(graph_similarity, feed_dict)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(test_word)):</span><br><span class="line">        nearest = (-similarity[index, :]).argsort()[<span class="number">0</span>:top_k]     <span class="comment"># argsort()默认按照从小大的顺序  最接近的词</span></span><br><span class="line">        log_info = <span class="string">"Nearest to %s: "</span> % test_word[index]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">            closest_word = [x <span class="keyword">for</span> x, v <span class="keyword">in</span> word2id.items() <span class="keyword">if</span> v == nearest[k]]</span><br><span class="line">            log_info = <span class="string">'%s %s,'</span> % (log_info, closest_word)</span><br><span class="line">        print(log_info)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    batch_size = <span class="number">40</span></span><br><span class="line">    window_len = <span class="number">4</span></span><br><span class="line">    words, word2id, sentences, inputs, labels, vocab_size = get_data(obj_path, windows_len=window_len, lines_number=<span class="number">2000</span>)</span><br><span class="line">    train_data = TrainData(inputs, labels, words, vocab_size, batch_size)</span><br><span class="line">    batch_nums = train_data.get_batch_nums()</span><br><span class="line">    <span class="comment"># print(words)</span></span><br><span class="line">    print(<span class="string">"vocab_size: "</span>, vocab_size)</span><br><span class="line">    print(<span class="string">"batch_nums"</span>, batch_nums)</span><br><span class="line">    model = Model(vocab_size=vocab_size, embedding_size=<span class="number">128</span>, batch_nums=batch_nums, num_sampled=<span class="number">5</span>, learning_rate=<span class="number">0.0001</span>)</span><br><span class="line">    model.train(train_data=train_data, train_steps=<span class="number">150</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    batch_size = <span class="number">200</span></span><br><span class="line">    window_len = <span class="number">4</span></span><br><span class="line">    words, word2id, sentences, inputs, labels, vocab_size = get_data(obj_path, windows_len=window_len,</span><br><span class="line">                                                                     lines_number=<span class="number">2000</span>)</span><br><span class="line">    test_word = []</span><br><span class="line">    <span class="keyword">for</span> count <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        test_word.append(np.random.choice(words))</span><br><span class="line">    print(test_word)</span><br><span class="line">    predict(test_word, word2id)</span><br></pre></td></tr></table></figure></p>
<h2 id="两个方法"><a href="#两个方法" class="headerlink" title="两个方法"></a>两个方法</h2><h3 id="1、Negative-Sampling"><a href="#1、Negative-Sampling" class="headerlink" title="1、Negative Sampling"></a>1、Negative Sampling</h3><p>在每一个训练的时间步，我们不去遍历整个词汇表，而仅仅是抽取一些负样例。考虑一对中心词和上下文词$(w,c)$。这词对是来自训练数据集吗？我们通过$P(D=1\mid w,c)$表示$(w,c)$是来自语料库。相应地，$P(D=0\mid w,c)$表示$(w,c)$不是来自语料库。</p>
<p>现在，我们建立一个新的目标函数，如果中心词和上下文词确实在语料库中，就最大化概率$P(D=1\mid w,c)$，如果中心词和上下文词确实不在语料库中，就最大化概率$P(D=0\mid w,c)$。我们对这两个概率采用一个简单的极大似然估计的方法（这里我们把$\theta$作为模型的参数，在我们的例子是$\mathcal{V}$和$\mathcal{U}$）</p>
<script type="math/tex; mode=display">\begin{aligned} \theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}} P(D=0 | w, c, \theta) \\ &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}}(1-P(D=1 | w, c, \theta)) \\ &=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \widetilde{D}} \log (1-P(D=1 | w, c, \theta)) \\ &=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\ &=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \end{aligned}</script><p>注意到最大化似然函数等同于最小化负对数似然：</p>
<script type="math/tex; mode=display">J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)</script><p>注意$\widetilde{D}$是“假的”或者“负的”语料。例如我们有句子类似“stock boil fish is toy”，这种无意义的句子出现时会得到一个很低的概率。我们可以从语料库中随机抽样出负样例$\widetilde{D}$。</p>
<p>对于 Skip-Gram 模型，我们对给定中心词$c$来观察的上下文单词$c-m+j$的新目标函数为</p>
<script type="math/tex; mode=display">-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)</script><p>对 CBOW 模型，我们对给定上下文向量$\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+…+v_{c+m}}{2m}$来观察中心词$u_{c}$的新的目标函数为</p>
<script type="math/tex; mode=display">-log\,\sigma(u_{c}^{T}\cdot \widehat{v})-\sum_{k=1}^{K}log\,\sigma(-\widetilde{u}_{k}^{T}\cdot \widehat{v})</script><p>在上面的公式中，$\{\widetilde{u}_{k}\mid k=1…K\}$是从$P_{n}(w)$中抽样。有很多关于如何得到最好近似的讨论，从实际效果看来最好的是指数为 ¾ 的 Unigram 模型。那么为什么是 ¾？下面有一些例子对比可能让你有一些直观的了解：</p>
<script type="math/tex; mode=display">\begin{eqnarray} is: 0.9^{3/4} &=& 0.92 \nonumber \\ Constitution: 0.09^{3/4}&=& 0.16 \nonumber \\ bombastic:0.01^{3/4}&=& 0.032 \nonumber \end{eqnarray}</script><p>“Bombastic”当前被抽样的概率是原来的三倍，而“is”只比原来的提高了一点点。</p>
<h3 id="2、Hierarchical-Softmax"><a href="#2、Hierarchical-Softmax" class="headerlink" title="2、Hierarchical Softmax"></a>2、Hierarchical Softmax</h3><p>Mikolov 在论文《Distributed Representations of Words and Phrases and their Compositionality.》中提出了 hierarchical softmax，相比普通的 softmax 这是一种更有效的替代方法。<strong>在实际中，hierarchical softmax 对低频词往往表现得更好，负采样对高频词和较低维度向量表现得更好。</strong></p>
<p>Hierarchical softmax 使用一个二叉树来表示词表中的所有词。树中的每个叶结点都是一个单词，而且只有一条路径从根结点到叶结点。在这个模型中，没有词的输出表示。相反，图的每个节点（根节点和叶结点除外）与模型要学习的向量相关联。单词作为输出单词的概率定义为从根随机游走到单词所对应的叶的概率。计算成本变为$O(log (|V|))$而不是$O(|V|)$。</p>
<p>在这个模型中，给定一个向量$w_{i}$的下的单词$w$的概率$p(w\mid w_{i})$，等于从根结点开始到对应$w$的叶结点结束的随机漫步概率。这个方法最大的优势是计算概率的时间复杂度仅仅是$O(log(|V|))$，对应着路径的长度。</p>
<p>下图是 Hierarchical softmax 的二叉树示意图：</p>
<p><img src="/posts/NLP/10.png" alt></p>
<p>令$L(w)$为从根结点到叶结点$w$的路径中节点数目。例如，上图中的$L(w_{2})$为3。我们定义$n(w,i)$为与向量$v_{n(w,i)}$相关的路径上第$i$个结点。因此$n(w,1)$是根结点，而$n(w,L(w))$是$w$的父节点。现在对每个内部节点$n$，我们任意选取一个它的子节点，定义为$ch(n)$（一般是左节点）。然后，我们可以计算概率为</p>
<script type="math/tex; mode=display">p\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)</script><p>其中</p>
<script type="math/tex; mode=display">[x]=\left\{\begin{array}{ll}{1} & {\text { if } x \text { is true }} \\ {-1} & {\text { otherwise }}\end{array}\right.</script><p>这个公式看起来非常复杂，让我们细细梳理一下。</p>
<p>首先，我们将根据从根节点$(n(w,1))$到叶节点$(w)$的路径的形状来计算相乘的项。如果我们假设$ch(n)$一直都是$n$的左节点，然后当路径往左时$[n(w,j+1)=ch(n(w,j))]$的值返回 1，往右则返回 0。</p>
<p>此外，$[n(w,j+1)=ch(n(w,j))]$提供了归一化的作用。在节点$n$处，如果我们将去往左和右节点的概率相加，对于$v_{n}^{T}v_{w_{i}}$的任何值则可以检查，</p>
<script type="math/tex; mode=display">\sigma\left(v_{n}^{T} v_{w_{i}}\right)+\sigma\left(-v_{n}^{T} v_{w_{i}}\right)=1</script><p>归一化也保证了$\sum_{w=1}^{|V|}P(w\mid w_{i})=1$，和在普通的 softmax 是一样的。</p>
<p>最后我们计算点积来比较输入向量$v_{w_{i}}$对每个内部节点向量$v_{n(w,j)}^{T}$的相似度。下面我们给出一个例子。以上图中的$w_{2}$为例，从根节点要经过两次左边的边和一次右边的边才到达$w_{2}$，因此</p>
<script type="math/tex; mode=display">\begin{aligned} p\left(w_{2} | w_{i}\right) &=p\left(n\left(w_{2}, 1\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 2\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 3\right), \text { right }\right) \\ &=\sigma\left(v_{n\left(w_{2}, 1\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(v_{n\left(w_{2}, 2\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(-v_{n\left(w_{2}, 3\right)}^{T} v_{w_{i}}\right) \end{aligned}</script><p>我们训练模型的目标是最小化负的对数似然$-log\,P(w\mid w_{i})$。不是更新每个词的输出向量，而是更新更新二叉树中从根结点到叶结点的路径上的节点的向量。</p>
<p>该方法的速度由构建二叉树的方式确定，并将词分配给叶节点。Mikolov 在论文《Distributed Representations of Words and Phrases and their Compositionality.》中使用的是哈夫曼树，在树中分配高频词到较短的路径。</p>
<blockquote>
<p>参考文章：<br><a href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/" target="_blank" rel="noopener">https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/</a><br><a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">https://arxiv.org/abs/1310.4546</a></p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>新闻文本分类实战(二)</title>
    <url>/posts/e39af9a7.html</url>
    <content><![CDATA[<p>这里我们接着前面的<a href="https://chenk.tech/posts/eb79fc5f.html" target="_blank" rel="noopener">新闻文本实战的比赛案例</a>，在这里介绍了比赛数据以及实现了机器学习方法及fastText，Word2Vec等方法进行了文本分类，下面我们使用Bert（Transformer框架）对文本实现分类，关于Bert的具体原理在前面的博客都有涉及（<a href="https://chenk.tech/posts/1424e830.html" target="_blank" rel="noopener">传送门1</a>，<a href="https://chenk.tech/posts/aefe1ee4.html" target="_blank" rel="noopener">传送门2</a>），这里就不一一阐述了，下面直接上代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import logging</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level&#x3D;logging.INFO, format&#x3D;&#39;%(asctime)-15s %(levelname)s: %(message)s&#39;)</span><br><span class="line"></span><br><span class="line"># set seed</span><br><span class="line">seed &#x3D; 666</span><br><span class="line">random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.cuda.manual_seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line"></span><br><span class="line"># set cuda</span><br><span class="line">gpu &#x3D; 0</span><br><span class="line">use_cuda &#x3D; gpu &gt;&#x3D; 0 and torch.cuda.is_available()</span><br><span class="line">if use_cuda:</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line">    device &#x3D; torch.device(&quot;cuda&quot;, gpu)</span><br><span class="line">else:</span><br><span class="line">    device &#x3D; torch.device(&quot;cpu&quot;)</span><br><span class="line">logging.info(&quot;Use cuda: %s, gpu id: %d.&quot;, use_cuda, gpu)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-04 20:51:59,686 INFO: Use cuda: False, gpu id: 0.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split data to 10 fold</span></span><br><span class="line">fold_num = <span class="number">10</span></span><br><span class="line">data_file = <span class="string">'train.csv'</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_data2fold</span><span class="params">(fold_num, num=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    fold_data = []</span><br><span class="line">    f = pd.read_csv(data_file, sep=<span class="string">'\t'</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">    texts = f[<span class="string">'text'</span>].tolist()[:num]</span><br><span class="line">    labels = f[<span class="string">'label'</span>].tolist()[:num]</span><br><span class="line"></span><br><span class="line">    total = len(labels)</span><br><span class="line"></span><br><span class="line">    index = list(range(total))</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line"></span><br><span class="line">    all_texts = []</span><br><span class="line">    all_labels = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        all_texts.append(texts[i])</span><br><span class="line">        all_labels.append(labels[i])</span><br><span class="line"></span><br><span class="line">    label2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total):</span><br><span class="line">        label = str(all_labels[i])</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label2id:</span><br><span class="line">            label2id[label] = [i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label2id[label].append(i)</span><br><span class="line"></span><br><span class="line">    all_index = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(fold_num)]</span><br><span class="line">    <span class="keyword">for</span> label, data <span class="keyword">in</span> label2id.items():</span><br><span class="line">        <span class="comment"># print(label, len(data))</span></span><br><span class="line">        batch_size = int(len(data) / fold_num)</span><br><span class="line">        other = len(data) - batch_size * fold_num</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(fold_num):</span><br><span class="line">            cur_batch_size = batch_size + <span class="number">1</span> <span class="keyword">if</span> i &lt; other <span class="keyword">else</span> batch_size</span><br><span class="line">            <span class="comment"># print(cur_batch_size)</span></span><br><span class="line">            batch_data = [data[i * batch_size + b] <span class="keyword">for</span> b <span class="keyword">in</span> range(cur_batch_size)]</span><br><span class="line">            all_index[i].extend(batch_data)</span><br><span class="line"></span><br><span class="line">    batch_size = int(total / fold_num)</span><br><span class="line">    other_texts = []</span><br><span class="line">    other_labels = []</span><br><span class="line">    other_num = <span class="number">0</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> range(fold_num):</span><br><span class="line">        num = len(all_index[fold])</span><br><span class="line">        texts = [all_texts[i] <span class="keyword">for</span> i <span class="keyword">in</span> all_index[fold]]</span><br><span class="line">        labels = [all_labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> all_index[fold]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num &gt; batch_size:</span><br><span class="line">            fold_texts = texts[:batch_size]</span><br><span class="line">            other_texts.extend(texts[batch_size:])</span><br><span class="line">            fold_labels = labels[:batch_size]</span><br><span class="line">            other_labels.extend(labels[batch_size:])</span><br><span class="line">            other_num += num - batch_size</span><br><span class="line">        <span class="keyword">elif</span> num &lt; batch_size:</span><br><span class="line">            end = start + batch_size - num</span><br><span class="line">            fold_texts = texts + other_texts[start: end]</span><br><span class="line">            fold_labels = labels + other_labels[start: end]</span><br><span class="line">            start = end</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fold_texts = texts</span><br><span class="line">            fold_labels = labels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> batch_size == len(fold_labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shuffle</span></span><br><span class="line">        index = list(range(batch_size))</span><br><span class="line">        np.random.shuffle(index)</span><br><span class="line"></span><br><span class="line">        shuffle_fold_texts = []</span><br><span class="line">        shuffle_fold_labels = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">            shuffle_fold_texts.append(fold_texts[i])</span><br><span class="line">            shuffle_fold_labels.append(fold_labels[i])</span><br><span class="line"></span><br><span class="line">        data = &#123;<span class="string">'label'</span>: shuffle_fold_labels, <span class="string">'text'</span>: shuffle_fold_texts&#125;</span><br><span class="line">        fold_data.append(data)</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">"Fold lens %s"</span>, str([len(data[<span class="string">'label'</span>]) <span class="keyword">for</span> data <span class="keyword">in</span> fold_data]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fold_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fold_data = all_data2fold(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-04 20:52:13,069 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build train, dev, test data</span></span><br><span class="line">fold_id = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dev</span></span><br><span class="line">dev_data = fold_data[fold_id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">train_texts = []</span><br><span class="line">train_labels = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, fold_id):</span><br><span class="line">    data = fold_data[i]</span><br><span class="line">    train_texts.extend(data[<span class="string">'text'</span>])</span><br><span class="line">    train_labels.extend(data[<span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line">train_data = &#123;<span class="string">'label'</span>: train_labels, <span class="string">'text'</span>: train_texts&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">test_data_file = <span class="string">'test.csv'</span></span><br><span class="line">f = pd.read_csv(test_data_file, sep=<span class="string">'\t'</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">texts = f[<span class="string">'text'</span>].tolist()</span><br><span class="line">test_data = &#123;<span class="string">'label'</span>: [<span class="number">0</span>] * len(texts), <span class="string">'text'</span>: texts&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build vocab</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BasicTokenizer</span><br><span class="line"></span><br><span class="line">basic_tokenizer = BasicTokenizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_data)</span>:</span></span><br><span class="line">        self.min_count = <span class="number">5</span></span><br><span class="line">        self.pad = <span class="number">0</span></span><br><span class="line">        self.unk = <span class="number">1</span></span><br><span class="line">        self._id2word = [<span class="string">'[PAD]'</span>, <span class="string">'[UNK]'</span>]</span><br><span class="line">        self._id2extword = [<span class="string">'[PAD]'</span>, <span class="string">'[UNK]'</span>]</span><br><span class="line"></span><br><span class="line">        self._id2label = []</span><br><span class="line">        self.target_names = []</span><br><span class="line"></span><br><span class="line">        self.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: dict(zip(x, range(len(x))))</span><br><span class="line">        self._word2id = reverse(self._id2word)</span><br><span class="line">        self._label2id = reverse(self._id2label)</span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">"Build vocab: words %d, labels %d."</span> % (self.word_size, self.label_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.word_counter = Counter()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> data[<span class="string">'text'</span>]:</span><br><span class="line">            words = text.split()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                self.word_counter[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> word, count <span class="keyword">in</span> self.word_counter.most_common():</span><br><span class="line">            <span class="keyword">if</span> count &gt;= self.min_count:</span><br><span class="line">                self._id2word.append(word)</span><br><span class="line"></span><br><span class="line">        label2name = &#123;<span class="number">0</span>: <span class="string">'科技'</span>, <span class="number">1</span>: <span class="string">'股票'</span>, <span class="number">2</span>: <span class="string">'体育'</span>, <span class="number">3</span>: <span class="string">'娱乐'</span>, <span class="number">4</span>: <span class="string">'时政'</span>, <span class="number">5</span>: <span class="string">'社会'</span>, <span class="number">6</span>: <span class="string">'教育'</span>, <span class="number">7</span>: <span class="string">'财经'</span>,</span><br><span class="line">                      <span class="number">8</span>: <span class="string">'家居'</span>, <span class="number">9</span>: <span class="string">'游戏'</span>, <span class="number">10</span>: <span class="string">'房产'</span>, <span class="number">11</span>: <span class="string">'时尚'</span>, <span class="number">12</span>: <span class="string">'彩票'</span>, <span class="number">13</span>: <span class="string">'星座'</span>&#125;</span><br><span class="line"></span><br><span class="line">        self.label_counter = Counter(data[<span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> range(len(self.label_counter)):</span><br><span class="line">            count = self.label_counter[label]</span><br><span class="line">            self._id2label.append(label)</span><br><span class="line">            self.target_names.append(label2name[label])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_pretrained_embs</span><span class="params">(self, embfile)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(embfile, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">            items = lines[<span class="number">0</span>].split()</span><br><span class="line">            word_count, embedding_dim = int(items[<span class="number">0</span>]), int(items[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        index = len(self._id2extword)</span><br><span class="line">        embeddings = np.zeros((word_count + index, embedding_dim))</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">            values = line.split()</span><br><span class="line">            self._id2extword.append(values[<span class="number">0</span>])</span><br><span class="line">            vector = np.array(values[<span class="number">1</span>:], dtype=<span class="string">'float64'</span>)</span><br><span class="line">            embeddings[self.unk] += vector</span><br><span class="line">            embeddings[index] = vector</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        embeddings[self.unk] = embeddings[self.unk] / word_count</span><br><span class="line">        embeddings = embeddings / np.std(embeddings)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: dict(zip(x, range(len(x))))</span><br><span class="line">        self._extword2id = reverse(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(set(self._id2extword)) == len(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2id</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(xs, list):</span><br><span class="line">            <span class="keyword">return</span> [self._word2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._word2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword2id</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(xs, list):</span><br><span class="line">            <span class="keyword">return</span> [self._extword2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._extword2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label2id</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(xs, list):</span><br><span class="line">            <span class="keyword">return</span> [self._label2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._label2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._id2word)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._id2extword)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._id2label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(train_data)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-04 20:52:21,004 INFO: PyTorch version 1.6.0+cpu available.
2020-08-04 20:52:23,281 INFO: Build vocab: words 4337, labels 14.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build module</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))</span><br><span class="line">        self.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        b = np.zeros(hidden_size, dtype=np.float32)</span><br><span class="line">        self.bias.data.copy_(torch.from_numpy(b))</span><br><span class="line"></span><br><span class="line">        self.query = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        self.query.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_hidden, batch_masks)</span>:</span></span><br><span class="line">        <span class="comment"># batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)</span></span><br><span class="line">        <span class="comment"># batch_masks:  b x len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        key = torch.matmul(batch_hidden, self.weight) + self.bias  <span class="comment"># b x len x hidden</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute attention</span></span><br><span class="line">        outputs = torch.matmul(key, self.query)  <span class="comment"># b x len</span></span><br><span class="line"></span><br><span class="line">        masked_outputs = outputs.masked_fill((<span class="number">1</span> - batch_masks).bool(), float(<span class="number">-1e32</span>))</span><br><span class="line"></span><br><span class="line">        attn_scores = F.softmax(masked_outputs, dim=<span class="number">1</span>)  <span class="comment"># b x len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0</span></span><br><span class="line">        masked_attn_scores = attn_scores.masked_fill((<span class="number">1</span> - batch_masks).bool(), <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sum weighted sources</span></span><br><span class="line">        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(<span class="number">1</span>), key).squeeze(<span class="number">1</span>)  <span class="comment"># b x hidden</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs, attn_scores</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build word encoder</span></span><br><span class="line"><span class="comment"># bert_path = 'bert/bert-mini/'</span></span><br><span class="line">dropout = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordBertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(WordBertEncoder, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.tokenizer = WhitespaceTokenizer()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line"></span><br><span class="line">        self.pooled = <span class="literal">False</span></span><br><span class="line">        logging.info(<span class="string">'Build Bert encoder with pooled &#123;&#125;.'</span>.format(self.pooled))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        tokens = self.tokenizer.tokenize(tokens)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bert_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        no_decay = [<span class="string">'bias'</span>, <span class="string">'LayerNorm.weight'</span>]</span><br><span class="line">        optimizer_parameters = [</span><br><span class="line">            &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> self.bert.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)],</span><br><span class="line">             <span class="string">'weight_decay'</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">            &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> self.bert.named_parameters() <span class="keyword">if</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)],</span><br><span class="line">             <span class="string">'weight_decay'</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> optimizer_parameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids, token_type_ids)</span>:</span></span><br><span class="line">        <span class="comment"># input_ids: sen_num x bert_len</span></span><br><span class="line">        <span class="comment"># token_type_ids: sen_num  x bert_len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># sen_num x bert_len x 256, sen_num x 256</span></span><br><span class="line">        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pooled:</span><br><span class="line">            reps = pooled_output</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reps = sequence_output[:, <span class="number">0</span>, :]  <span class="comment"># sen_num x 256</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            reps = self.dropout(reps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reps</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WhitespaceTokenizer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""WhitespaceTokenizer with vocab."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        vocab_file = bert_path + <span class="string">'vocab.txt'</span></span><br><span class="line">        self._token2id = self.load_vocab(vocab_file)</span><br><span class="line">        self._id2token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self._token2id.items()&#125;</span><br><span class="line">        self.max_len = <span class="number">256</span></span><br><span class="line">        self.unk = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">"Build Bert vocab with size %d."</span> % (self.vocab_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_vocab</span><span class="params">(self, vocab_file)</span>:</span></span><br><span class="line">        f = open(vocab_file, <span class="string">'r'</span>)</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        lines = list(map(<span class="keyword">lambda</span> x: x.strip(), lines))</span><br><span class="line">        vocab = dict(zip(lines, range(len(lines))))</span><br><span class="line">        <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(tokens) &lt;= self.max_len - <span class="number">2</span></span><br><span class="line">        tokens = [<span class="string">"[CLS]"</span>] + tokens + [<span class="string">"[SEP]"</span>]</span><br><span class="line">        output_tokens = self.token2id(tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">token2id</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(xs, list):</span><br><span class="line">            <span class="keyword">return</span> [self._token2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._token2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._id2token)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build sent encoder</span></span><br><span class="line">sent_hidden_size = <span class="number">256</span></span><br><span class="line">sent_num_layers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sent_rep_size)</span>:</span></span><br><span class="line">        super(SentEncoder, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.sent_lstm = nn.LSTM(</span><br><span class="line">            input_size=sent_rep_size,</span><br><span class="line">            hidden_size=sent_hidden_size,</span><br><span class="line">            num_layers=sent_num_layers,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sent_reps, sent_masks)</span>:</span></span><br><span class="line">        <span class="comment"># sent_reps:  b x doc_len x sent_rep_size</span></span><br><span class="line">        <span class="comment"># sent_masks: b x doc_len</span></span><br><span class="line"></span><br><span class="line">        sent_hiddens, _ = self.sent_lstm(sent_reps)  <span class="comment"># b x doc_len x hidden*2</span></span><br><span class="line">        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            sent_hiddens = self.dropout(sent_hiddens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sent_hiddens</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.sent_rep_size = <span class="number">256</span></span><br><span class="line">        self.doc_rep_size = sent_hidden_size * <span class="number">2</span></span><br><span class="line">        self.all_parameters = &#123;&#125;</span><br><span class="line">        parameters = []</span><br><span class="line">        self.word_encoder = WordBertEncoder()</span><br><span class="line">        bert_parameters = self.word_encoder.get_bert_parameters()</span><br><span class="line"></span><br><span class="line">        self.sent_encoder = SentEncoder(self.sent_rep_size)</span><br><span class="line">        self.sent_attention = Attention(self.doc_rep_size)</span><br><span class="line">        parameters.extend(list(filter(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_encoder.parameters())))</span><br><span class="line">        parameters.extend(list(filter(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_attention.parameters())))</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=<span class="literal">True</span>)</span><br><span class="line">        parameters.extend(list(filter(<span class="keyword">lambda</span> p: p.requires_grad, self.out.parameters())))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            self.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(parameters) &gt; <span class="number">0</span>:</span><br><span class="line">            self.all_parameters[<span class="string">"basic_parameters"</span>] = parameters</span><br><span class="line">        self.all_parameters[<span class="string">"bert_parameters"</span>] = bert_parameters</span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">'Build model with bert word encoder, lstm sent encoder.'</span>)</span><br><span class="line"></span><br><span class="line">        para_num = sum([np.prod(list(p.size())) <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters()])</span><br><span class="line">        logging.info(<span class="string">'Model param num: %.2f M.'</span> % (para_num / <span class="number">1e6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_inputs)</span>:</span></span><br><span class="line">        <span class="comment"># batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len</span></span><br><span class="line">        <span class="comment"># batch_masks : b x doc_len x sent_len</span></span><br><span class="line">        batch_inputs1, batch_inputs2, batch_masks = batch_inputs</span><br><span class="line">        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[<span class="number">0</span>], batch_inputs1.shape[<span class="number">1</span>], batch_inputs1.shape[<span class="number">2</span>]</span><br><span class="line">        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  <span class="comment"># sen_num x sent_len</span></span><br><span class="line">        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  <span class="comment"># sen_num x sent_len</span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  <span class="comment"># sen_num x sent_len</span></span><br><span class="line"></span><br><span class="line">        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  <span class="comment"># sen_num x sent_rep_size</span></span><br><span class="line"></span><br><span class="line">        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  <span class="comment"># b x doc_len x sent_rep_size</span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  <span class="comment"># b x doc_len x max_sent_len</span></span><br><span class="line">        sent_masks = batch_masks.bool().any(<span class="number">2</span>).float()  <span class="comment"># b x doc_len</span></span><br><span class="line"></span><br><span class="line">        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  <span class="comment"># b x doc_len x doc_rep_size</span></span><br><span class="line">        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  <span class="comment"># b x doc_rep_size</span></span><br><span class="line"></span><br><span class="line">        batch_outputs = self.out(doc_reps)  <span class="comment"># b x num_labels</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_path = <span class="string">'bert/bert-mini/'</span></span><br><span class="line">model = Model(vocab)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-04 21:08:52,810 INFO: Build Bert vocab with size 5981.
2020-08-04 21:08:52,812 INFO: loading configuration file bert/bert-mini/config.json
2020-08-04 21:08:52,812 INFO: Model config BertConfig {
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 256,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 1024,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 256,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 4,
  &quot;num_hidden_layers&quot;: 4,
  &quot;pad_token_id&quot;: 0,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 5981
}

2020-08-04 21:08:52,814 INFO: loading weights file bert/bert-mini/pytorch_model.bin
2020-08-04 21:08:53,114 INFO: All model checkpoint weights were used when initializing BertModel.

2020-08-04 21:08:53,115 INFO: All the weights of BertModel were initialized from the model checkpoint at bert/bert-mini/.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
2020-08-04 21:08:53,117 INFO: Build Bert encoder with pooled False.
2020-08-04 21:08:53,156 INFO: Build model with bert word encoder, lstm sent encoder.
2020-08-04 21:08:53,159 INFO: Model param num: 7.72 M.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build optimizer</span></span><br><span class="line">learning_rate = <span class="number">2e-4</span></span><br><span class="line">bert_lr = <span class="number">5e-5</span></span><br><span class="line">decay = <span class="number">.75</span></span><br><span class="line">decay_step = <span class="number">1000</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_parameters, steps)</span>:</span></span><br><span class="line">        self.all_params = []</span><br><span class="line">        self.optims = []</span><br><span class="line">        self.schedulers = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, parameters <span class="keyword">in</span> model_parameters.items():</span><br><span class="line">            <span class="keyword">if</span> name.startswith(<span class="string">"basic"</span>):</span><br><span class="line">                optim = torch.optim.Adam(parameters, lr=learning_rate)</span><br><span class="line">                self.optims.append(optim)</span><br><span class="line"></span><br><span class="line">                l = <span class="keyword">lambda</span> step: decay ** (step // decay_step)</span><br><span class="line">                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)</span><br><span class="line">                self.schedulers.append(scheduler)</span><br><span class="line">                self.all_params.extend(parameters)</span><br><span class="line">            <span class="keyword">elif</span> name.startswith(<span class="string">"bert"</span>):</span><br><span class="line">                optim_bert = AdamW(parameters, bert_lr, eps=<span class="number">1e-8</span>)</span><br><span class="line">                self.optims.append(optim_bert)</span><br><span class="line"></span><br><span class="line">                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, <span class="number">0</span>, steps)</span><br><span class="line">                self.schedulers.append(scheduler_bert)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> group <span class="keyword">in</span> parameters:</span><br><span class="line">                    <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                        self.all_params.append(p)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                Exception(<span class="string">"no nameed parameters."</span>)</span><br><span class="line"></span><br><span class="line">        self.num = len(self.optims)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> optim, scheduler <span class="keyword">in</span> zip(self.optims, self.schedulers):</span><br><span class="line">            optim.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> optim <span class="keyword">in</span> self.optims:</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self)</span>:</span></span><br><span class="line">        lrs = tuple(map(<span class="keyword">lambda</span> x: x.get_lr()[<span class="number">-1</span>], self.schedulers))</span><br><span class="line">        lr = <span class="string">' %.5f'</span> * self.num</span><br><span class="line">        res = lr % lrs</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_split</span><span class="params">(text, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">16</span>)</span>:</span></span><br><span class="line">    words = text.strip().split()</span><br><span class="line">    document_len = len(words)</span><br><span class="line"></span><br><span class="line">    index = list(range(<span class="number">0</span>, document_len, max_sent_len))</span><br><span class="line">    index.append(document_len)</span><br><span class="line"></span><br><span class="line">    segments = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(index) - <span class="number">1</span>):</span><br><span class="line">        segment = words[index[i]: index[i + <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">assert</span> len(segment) &gt; <span class="number">0</span></span><br><span class="line">        segment = [word <span class="keyword">if</span> word <span class="keyword">in</span> vocab._id2word <span class="keyword">else</span> <span class="string">'&lt;UNK&gt;'</span> <span class="keyword">for</span> word <span class="keyword">in</span> segment]</span><br><span class="line">        segments.append([len(segment), segment])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> len(segments) &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> len(segments) &gt; max_segment:</span><br><span class="line">        segment_ = int(max_segment / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> segments[:segment_] + segments[-segment_:]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> segments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_examples</span><span class="params">(data, word_encoder, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">8</span>)</span>:</span></span><br><span class="line">    label2id = vocab.label2id</span><br><span class="line">    examples = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text, label <span class="keyword">in</span> zip(data[<span class="string">'text'</span>], data[<span class="string">'label'</span>]):</span><br><span class="line">        <span class="comment"># label</span></span><br><span class="line">        id = label2id(label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># words</span></span><br><span class="line">        sents_words = sentence_split(text, vocab, max_sent_len<span class="number">-2</span>, max_segment)</span><br><span class="line">        doc = []</span><br><span class="line">        <span class="keyword">for</span> sent_len, sent_words <span class="keyword">in</span> sents_words:</span><br><span class="line">            token_ids = word_encoder.encode(sent_words)</span><br><span class="line">            sent_len = len(token_ids)</span><br><span class="line">            token_type_ids = [<span class="number">0</span>] * sent_len</span><br><span class="line">            doc.append([sent_len, token_ids, token_type_ids])</span><br><span class="line">        examples.append([id, len(doc), doc])</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">'Total %d docs.'</span> % len(examples))</span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build loader</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_slice</span><span class="params">(data, batch_size)</span>:</span></span><br><span class="line">    batch_num = int(np.ceil(len(data) / float(batch_size)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">        cur_batch_size = batch_size <span class="keyword">if</span> i &lt; batch_num - <span class="number">1</span> <span class="keyword">else</span> len(data) - batch_size * i</span><br><span class="line">        docs = [data[i * batch_size + b] <span class="keyword">for</span> b <span class="keyword">in</span> range(cur_batch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> docs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(data, batch_size, shuffle=True, noise=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    randomly permute data, then sort by source length, and partition into batches</span></span><br><span class="line"><span class="string">    ensure that the length of  sentences in each batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    batched_data = []</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(data)</span><br><span class="line"></span><br><span class="line">        lengths = [example[<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> data]</span><br><span class="line">        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) <span class="keyword">for</span> l <span class="keyword">in</span> lengths]</span><br><span class="line">        sorted_indices = np.argsort(noisy_lengths).tolist()</span><br><span class="line">        sorted_data = [data[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sorted_data =data</span><br><span class="line">        </span><br><span class="line">    batched_data.extend(list(batch_slice(sorted_data, batch_size)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(batched_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> batched_data:</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># some function</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(y_ture, y_pred)</span>:</span></span><br><span class="line">    y_ture = np.array(y_ture)</span><br><span class="line">    y_pred = np.array(y_pred)</span><br><span class="line">    f1 = f1_score(y_ture, y_pred, average=<span class="string">'macro'</span>) * <span class="number">100</span></span><br><span class="line">    p = precision_score(y_ture, y_pred, average=<span class="string">'macro'</span>) * <span class="number">100</span></span><br><span class="line">    r = recall_score(y_ture, y_pred, average=<span class="string">'macro'</span>) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> str((reformat(p, <span class="number">2</span>), reformat(r, <span class="number">2</span>), reformat(f1, <span class="number">2</span>))), reformat(f1, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat</span><span class="params">(num, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> float(format(num, <span class="string">'0.'</span> + str(n) + <span class="string">'f'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build trainer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">clip = <span class="number">5.0</span></span><br><span class="line">epochs = <span class="number">1</span></span><br><span class="line">early_stops = <span class="number">3</span></span><br><span class="line">log_interval = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">test_batch_size = <span class="number">16</span></span><br><span class="line">train_batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">save_model = <span class="string">'./bert.bin'</span></span><br><span class="line">save_test = <span class="string">'./bert.csv'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, vocab)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.report = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        self.train_data = get_examples(train_data, model.word_encoder, vocab)</span><br><span class="line">        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))</span><br><span class="line">        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)</span><br><span class="line">        self.test_data = get_examples(test_data, model.word_encoder, vocab)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># criterion</span></span><br><span class="line">        self.criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label name</span></span><br><span class="line">        self.target_names = vocab.target_names</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># count</span></span><br><span class="line">        self.step = <span class="number">0</span></span><br><span class="line">        self.early_stop = <span class="number">-1</span></span><br><span class="line">        self.best_train_f1, self.best_dev_f1 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        self.last_epoch = epochs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        logging.info(<span class="string">'Start training...'</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">            train_f1 = self._train(epoch)</span><br><span class="line"></span><br><span class="line">            dev_f1 = self._eval(epoch)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.best_dev_f1 &lt;= dev_f1:</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">"Exceed history dev = %.2f, current dev = %.2f"</span> % (self.best_dev_f1, dev_f1))</span><br><span class="line">                torch.save(self.model.state_dict(), save_model)</span><br><span class="line"></span><br><span class="line">                self.best_train_f1 = train_f1</span><br><span class="line">                self.best_dev_f1 = dev_f1</span><br><span class="line">                self.early_stop = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.early_stop += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> self.early_stop == early_stops:</span><br><span class="line">                    logging.info(</span><br><span class="line">                        <span class="string">"Eearly stop in epoch %d, best train: %.2f, dev: %.2f"</span> % (</span><br><span class="line">                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))</span><br><span class="line">                    self.last_epoch = epoch</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.model.load_state_dict(torch.load(save_model))</span><br><span class="line">        self._eval(self.last_epoch + <span class="number">1</span>, test=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        self.model.train()</span><br><span class="line"></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        epoch_start_time = time.time()</span><br><span class="line">        overall_losses = <span class="number">0</span></span><br><span class="line">        losses = <span class="number">0</span></span><br><span class="line">        batch_idx = <span class="number">1</span></span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(self.train_data, train_batch_size, shuffle=<span class="literal">True</span>):</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line">            batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">            batch_outputs = self.model(batch_inputs)</span><br><span class="line">            loss = self.criterion(batch_outputs, batch_labels)</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            loss_value = loss.detach().cpu().item()</span><br><span class="line">            losses += loss_value</span><br><span class="line">            overall_losses += loss_value</span><br><span class="line"></span><br><span class="line">            y_pred.extend(torch.max(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">            y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)</span><br><span class="line">            <span class="keyword">for</span> optimizer, scheduler <span class="keyword">in</span> zip(self.optimizer.optims, self.optimizer.schedulers):</span><br><span class="line">                optimizer.step()</span><br><span class="line">                scheduler.step()</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            self.step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">                elapsed = time.time() - start_time</span><br><span class="line"></span><br><span class="line">                lrs = self.optimizer.get_lr()</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">'| epoch &#123;:3d&#125; | step &#123;:3d&#125; | batch &#123;:3d&#125;/&#123;:3d&#125; | lr&#123;&#125; | loss &#123;:.4f&#125; | s/batch &#123;:.2f&#125;'</span>.format(</span><br><span class="line">                        epoch, self.step, batch_idx, self.batch_num, lrs,</span><br><span class="line">                        losses / log_interval,</span><br><span class="line">                        elapsed / log_interval))</span><br><span class="line"></span><br><span class="line">                losses = <span class="number">0</span></span><br><span class="line">                start_time = time.time()</span><br><span class="line"></span><br><span class="line">            batch_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        overall_losses /= self.batch_num</span><br><span class="line">        during_time = time.time() - epoch_start_time</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reformat</span></span><br><span class="line">        overall_losses = reformat(overall_losses, <span class="number">4</span>)</span><br><span class="line">        score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">        logging.info(</span><br><span class="line">            <span class="string">'| epoch &#123;:3d&#125; | score &#123;&#125; | f1 &#123;&#125; | loss &#123;:.4f&#125; | time &#123;:.2f&#125;'</span>.format(epoch, score, f1,</span><br><span class="line">                                                                                  overall_losses,</span><br><span class="line">                                                                                  during_time))</span><br><span class="line">        <span class="keyword">if</span> set(y_true) == set(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">            report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">            logging.info(<span class="string">'\n'</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_eval</span><span class="params">(self, epoch, test=False)</span>:</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        data = self.test_data <span class="keyword">if</span> test <span class="keyword">else</span> self.dev_data</span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(data, test_batch_size, shuffle=<span class="literal">False</span>):</span><br><span class="line">                torch.cuda.empty_cache()</span><br><span class="line">                batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">                batch_outputs = self.model(batch_inputs)</span><br><span class="line">                y_pred.extend(torch.max(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">                y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">            score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">            during_time = time.time() - start_time</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> test:</span><br><span class="line">                df = pd.DataFrame(&#123;<span class="string">'label'</span>: y_pred&#125;)</span><br><span class="line">                df.to_csv(save_test, index=<span class="literal">False</span>, sep=<span class="string">','</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">'| epoch &#123;:3d&#125; | dev | score &#123;&#125; | f1 &#123;&#125; | time &#123;:.2f&#125;'</span>.format(epoch, score, f1,</span><br><span class="line">                                                                              during_time))</span><br><span class="line">                <span class="keyword">if</span> set(y_true) == set(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">                    report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">                    logging.info(<span class="string">'\n'</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch2tensor</span><span class="params">(self, batch_data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        batch_size = len(batch_data)</span><br><span class="line">        doc_labels = []</span><br><span class="line">        doc_lens = []</span><br><span class="line">        doc_max_sent_len = []</span><br><span class="line">        <span class="keyword">for</span> doc_data <span class="keyword">in</span> batch_data:</span><br><span class="line">            doc_labels.append(doc_data[<span class="number">0</span>])</span><br><span class="line">            doc_lens.append(doc_data[<span class="number">1</span>])</span><br><span class="line">            sent_lens = [sent_data[<span class="number">0</span>] <span class="keyword">for</span> sent_data <span class="keyword">in</span> doc_data[<span class="number">2</span>]]</span><br><span class="line">            max_sent_len = max(sent_lens)</span><br><span class="line">            doc_max_sent_len.append(max_sent_len)</span><br><span class="line"></span><br><span class="line">        max_doc_len = max(doc_lens)</span><br><span class="line">        max_sent_len = max(doc_max_sent_len)</span><br><span class="line"></span><br><span class="line">        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)</span><br><span class="line">        batch_labels = torch.LongTensor(doc_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="keyword">for</span> sent_idx <span class="keyword">in</span> range(doc_lens[b]):</span><br><span class="line">                sent_data = batch_data[b][<span class="number">2</span>][sent_idx]</span><br><span class="line">                <span class="keyword">for</span> word_idx <span class="keyword">in</span> range(sent_data[<span class="number">0</span>]):</span><br><span class="line">                    batch_inputs1[b, sent_idx, word_idx] = sent_data[<span class="number">1</span>][word_idx]</span><br><span class="line">                    batch_inputs2[b, sent_idx, word_idx] = sent_data[<span class="number">2</span>][word_idx]</span><br><span class="line">                    batch_masks[b, sent_idx, word_idx] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            batch_inputs1 = batch_inputs1.to(device)</span><br><span class="line">            batch_inputs2 = batch_inputs2.to(device)</span><br><span class="line">            batch_masks = batch_masks.to(device)</span><br><span class="line">            batch_labels = batch_labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (batch_inputs1, batch_inputs2, batch_masks), batch_labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line">trainer = Trainer(model, vocab)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-04 21:09:42,141 INFO: Total 9000 docs.
2020-08-04 21:09:46,220 INFO: Total 1000 docs.
2020-08-04 21:13:16,817 INFO: Total 50000 docs.
2020-08-04 21:13:16,818 INFO: Start training...
C:\Users\16402\Anaconda3\envs\torch\lib\site-packages\torch\optim\lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn(&quot;To get the last learning rate computed by the scheduler, &quot;
2020-08-04 21:18:31,349 INFO: | epoch   1 | step  50 | batch  50/563 | lr 0.00020 0.00005 | loss 2.0842 | s/batch 6.29
2020-08-04 21:23:30,647 INFO: | epoch   1 | step 100 | batch 100/563 | lr 0.00020 0.00004 | loss 1.3136 | s/batch 5.99
2020-08-04 21:28:09,804 INFO: | epoch   1 | step 150 | batch 150/563 | lr 0.00020 0.00004 | loss 0.8795 | s/batch 5.58
2020-08-04 21:32:29,511 INFO: | epoch   1 | step 200 | batch 200/563 | lr 0.00020 0.00003 | loss 0.8083 | s/batch 5.19
2020-08-04 21:37:23,139 INFO: | epoch   1 | step 250 | batch 250/563 | lr 0.00020 0.00003 | loss 0.7032 | s/batch 5.87
2020-08-04 21:41:54,379 INFO: | epoch   1 | step 300 | batch 300/563 | lr 0.00020 0.00002 | loss 0.7255 | s/batch 5.42
2020-08-04 21:46:54,413 INFO: | epoch   1 | step 350 | batch 350/563 | lr 0.00020 0.00002 | loss 0.4691 | s/batch 6.00
2020-08-04 21:51:52,438 INFO: | epoch   1 | step 400 | batch 400/563 | lr 0.00020 0.00001 | loss 0.6759 | s/batch 5.96
2020-08-04 21:56:39,501 INFO: | epoch   1 | step 450 | batch 450/563 | lr 0.00020 0.00001 | loss 0.5045 | s/batch 5.74
2020-08-04 22:01:25,859 INFO: | epoch   1 | step 500 | batch 500/563 | lr 0.00020 0.00001 | loss 0.4234 | s/batch 5.73
2020-08-04 22:06:26,352 INFO: | epoch   1 | step 550 | batch 550/563 | lr 0.00020 0.00000 | loss 0.5096 | s/batch 6.01
2020-08-04 22:07:30,164 INFO: | epoch   1 | score (70.97, 58.94, 62.61) | f1 62.61 | loss 0.8152 | time 3253.28
2020-08-04 22:07:30,209 INFO: 
              precision    recall  f1-score   support

          科技     0.7663    0.8156    0.7902      1697
          股票     0.7096    0.8917    0.7903      1680
          体育     0.8803    0.9210    0.9002      1405
          娱乐     0.7736    0.8198    0.7960       971
          时政     0.7661    0.7056    0.7346       710
          社会     0.7122    0.7097    0.7110       558
          教育     0.8329    0.7231    0.7741       455
          财经     0.6778    0.3177    0.4326       384
          家居     0.6568    0.5936    0.6236       374
          游戏     0.7650    0.5018    0.6061       279
          房产     0.6987    0.5000    0.5829       218
          时尚     0.6506    0.3649    0.4675       148
          彩票     0.8788    0.3625    0.5133        80
          星座     0.1667    0.0244    0.0426        41

    accuracy                         0.7639      9000
   macro avg     0.7097    0.5894    0.6261      9000
weighted avg     0.7607    0.7639    0.7538      9000

C:\Users\16402\Anaconda3\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
2020-08-04 22:11:06,013 INFO: | epoch   1 | dev | score (77.81, 73.77, 74.78) | f1 74.78 | time 215.80
2020-08-04 22:11:06,013 INFO: Exceed history dev = 0.00, current dev = 74.78
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">trainer.test()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>贪心NLP刷课笔记（二）</title>
    <url>/posts/ecee2a21.html</url>
    <content><![CDATA[<h1 id="Spell-Correction实现"><a href="#Spell-Correction实现" class="headerlink" title="Spell-Correction实现"></a>Spell-Correction实现</h1><p>我们的目标函数是$\text{argmax}_{c\in \text{Candidates}}P(c)P(s|c)$，因此我们第一步需要找出Candidates，也就是围绕原始单词生成编辑距离为1的错误单词，然后我们需要再做一层过滤，过滤掉词典库过滤掉不存在于词典库中的单词，最后我们通过Noisy Channel Model选择最好的Candidates。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>NLP任务分类</title>
    <url>/posts/82d9aeed.html</url>
    <content><![CDATA[<p>本文系综合NLP领域的最新发展成果，并应对NLP处理时遇到的各类状况。</p>
<h1 id="词干提取"><a href="#词干提取" class="headerlink" title="词干提取"></a>词干提取</h1><p>词干提取是将词语去除变化或衍生形式，转换为词干或原型形式的过程。词干提取的目标是将相关词语还原为同样的词干，哪怕词干并非词典的词目。例如，英文中:</p>
<blockquote>
<p>1.beautiful和beautifully的词干同为beauti</p>
<p>2.Good,better和best 的词干分别为good,better和best。</p>
</blockquote>
<ul>
<li><p>相关论文：Martin Porter的<a href="https://tartarus.org/martin/PorterStemmer/def.txt" target="_blank" rel="noopener">波特词干算法</a></p>
</li>
<li><p>相关算法：在Python上可以使用Porter2词干算法</p>
</li>
</ul>
<p>需要注意到英语词汇由两部分构成，词干和词缀，词缀又分前缀和后缀，这里的词干提取<strong>仅指去除后缀</strong>的操作。这里给出了在<a href="https://bitbucket.org/mchaput/stemming/src/5c242aa592a6d4f0e9a0b2e1afdca4fd757b8e8a/stemming/porter2.py?at=default&amp;fileviewer=file-view-default" target="_blank" rel="noopener">python的stemming库中使用</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install stemming</span><br><span class="line"><span class="keyword">from</span> stemming.porter2 <span class="keyword">import</span> stem</span><br><span class="line">stem(<span class="string">"casually"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="词形还原"><a href="#词形还原" class="headerlink" title="词形还原"></a>词形还原</h1><p>词形还原是将一组词语还原为词源或词典的词目形式的过程。还原过程考虑到了POS问题，即词语在句中的语义，词语对相邻语句的语义等。例如，英语中：</p>
<blockquote>
<p>1.beautiful和beautifully被分别还原为beautiful和beautifully。</p>
<p>2.good, better和best被分别还原为good, good和good</p>
</blockquote>
<ul>
<li><p>相关论文1: <a href="http://www.ijrat.org/downloads/icatest2015/ICATEST-2015127.pdf" target="_blank" rel="noopener">这篇文章</a>详细讨论了<strong>词形还原的不同方法</strong>。想要了解传统词形还原的工作原理必读。</p>
</li>
<li><p>相关论文2:  <a href="https://academic.oup.com/dsh/article-abstract/doi/10.1093/llc/fqw034/2669790/Lemmatization-for-variation-rich-languages-using" target="_blank" rel="noopener">这篇论文</a>非常出色，讨论了<strong>运用深度学习对变化丰富的语种做词形还原时会遇到的问题</strong>。</p>
</li>
<li><p>数据集: <a href="https://catalog.ldc.upenn.edu/ldc99t42" target="_blank" rel="noopener">这里</a>是Treebank-3数据集的链接，你可以使用它创建一个自己的词形还原工具。</p>
</li>
</ul>
<p>下面给出了在spacy上的英语词形还原代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install spacy</span><br><span class="line"><span class="comment">#python -m spacy download en</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">"en"</span>)</span><br><span class="line">doc=<span class="string">"good better best"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(doc):</span><br><span class="line">    print(token,token.lemma_)</span><br></pre></td></tr></table></figure>
<h1 id="词向量化"><a href="#词向量化" class="headerlink" title="词向量化"></a>词向量化</h1><p>词向量化是用一组实数构成的向量代表自然语言的叫法。这种技术非常实用，因为电脑无法处理自然语言。词向量化可以捕捉到自然语言和实数间的本质关系。通过词向量化，一个词语或者一段短语可以用一个定维的向量表示，例如向量的长度可以为100，这里维度中的每个数字代表了词语在某个特定方向上的量级。</p>
<ul>
<li>相关博文：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">这篇文章</a>详细解释了词向量化。</li>
<li><p>相关论文：<a href="https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/" target="_blank" rel="noopener">这篇论文</a>解释了词向量化的细节。深入理解词向量化必读。</p>
</li>
<li><p>相关工具：<a href="https://ronxin.github.io/wevi/" target="_blank" rel="noopener">这是</a>个基于浏览器的词向量可视化工具。</p>
</li>
<li><p>预训练词向量：</p>
<ul>
<li><a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" target="_blank" rel="noopener">这里</a>有一份facebook的预训练词向量列表，包含294种语言。</li>
<li><a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" target="_blank" rel="noopener">这里</a>可以下载google news的预训练词向量。</li>
</ul>
</li>
</ul>
<p>下面是具体调用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install gensim</span><br><span class="line">fromgensim.models.keyedvectors <span class="keyword">import</span> KeyedVectors</span><br><span class="line">word_vectors=KeyedVectors.load_word2vec_format(<span class="string">'GoogleNews-vectors-negative300.bin'</span>,binary=<span class="literal">True</span>)</span><br><span class="line">word_vectors[<span class="string">'human'</span>]</span><br></pre></td></tr></table></figure>
<p>这段代码可以用gensim训练你自己的词向量，具体训练往期博客已有阐述，这里是<a href="https://chenk.tech/posts/eb79fc5f.html" target="_blank" rel="noopener">传送门</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence=[[<span class="string">'first'</span>,<span class="string">'sentence'</span>],[<span class="string">'second'</span>,<span class="string">'sentence'</span>]]</span><br><span class="line">model = gensim.models.Word2Vec(sentence, min_count=<span class="number">1</span>,size=<span class="number">300</span>,workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h1><p>词性标注是对句子中的词语标注为名字、动词、形容词、副词等的过程。</p>
<ul>
<li><p>论文1：choi aptly的这篇<a href="https://aclweb.org/anthology/N16-1031.pdf" target="_blank" rel="noopener">《The Last Gist to theState-of-the-Art 》</a>介绍了一种叫动态特征归纳的新方法，这是目前词性标注最先进的方法。</p>
</li>
<li><p>论文2：<a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/837/192" target="_blank" rel="noopener">这篇文章</a>介绍了通过隐马尔科夫模型做无监督词性标注学习的方法。</p>
</li>
</ul>
<p>这段代码可以在spacy上做词性标注：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install spacy</span><br><span class="line"><span class="comment">#!python -m spacy download en</span></span><br><span class="line">nlp=spacy.load(<span class="string">'en'</span>)</span><br><span class="line">sentence=<span class="string">"Ashok killed the snake with a stick"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(sentence):</span><br><span class="line">    print(token,token.pos_)</span><br></pre></td></tr></table></figure>
<h1 id="命名实体消岐"><a href="#命名实体消岐" class="headerlink" title="命名实体消岐"></a>命名实体消岐</h1><p>命名实体消岐是对句子中的提到的实体识别的过程。例如，对句子“Apple earned a revenue of 200 Billion  USD in  2016”，命名实体消岐会推断出句子中的Apple是苹果公司而不是指一种水果。一般来说，命名实体要求有一个实体知识库，能够将句子中提到的实体和知识库联系起来。</p>
<ul>
<li><p>论文1：Huang的<a href="https://arxiv.org/pdf/1504.07678.pdf" target="_blank" rel="noopener">这篇论文</a>运用了基于深度神经网络和知识库的深层语义关联模型，在命名实体消岐上达到了领先水平。</p>
</li>
<li><p>论文2：Ganea and Hofmann的<a href="https://arxiv.org/pdf/1704.04920.pdf" target="_blank" rel="noopener">这篇文章</a>运用了局部神经关注模型和词向量化，没有人为设置特征。</p>
</li>
</ul>
<h1 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h1><p>命名实体识别是识别一个句子中有特定意义的实体并将其区分为人名，机构名，日期，地名，时间等类别的任务。</p>
<ul>
<li>论文：<a href="https://arxiv.org/pdf/1603.01360.pdf" target="_blank" rel="noopener">这篇优秀</a>的论文使用双向LSTM（长短期记忆网络）神经网络结合监督学习和非监督学习方法，在4种语言领域实现了命名实体识别的最新成果。</li>
</ul>
<p>以下是如何使用spacy执行命名实体识别：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">'en'</span>)</span><br><span class="line">sentence=<span class="string">"Ram of Apple Inc. travelled to Sydney on 5th October 2017"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(sentence):</span><br><span class="line">    print(token,  token.ent_type_)</span><br></pre></td></tr></table></figure>
<h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>情感分析是一种广泛的主观分析，它使用自然语言处理技术来识别客户评论的语义情感，语句表达的情绪正负面以及通过语音分析或书面文字判断其表达的情感等等。例如：</p>
<blockquote>
<p>“我不喜欢巧克力冰淇淋” 是对该冰淇淋的负面评价。</p>
<p>“我并不讨厌巧克力冰激凌”—可以被认为是一种中性的评价。</p>
</blockquote>
<p>从使用LSTMs和Word嵌入来计算一个句子中的正负词数开始，有很多方法都可以用来进行情感分析。</p>
<ul>
<li><p>博文1：<a href="https://www.analyticsvidhya.com/blog/2016/02/step-step-guide-building-sentiment-analysis-model-graphlab/" target="_blank" rel="noopener">本文</a>重点对电影推文进行情感分析</p>
</li>
<li><p>博文2：<a href="https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/" target="_blank" rel="noopener">本文</a>重点对印度金奈洪水期间的推文进行情感分析。</p>
</li>
<li><p>论文1：<a href="https://arxiv.org/pdf/1305.6143.pdf" target="_blank" rel="noopener">本文</a>采用朴素贝叶斯的监督学习方法对IMDB评论进行分类。</p>
</li>
<li><p>论文2：<a href="http://www.cs.cmu.edu/~yohanj/research/papers/WSDM11.pdf" target="_blank" rel="noopener">本文</a>利用LDA的无监督学习方法来识别用户生成评论的观点和情感。本文在解决注释评论短缺的问题上表现突出。</p>
</li>
<li><p>资料库：<a href="https://github.com/xiamx/awesome-sentiment-analysis" target="_blank" rel="noopener">这是</a>一个很好的包含相关研究论文和各种语言情感分析程序实现的资料库。</p>
</li>
<li><p>数据集1：<a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/" target="_blank" rel="noopener">多域情感数据集版本2.0</a></p>
</li>
<li><p>数据集2：<a href="http://www.sananalytics.com/lab/twitter-sentiment/" target="_blank" rel="noopener">Twitter情感分析数据集</a></p>
</li>
<li><p>竞赛：一个非常好的<a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank" rel="noopener">比赛</a>，你可以检查你的模型在烂番茄电影评论的情感分析任务中的表现。</p>
</li>
</ul>
<h1 id="文本语义相似分析"><a href="#文本语义相似分析" class="headerlink" title="文本语义相似分析"></a>文本语义相似分析</h1><p>语义文本相似度分析是对两段文本的意义和本质之间的相似度进行分析的过程。注意，相似性与相关性是不同的。例如：汽车和公共汽车是相似的，但是汽车和燃料是相关的。</p>
<ul>
<li><p>论文1：<a href="https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf" target="_blank" rel="noopener">本文</a>详细介绍了文本相似度测量的不同方法。是一篇可以一站式了解目前所有方法的必读文章。</p>
</li>
<li><p>论文2：<a href="http://casa.disi.unitn.it/~moschitt/since2013/2015_SIGIR_Severyn_LearningRankShort.pdf" target="_blank" rel="noopener">本文</a>介绍了用CNN神经网络去比对两个短文本。</p>
</li>
<li><p>论文3：<a href="https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf" target="_blank" rel="noopener">本文</a>利用Tree-LSTMs方法得到了文本的语义相关和语义分类的最新成果。</p>
</li>
</ul>
<h1 id="语种辨识"><a href="#语种辨识" class="headerlink" title="语种辨识"></a>语种辨识</h1><p>什么是语言识别（语种辨识）？语言识别指的是将不同语言的文本区分出来。其利用语言的统计和语法属性来执行此任务。语言识别也可以被认为是文本分类的特殊情况。</p>
<ul>
<li><p>博文：在<a href="https://fasttext.cc/blog/2017/10/02/blog-post.html" target="_blank" rel="noopener">这篇</a>由fastText撰写的博文中介绍了一种新的工具，其可以在1MB的内存使用情况下识别170种语言。</p>
</li>
<li><p>论文1：<a href="http://www.ep.liu.se/ecp/131/021/ecp17131021.pdf" target="_blank" rel="noopener">本文</a>讨论了285种语言的7种语言识别方法。</p>
</li>
<li><p>论文2：<a href="https://repositorio.uam.es/bitstream/handle/10486/666848/automatic_lopez-moreno_ICASSP_2014_ps.pdf?sequence=1" target="_blank" rel="noopener">本文</a>描述了如何使用深度神经网络来实现自动语言识别的最新成果。</p>
</li>
</ul>
<h1 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h1><p>文本摘要是通过识别文本的重点并使用这些要点创建摘要来缩短文本的过程。文本摘要的目的是在不改变文本含义的前提下最大限度地缩短文本。</p>
<ul>
<li>论文1：<a href="https://arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="noopener">本文</a>描述了基于神经注意模型的抽象语句梗概方法。</li>
<li>论文2：<a href="https://arxiv.org/pdf/1602.06023.pdf" target="_blank" rel="noopener">本文</a>描述了使用序列到序列的RNN在文本摘要中达到的最新结果。</li>
<li>资料库：Google  Brain团队的<a href="https://github.com/tensorflow/models/tree/master/research/textsum" target="_blank" rel="noopener">这个资料库</a>拥有使用为文本摘要定制的序列到序列模型的代码。该模型在Gigaword数据集上进行训练。</li>
<li>应用程序：<a href="https://www.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/" target="_blank" rel="noopener">Reddit的autotldr机器人</a>使用文本摘要来梗概从文章到帖子的各种评论。这个功能在Reddit用户中非常有名。</li>
</ul>
<p>程序实现：以下是如何用gensim包快速实现文本摘要。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.summarization <span class="keyword">import</span> summarize</span><br><span class="line">sentence=<span class="string">"Automatic  summarization is the process of shortening a text document with  software, in order to create a summary with the major points of the  original document. Technologies that can make a coherent summary take  into account variables such as length, writing style and  syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data  which contains the information of the entire set. Such techniques are  widely used in industry today. Search engines are an example; others  include summarization of documents, image collections and videos.  Document summarization tries to create a representative summary or  abstract of the entire document, by finding the most informative  sentences, while in image summarization the system finds the most  representative and important (i.e. salient) images. For surveillance  videos, one might want to extract the important events from the  uneventful context.There are two general approaches to automatic  summarization: extraction and abstraction. Extractive methods work by  selecting a subset of existing words, phrases, or sentences in the  original text to form the summary. In contrast, abstractive methods  build an internal semantic representation and then use natural language  generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations.  Research to date has focused primarily on extractive methods, which are  appropriate for image collection summarization and video  summarization."</span></span><br><span class="line">summarize(sentence)</span><br></pre></td></tr></table></figure>
<p>参考链接：</p>
<ul>
<li><a href="https://www.sohu.com/a/203314063_308467" target="_blank" rel="noopener">https://www.sohu.com/a/203314063_308467</a> （本文实际上系转载该文章，由于排版问题不便日后翻阅特整理于此以便查阅）</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>贪心NLP刷课笔记（一）</title>
    <url>/posts/dcf8c6f9.html</url>
    <content><![CDATA[<h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><h2 id="自然语言处理技术"><a href="#自然语言处理技术" class="headerlink" title="自然语言处理技术"></a>自然语言处理技术</h2><h3 id="四个维度"><a href="#四个维度" class="headerlink" title="四个维度"></a>四个维度</h3><ul>
<li>Semantic 语义</li>
<li>Syntax 句子结构</li>
<li>Morphology 单词</li>
<li>Phonetics 声音</li>
</ul>
<p>上面四个维度是自订而底的，核心为前三类：Morphology关注的是分词，pos（词性）和NER等基础技术，我们要做到顶层的可用性，其底层至少需要达到某个阈值；Syntax关注的是句法分析（提取特征；有一个算法为CYK，是一个根据动态规划的算法）、依存分析（Dependence，分析每个词之间有什么关系，可以分析单词之间潜在的联系，可作为特征）；语义分析为最终的目标，是最上层的生态。</p>
<h3 id="基本问题"><a href="#基本问题" class="headerlink" title="基本问题"></a>基本问题</h3><p>NLP的问题可以分为三大类：第一类是基本解决的问题：Spam detection, 命名实体识别等为简单的问题，基本可以保证准确率95%以上，第二类是比较难的算法，比如：Coreference resolution, Word sense disambiguation, Parsing, Machine Translation, Information Translation，第三类是很难的且需要花费较多时间的工作，比如文本摘要与对话系统。</p>
<h2 id="4P"><a href="#4P" class="headerlink" title="4P"></a>4P</h2><p>咳这个不是多人运动，这里讲的是<strong>P, NP, NP Hard和NP Complete</strong>，我们一般来说，可以将算法复杂度分为指数级复杂度$O(p^n)$与多项式级复杂度$O(n^p)$，只要是多项式级复杂度，我们都可以将其理解为可解决的问题，而指数级复杂度我们一般认为是不可解决的问题（量级很小仍可以解决），但我们可以提出一个近似算法，其复杂度为$O(n^p)$可以得到一个近似解（不能保证得到精确解），我们仍可以认为其是近似可以解决的（当然未来也可以使用量子计算机）。</p>
<p>上面的多项式级的复杂度我们称为P问题，指数级复杂度为NP Hard或者NP Complete问题，其中NP Complete问题是NP Hard问题的子集（这两者较难区分这里不详述），NP的意思是可以在多项式复杂度内能验证的问题。</p>
<h2 id="基于搜索的问答系统"><a href="#基于搜索的问答系统" class="headerlink" title="基于搜索的问答系统"></a>基于搜索的问答系统</h2><p>首先我们有一个知识库（问题答案对），预处理环节我们对用户提出的问题先做分词处理，然后做拼写纠错，然后将所有的语态转化为相同的语态（比如went和going转为go），这是Stemming环节，然后就是停用词过滤以及无用单词的过滤（Word Filtering），最后还可以考虑同义词替换。接着我们可以将用户提出的问题转换为向量表示（boolen vector——&gt;count vector——&gt; tf-idf——&gt;Word2Vector——&gt; Seq2Seq），转换完向量后我们就需要计算相似度（可以使用欧氏距离,Cosine距离或其他距离公式），然后按照相似度距离排序，获得最终结果，排序完之后也可能再进行最后一轮的过滤。如果文本数量太多可以通过建立倒排表降低时间复杂度加速过程。</p>
<h1 id="NLP项目Pipeline"><a href="#NLP项目Pipeline" class="headerlink" title="NLP项目Pipeline"></a>NLP项目Pipeline</h1><p>下图为NLP项目的Pipeline：</p>
<p><img src="/posts/Pic\NLP\23.png" alt></p>
<p>下面介绍的前两个流程涉及的算法较多，后两个算法较少。下面分别进行介绍：</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><h3 id="Word-Segmentation"><a href="#Word-Segmentation" class="headerlink" title="Word Segmentation"></a>Word Segmentation</h3><p>常见的分词工具有Jieba分词，SnowNLP，LTP，HanNLP，目前较常用的是Jieba分词工具，Python调用直接impor jieba即可，可以通过cut语句切分，可以通过add_word函数加入希望其识别的专有名词，此时不会对其做切分。</p>
<h4 id="前向最大匹配（forward-max-matching）"><a href="#前向最大匹配（forward-max-matching）" class="headerlink" title="前向最大匹配（forward-max matching）"></a>前向最大匹配（forward-max matching）</h4><p>首先我们举例：我们经常有意见分歧。我们第一步建立词典：[“我们”，”经常”，”有”，”有意见”，”意见”，”分歧”]，我们匹配方式是将整个句子不断剪掉后面的单词，然后匹配词典中的单词，最终得到的分词结果是：[“我们”，”经常”，”有意见”，”分歧”]，这种匹配法会尽量匹配更多的字符。这是一个贪心的算法。</p>
<p>顺便提一句，算法分为贪心算法和DP算法，其中贪心算法是寻找当前最优的，DP算法近似寻找全局最优，可以考虑全局信息。</p>
<h4 id="后向最大匹配（backward-max-matching）"><a href="#后向最大匹配（backward-max-matching）" class="headerlink" title="后向最大匹配（backward-max matching）"></a>后向最大匹配（backward-max matching）</h4><p>过程与前向最大匹配类似，首先设置max-len=5，先考虑”有意见分歧“，发现不在词典里，我们缩减单词为“意见分歧”，逐步这样炒粉得到”分歧“为词典中的单词，最终这个例子我们得到的结果与前向最大匹配得到的结果一样，当然有时候这两个匹配方式得到的结果不一样（但机率很低）。结合这两个匹配方式就得到了双向匹配，也就是结合前向最大匹配和后向最大匹配 。</p>
<p>上面的分词方法的缺点：</p>
<ol>
<li>无法对词进行细分，当我们遇到细分后单词可能为更好的单词时就不太准确；</li>
<li>考虑的是局部最优而不是全局最优；</li>
<li>效率受到Max-len影响</li>
<li>不能考虑到语义信息</li>
</ol>
<p>最大匹配算法只能看到我们前面说到的单词信息，是最简单的模型，而考虑不到语义和句子结构的信息。但我们考虑到这两个信息之后当然我们的复杂度也会随之提高。因此我们可以先试验简单地分词方法，效果还不错的话我们可以再提升为更复杂的模型（考虑到语义信息）。</p>
<h4 id="Incorporate-Semantic（考虑语义）"><a href="#Incorporate-Semantic（考虑语义）" class="headerlink" title="Incorporate Semantic（考虑语义）"></a>Incorporate Semantic（考虑语义）</h4><p>我们希望有一个工具，能够将句子传入，返回一个语义的概率（不同分词法的概率），然后选取概率高的分法即可。<strong>也就是我们输入句子，根据词典库生成所有可能的分割，在其中选择最好的。</strong></p>
<p>我们有一个工具（Language Model），最简单的算法就是根据条件概率假设（所有词出现概率不相关），将句子概率分解成单词的概率乘积，也就是：</p>
<script type="math/tex; mode=display">P(\text{经常，有，意见，分歧})=P(\text{经常})P(\text{有})P(\text{意见})P(\text{分歧}) v.s. P(\text{经常，有意见，分歧})=P(\text{经常})P(\text{有意见})P(\text{分歧})</script><p>由于上面的计算中每个概率值都很低（在一个语料库中每个单词出现的次数都比较少），我们计算乘积时一般会计算$\text{log}P(\text{经常，有，意见，分歧})$，然后将取log之后的结果累加即可。由于$\text{log}$函数时严格递增的，因此可以保证比较是等同的，有时候我们也会加平滑项，这里先不讲。上面的计算方法是Unigram方法，我们当然还可以考虑两个或更多单词的联合概率分布。</p>
<p>我们前面考虑的是先列出所有可能，然后找到其中最好的（概率最高的），我们可以将这两个步骤合并起来做优化，也就是我们常说的维特比算法（也是DP算法）。首先我们计算出每个分词的负log概率，如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>词典</th>
<th>经常</th>
<th>经</th>
<th>有</th>
<th>有意见</th>
<th>意见</th>
<th>分歧</th>
<th>见</th>
<th>意</th>
<th>见分歧</th>
<th>分</th>
</tr>
</thead>
<tbody>
<tr>
<td>概率</td>
<td>0.1</td>
<td>0.05</td>
<td>0.1</td>
<td>0.1</td>
<td>0.2</td>
<td>0.2</td>
<td>0.05</td>
<td>0.05</td>
<td>0.05</td>
<td>0.1</td>
</tr>
<tr>
<td>-log(x)</td>
<td>2.3</td>
<td>3</td>
<td>2.3</td>
<td>2.3</td>
<td>1.6</td>
<td>1.6</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2.3</td>
</tr>
</tbody>
</table>
</div>
<p>可以转化为下图：</p>
<p><img src="/posts/Pic\NLP\24.png" alt></p>
<p>我们可以<strong>将分词目标转化为最小路径</strong>（路径为-log(x)），我们只需要找到最短的从第一个单词经过所有单词（按次序）直到最后一个单词的路径即可。我们定义$f(n)=\text{从节点1到节点n的最短路径的值}$，这个过程我们完全可以通过递归实现，但我们可以想道斐波那契数列用这种方法来计算的话会很浪费计算资源，我们可以维护一个$f(n)$数组来简化运算。其中$f(1)=0,f(2)=3,f(3)=2.3$等等得到第一个节点到最后一个节点的最短路径值，同时也可以得到其路径（需要反推 ），读者不妨自己推一下，最终我们得到最优路径为1——&gt;3——&gt;6——&gt;8（上面的概率是随意取的不代表真实概率 ）。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sys.setrecursionlimit(<span class="number">9000000</span>)</span><br><span class="line">    <span class="comment">## 请编写word_segment_viterbi函数来实现对输入字符串的分词</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_segment_viterbi</span><span class="params">(input_str)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。</span></span><br><span class="line"><span class="string">    2. 编写维特比算法来寻找最优的PATH</span></span><br><span class="line"><span class="string">    3. 返回分词结果</span></span><br><span class="line"><span class="string">    """</span> </span><br><span class="line">    <span class="keyword">if</span> len(input_str) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一步： 构建图                 </span></span><br><span class="line">    n = len(input_str)</span><br><span class="line">    path = np.zeros([n+<span class="number">1</span>,n+<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> input_str[i] <span class="keyword">in</span> word_prob:</span><br><span class="line">            path[i,i+<span class="number">1</span>] = word_prob[input_str[i]]</span><br><span class="line">        <span class="keyword">elif</span> input_str[i] <span class="keyword">in</span> dic_words:</span><br><span class="line">            path[i,i+<span class="number">1</span>] = <span class="number">0.00001</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> input_str[i:j] <span class="keyword">in</span> word_prob:</span><br><span class="line">                path[i,j] = word_prob[input_str[i:j]]</span><br><span class="line">            <span class="keyword">elif</span> input_str[i:j] <span class="keyword">in</span> dic_words:</span><br><span class="line">                    path[i,i+<span class="number">1</span>] = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二步： 利用维特比算法来找出最好的PATH， 这个PATH是P(sentence)最大或者 -log P(sentence)最小的PATH。</span></span><br><span class="line">    <span class="comment"># hint: 思考为什么不用相乘: p(w1)p(w2)...而是使用negative log sum:  -log(w1)-log(w2)-...  </span></span><br><span class="line">    minPath = np.zeros(n+<span class="number">1</span>)</span><br><span class="line">    score = [sys.maxsize]*(n+<span class="number">1</span>)</span><br><span class="line">    score[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    seg = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">        p = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,n):</span><br><span class="line">            <span class="keyword">if</span> path[j,i] != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> score[i] &gt; -np.log(path[j,i])+score[j]:</span><br><span class="line">                    score[i] = -np.log(path[j,i])+score[j]</span><br><span class="line">                    minPath[i] = j</span><br><span class="line">                     </span><br><span class="line">    <span class="comment"># 第三步： 根据最好的PATH, 返回最好的切分</span></span><br><span class="line"></span><br><span class="line">    i = n</span><br><span class="line">    <span class="keyword">while</span> i &gt; <span class="number">0</span>:</span><br><span class="line">        seg.append(input_str[int(minPath[i]):i])</span><br><span class="line">        i = int(minPath[i])</span><br><span class="line">    seg = seg[::<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> seg</span><br></pre></td></tr></table></figure>
<h3 id="Spell-Correction"><a href="#Spell-Correction" class="headerlink" title="Spell Correction"></a>Spell Correction</h3><p>由于用户输入内容可能错误，我们需要有一个算法可以将错误的输入修改为正确的，这里有一个重要的概念是编辑距离。错误的输入可能是错别字，而有些则可能是输入的词不适合，这里可以使用语言模型来纠正。</p>
<p>编辑距离是两个字符串的距离有多长（要多少个操作才能转换为目标字符串），我们以therr这个错误拼写为例求得以下编辑距离：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>目标单词</th>
<th>there</th>
<th>their</th>
<th>thesis</th>
<th>theirs</th>
<th>the</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>编辑距离</strong></td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>而我们具体要返回哪个词需要根据上下文（结合语义）来判断，而计算编辑距离我们会使用DP算法。</p>
<p>最笨的选择方法就是遍历词典库的所有单词，然后选择其中编辑距离最小的（实际上就是ASM近似串匹配算法），编辑距离的DP算法如下 ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_leven</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    len_str1 = len(str1) + <span class="number">1</span></span><br><span class="line">    len_str2 = len(str2) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 创建矩阵</span></span><br><span class="line">    matrix = [<span class="number">0</span> <span class="keyword">for</span> n <span class="keyword">in</span> range(len_str1 * len_str2)]</span><br><span class="line">    <span class="comment"># 矩阵的第一行</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len_str1):</span><br><span class="line">        matrix[i] = i</span><br><span class="line">    <span class="comment"># 矩阵的第一列</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(matrix), len_str1):</span><br><span class="line">        <span class="keyword">if</span> j % len_str1 == <span class="number">0</span>:</span><br><span class="line">            matrix[j] = j // len_str1</span><br><span class="line">    <span class="comment"># 根据状态转移方程逐步得到编辑距离</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len_str1):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len_str2):</span><br><span class="line">            <span class="keyword">if</span> str1[i - <span class="number">1</span>] == str2[j - <span class="number">1</span>]:</span><br><span class="line">                cost = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cost = <span class="number">1</span></span><br><span class="line">            matrix[j * len_str1 + i] = min(matrix[(j - <span class="number">1</span>) * len_str1 + i] + <span class="number">1</span>,</span><br><span class="line">                                           matrix[j * len_str1 + (i - <span class="number">1</span>)] + <span class="number">1</span>,</span><br><span class="line">                                           matrix[(j - <span class="number">1</span>) * len_str1 + (i - <span class="number">1</span>)] + cost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> matrix[<span class="number">-1</span>]  <span class="comment"># 返回矩阵的最后一个值，也就是编辑距离</span></span><br><span class="line"></span><br><span class="line">str1=<span class="string">'谁是谁的谁的谁'</span></span><br><span class="line">str2=<span class="string">'你爱我们谁的是'</span></span><br><span class="line"></span><br><span class="line">a=normal_leven(str1, str2)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<p>另外一种简洁写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    matrix = [[i + j <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2) + <span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1) + <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(str1) + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(str2) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> str1[i - <span class="number">1</span>] == str2[j - <span class="number">1</span>]:</span><br><span class="line">                d = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d = <span class="number">1</span></span><br><span class="line">            matrix[i][j] = min(matrix[i - <span class="number">1</span>][j] + <span class="number">1</span>, matrix[i][j - <span class="number">1</span>] + <span class="number">1</span>, matrix[i - <span class="number">1</span>][j - <span class="number">1</span>] + d)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> matrix[len(str1)][len(str2)]</span><br></pre></td></tr></table></figure>
<p>a=edit(‘谁是谁的谁的谁’,’你爱我们谁的是’)<br>print(a)</p>
<p>我们可以直接调包：import Levenshtein，使用包中的distance函数计算编辑距离，</p>
<p>实际上我们不必遍历词典选择编辑距离最小的，我们可以反过来，使用用户输入字符串生成编辑距离为1或2的字符串（Replace，Add，Delete），然后过滤后返回结果（不会依赖于词典库大小）。</p>
<p>我们得到了一个备选较小编辑距离字符串集合后，可以通过计算$\hat c=argmax_{c\in condidates}p(c|s)$计算得到最可能成为正确的字符串c。上式可以通过贝叶斯定理稍作转换：</p>
<script type="math/tex; mode=display">\hat c = argmax_{c\in candidates}p(c|s) = argmax_{c\in candidates}\frac{p(s|c)*p(c)}{p(s)}= argmax_{c\in candidates}p(s|c)*p(c)</script><p>其中$p(s|c)$应理解为对于apple这个正确的单词，有多少用户将apple写程appla（或者之类的），此数据我们可以从历史数据统计得到，而第二项$p(c)$是一个Unigram的概率，可以通过直接统计apple这个单词在所有文章中出现的次数计算得到（偏向于选择更常用的单词）。</p>
<h3 id="Stop-Words-Removal"><a href="#Stop-Words-Removal" class="headerlink" title="Stop Words Removal"></a>Stop Words Removal</h3><p>对于NLP的应用，我们通常先把停用词、出现频率低的词汇过滤掉，这其实类似于特征筛选的过程。在英文里，“the“，”an“，”their“这些都可以作为停用词来处理（不绝对），<strong>需要考虑把自己的应用场景</strong>。一般做法是拿别人做好的停用词库然后做一些删改，对于很大文本的项目我们可以将出现次数低于10或20之类的词去掉。</p>
<h3 id="Stemming"><a href="#Stemming" class="headerlink" title="Stemming"></a>Stemming</h3><p>词的标准化操作（主要针对英文）有两种常用技术：Stemming和Lemmazation。Stemming不能保证转换出来的单词是正常的英文单词，需要进一步转换，而Lemmazation则比Stemming更严格，可以保证转换出来的单词是一个出现在词库的单词。目前最广泛使用的Stemming操作时Porter Stemmer，其主要分为四个模块：</p>
<ul>
<li>制定规则，可以将后缀改为我们希望的后缀形式，如sses改为ss，ies改为i，ss改为ss，s直接去掉</li>
<li>修改时态：(v)ing的ing去掉，(v)ed的ed去掉，有些需要特殊处理的，比如sing</li>
<li>for long stems：ational改为ate，izer改为ize，ator改为ate</li>
<li>for longers stems：去掉al，able，ate等</li>
</ul>
<p>此过程需要依赖语言学家的经验，由程序员实现。</p>
<h2 id="句子表示"><a href="#句子表示" class="headerlink" title="句子表示"></a>句子表示</h2><h3 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h3><ul>
<li>boolean：构建词典库通过one-hot表示，向量长度与词典库大小一样。</li>
<li>tf-idf：核心方法论为<strong>单词并不是出现的越多就越重要，并不是出现的越少就越不重要</strong>。计算公式为$tfidf(w)=tf(d,w)*idf(w)$，其中$tf(d,w)$为文档$d$中$w$的词频，$idf(w)=\text{log}\frac{N}{N(w)}$，$N$为语料库中的文档总数，$N(w)$为词语$w$ 出现在多少个文档。 也就是一个单词如果每个文档都出现了，tfidf值会很低，相反一个单词只出现少数几个文档中，在这里个文档中这些个单词的重要性一定是比较高的。<ul>
<li>但无论是Boolean方法还是tf-idf均无法通过欧氏距离体现单词的相似性，计算余弦相似度很多都是0，也无法体现单词相似性</li>
<li>上面两个方法的稀疏性（Sparsity）很高</li>
</ul>
</li>
<li>Word2Vec：分布式表示法，能体现单词语义，稀疏度低</li>
</ul>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>训练时输入为一个很长的字符串，当我们有一篇文章时可以直接将其拼接起来得到一个字符串进行训练，一般需要10亿或100亿个单词的语料库，数据很少时训练出来的结果不准确。训练词向量的主要方法有Skip-Gram，Glove，CBOW，RNN，LSTM，MF，Gaussian Embedding等，训练时最重要的参数为希望训练得到词向量的维度。</p>
<p>实际上我们不需要自己训练词向量，可以直接调用。但有些垂直领域需要自己训练词向量，比如金融领域和医疗领域等等，需要根据特定的语料库进行训练。</p>
<p>词向量某种程度上代表了单词的意思，那么当我们训练得到了一个词向量，对于一个句子我们该如何得到他的意思呢？这里主要有两种方法：</p>
<ul>
<li>平均法</li>
<li>LSTM/RNN</li>
</ul>
<h2 id="加速方式（倒排表）"><a href="#加速方式（倒排表）" class="headerlink" title="加速方式（倒排表）"></a>加速方式（倒排表）</h2><p>用户输入了一个问题，我们首先要匹配到知识库中的问题集，计算其相似度，返回其中最匹配的问题，其时间复杂度为$O(n)$，计算量很大。我们先讨论解决这个问题的思路——”层次过滤思想“</p>
<p>我们不需要计算所有问题集与用户输入的问题的余弦相似度，可以通过简单的方法层层过滤，最终得到较低数量级的备选问题得到较少的新备选问题集。我们需要保证：复杂度（过滤器1）&lt;复杂度（过滤器2）&lt;…….&lt;复杂度（过滤器N）。过滤的方式我们通常会使用倒排表来做。</p>
<p>比如我们文档中有如下关键词：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文档</th>
<th>Doc1</th>
<th>Doc2</th>
<th>Doc3</th>
<th>Doc4</th>
</tr>
</thead>
<tbody>
<tr>
<td>关键词</td>
<td>我们，今天，运动</td>
<td>我们，昨天，运动</td>
<td>你们，上，课</td>
<td>你们上什么课</td>
</tr>
</tbody>
</table>
</div>
<p>我们可以构建以下词典：[我们，今天，运动，昨天，上，课，什么]，然后给每个词标注所在的文档：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>单词</th>
<th>文档</th>
</tr>
</thead>
<tbody>
<tr>
<td>我们</td>
<td>Doc1，Doc2</td>
</tr>
<tr>
<td>今天</td>
<td>Doc1</td>
</tr>
<tr>
<td>运动</td>
<td>Doc1，Doc2</td>
</tr>
<tr>
<td>昨天</td>
<td>Doc2</td>
</tr>
<tr>
<td>上</td>
<td>Doc3，Doc4</td>
</tr>
<tr>
<td>课</td>
<td>Doc3，Doc4</td>
</tr>
<tr>
<td>什么</td>
<td>Doc4</td>
</tr>
</tbody>
</table>
</div>
<p>当我们输入我们”运动“，就可以直接返回Doc1与Doc2，我们输入“我们上课”时，由于两个单词没有交集，我们就可以采用并集操作得出结果。我们的过滤器就可以由倒排表的一系列规则组合而成，比如最低层的倒排表过滤结果可以是包含问句中所有单词中任意一个的返回结果，而层数越高则需要的规则也就越苛刻，比如需要包含更多的单词。</p>
<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><h2 id="Noisy-Channel-Model"><a href="#Noisy-Channel-Model" class="headerlink" title="Noisy Channel Model"></a>Noisy Channel Model</h2><script type="math/tex; mode=display">\text{p(text|source)}\text{p(source|text)~p(text)}</script><p>上面这个公式只需要简单地应用贝叶斯公式就可以得到，这个公式看起来简单，应用场景却很多，语音识别、机器翻译、拼写纠错、OCR、密码破解等领域都用到了这个公式。其共同点都是要将一段信号转换为一个文本。</p>
<p><strong>机器翻译</strong>：$\text{P(中文|英文)}\text{~}\text{P(英文|中文)}*\text{P(中文)}$，其中$\text{P(英文|中文)}$为Translation模型，$P(\text{中文})$为语言模型</p>
<p><strong>拼写纠错</strong>：$\text{P(正确的写法|错误的写法)}\text{~}\text{P(错误的写法|正确的写法)}*\text{P(正确的写法)}$</p>
<p><strong>语音识别</strong>：输入为波形，希望将波形转为文本。$\text{P(文本|语音信息)}\text{~}\text{P(语音信息|文本)}*\text{P(文本)}$</p>
<p><strong>密码破解</strong>：输入encrypted string，转为可读的字符串。$\text{P(明文|暗文)}\text{~}\text{P(暗文|明文)}*\text{P(明文)}$</p>
<p>语言模型的作用就是保证转化后的信号是符合常理的，将<strong>判断一句话从语法上是否通顺</strong>（是否是人话）。</p>
<h2 id="Markov-Assumption"><a href="#Markov-Assumption" class="headerlink" title="Markov Assumption"></a>Markov Assumption</h2><p>我们在计算一个语言模型的时候，首先考虑到的就是使用整个句子除了最后一个单词的所有单词来预测这个句子发生的概率，举个例子我们要预测今天是春节，我们都休息这句话的概率，那么我们会计算$P(休息|今天，是，春节，我们，都)$，我们可以想到在一个语料库中出现这样句子的概率肯定是极低的，因此我们会使用到Markov假设， 将上面这个条件概率简化为$P(休息|都)$这里使用的是1st order markov assumption，我们当然也可以使用2rd order……，根据自己的需要调整即可，但需要注意到的是随着加入单词的长度越多概率会越低。再举个例子：</p>
<p>比较今天是周日与今天周日是这两个句子的概率：</p>
<script type="math/tex; mode=display">P_{LM}(今天是周日)=P(今天)P(是|今天)P(周日|是)</script><script type="math/tex; mode=display">P_{LM}(今天周日是)=P(今天)P(周日|今天)P(是|周日)</script><p>我们使用不同的order，就可以得到N-gram的语言模型，其中Unigram就是不考虑前后文信息，无法分析单词依赖性，我们最常用的还是Bigram的语言模型。</p>
<h2 id="平滑项"><a href="#平滑项" class="headerlink" title="平滑项"></a>平滑项</h2><p>往往我们的概率值某一项是0，此时会导致整个句子的概率为0，为了避免这种情况发生，我们需要在每次计算概率的时候加一个平滑项，一般来说，语言模型的平滑处理可分为以下三类：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Discounting（折扣）：通过给概率不为0的项打折扣，来提高概率为0的项的概率；</span><br><span class="line">Interpolation（插值）：在使用N-gram模型计算某一项的概率时，同时结合低阶的模型所计算出的概率；</span><br><span class="line">Back‐off：approximate counts of unobserved N‐gram based on the proportion of back‐off events (e.g., N‐1 gram)。</span><br></pre></td></tr></table></figure>
<h4 id="Discounting"><a href="#Discounting" class="headerlink" title="Discounting"></a>Discounting</h4><p><em>Discounting 包括 Add‐One Smoothing、Add‐K Smoothing、Good-Turing Smoothing等。</em></p>
<h5 id="Add‐One-Smoothing"><a href="#Add‐One-Smoothing" class="headerlink" title="Add‐One Smoothing"></a>Add‐One Smoothing</h5><p>假设N为语料中的单词个数，V为词典中单词的个数，那么对于Unigram和Bigram模型，平滑处理后计算每一项概率的公式为：</p>
<ul>
<li>Unigram case：$P_{L}(w_i) = \frac{count(w_i)+1}{N+V}$</li>
<li>Bigram case：$P_L(w_i|w_{i-1})=\frac{count(w_iw_{i-1})+1}{count(w_{i-1})+V}$</li>
</ul>
<p>举例来说，假设使用Bigram模型，$V=20，count(我们)=3，count(我们,是)=0$。若不使用平滑处理，则$P(是|我们)=0$；若使用上述处理，则$P(是|我们)=(0+1)/(3+20)=1/23$。</p>
<h5 id="Add‐K-Smoothing-Laplace-Smoothing"><a href="#Add‐K-Smoothing-Laplace-Smoothing" class="headerlink" title="Add‐K Smoothing(Laplace Smoothing)"></a>Add‐K Smoothing(Laplace Smoothing)</h5><p>对于这种方式，有$P_L(w_i|w_{i-1})=\frac{count(w_iw_{i-1})+K}{count(w_{i-1})+KV}$。可以看出，Add‐One Smoothing就是$K=1$的情况。<br>在使用Add‐K Smoothing的时候，可以使用优化的方法来寻找最佳的$K$。具体来说，先计算文本的perplexity，由于perplexity是关于$K$的函数，因此通过优化得到最小的perplexity时也同时得到了最佳的$K$。</p>
<h5 id="Good-Turing"><a href="#Good-Turing" class="headerlink" title="Good-Turing"></a>Good-Turing</h5><p>Good-Turing技术是在1953年由古德（I.J.Good）引用图灵（Turing）的方法而提出来的，其基本思想是<strong>用观察计数较高的N元语法数重新估计概率量的大小，并把它指派给那些具有零计数或者较低计数的N元语法</strong>，具体使用公式如下：</p>
<script type="math/tex; mode=display">c^∗=(c+1)\frac{N_{c+1}}{N_c}</script><p>其中，$c$代表某个单词出现的频数，$N_c$代表出现$c$次的单词的个数，而$c^∗$是频数为$c$的单词经过Good-Turing处理后的新的频数。例如，在某个语料库中，单词“love”出现了20次，而出现20次的单词共有100个，经过处理后，这100个出现过20次单词的频数可能变成18.2。</p>
<h4 id="Interpolation："><a href="#Interpolation：" class="headerlink" title="Interpolation："></a>Interpolation：</h4><p>Interpolation 包括 Linear Interpolation等。这种方式的思路为：在使用N-gram模型计算某一项的概率时，同时结合低阶的模型所计算出的概率。 以Trigram模型来说，使用interpolation方式后，计算每一项的概率公式为：</p>
<script type="math/tex; mode=display">\hat P(w_i|w_{i-1},w_{i-2})=\lambda_1P_{ML}(w_i|w_{i-1},w_{i-2})+\lambda_2P_{ML}(w_i|w_{i-1})+\lambda_3P_{ML}(w_i)</script><p>其中$\lambda_1+\lambda_2+\lambda_3=1$，其核心思想是：<strong>现在没有出现不代表未来不会出现</strong>。</p>
<h2 id="评估语言模型"><a href="#评估语言模型" class="headerlink" title="评估语言模型"></a>评估语言模型</h2><p>评估语言模型的核心思路是做填空题，我们给定第一个单词让语言模型预测下一个单词，如此往复可以评估出语言模型的效果如何，其中的评估方法为Perplexity：Perplexity$=2^{-x},x:\text{average log likehood}$，我们训练语言模型的时候自然会希望$x$越小越好，于是Perplexity越小越好，因此我们训练出来的Perplexity应该是不断下降的，最后达到一个均衡值。Perplexity之于我们目前的训练目标就如同召回率精确率之于推荐系统，后面根据不同的任务还有不同的评估方法，但Perplexity是其中很重要的一个评估方法。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>N-gram Order</th>
<th>Unigram</th>
<th>Bigram</th>
<th>Trigram</th>
</tr>
</thead>
<tbody>
<tr>
<td>Perplexity</td>
<td>962</td>
<td>170</td>
<td>109</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>新闻文本分类实战</title>
    <url>/posts/eb79fc5f.html</url>
    <content><![CDATA[<h1 id="赛题理解及思考"><a href="#赛题理解及思考" class="headerlink" title="赛题理解及思考"></a>赛题理解及思考</h1><h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><p>我们直接打开天池大赛找到<a href="https://tianchi.aliyun.com/competition/entrance/531810/information" target="_blank" rel="noopener">零基础入门NLP比赛</a>，看到的是这样一个页面：</p>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/25.png" alt></p>
<p>接着就三步走：注册报名下载数据，查看数据前五行可以看到我们获得的数据如下：</p>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/26.png" alt></p>
<p>其中左边的label是数据集文本对应的标签，而右边的text则是编码后的文本，文本对应的标签列举如下：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&#123;'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13&#125;</span><br></pre></td></tr></table></figure>
<p>根据官方描述：赛题以匿名处理后的新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。</p>
<p>赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。</p>
<p>同时我们还应该注意到官网有给出结果评价指标，我们也需要根据这个评价指标衡量我们的验证集数据误差：</p>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/27.png" alt></p>
<p>既然该拿到的我们都拿到了，我们接下来就开始构思我们都应该使用哪些思路来完成我们的预测。</p>
<h2 id="赛题构思"><a href="#赛题构思" class="headerlink" title="赛题构思"></a>赛题构思</h2><p>由于赛题给出的数据是匿名化的，因此我们无法使用分词等操作提取关键词来简单预测，我们可以使用的是对文本提取特征的分类器或者是深度学习分类器，综合我们有如下思路：</p>
<ul>
<li><p>思路1：TF-IDF + 机器学习分类器：直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。</p>
</li>
<li><p>思路2：FastText：FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。</p>
</li>
<li><p>思路3：WordVec + 深度学习分类器：WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRNN或者BiLSTM。</p>
</li>
<li><p>思路4：Bert词向量：Bert是高配款的词向量，具有强大的建模学习能力。</p>
</li>
</ul>
<p>我们后续将一一实现。</p>
<h2 id="基础数据分析"><a href="#基础数据分析" class="headerlink" title="基础数据分析"></a>基础数据分析</h2><p>虽然这里的数据都是编码的文本数据，我们很难通过简单的数据分析获得很多有价值的信息，但我们还是希望获得诸如文本长度分布，文本类别分布、文本字符分布等信息以获得一个直观的印象，下面我们完成这些操作：</p>
<p>读取数据（由于数据量较大读取前一百行便于操作）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">"train_set.csv"</span>, sep=<span class="string">"\t"</span>, nrows=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<h3 id="统计文章词数"><a href="#统计文章词数" class="headerlink" title="统计文章词数"></a>统计文章词数</h3><p>统计每篇文章的词数并绘制直方图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">train_df[<span class="string">"text_len"</span>] = train_df[<span class="string">"text"</span>].apply(<span class="keyword">lambda</span> x:len(x.split(<span class="string">" "</span>)))</span><br><span class="line">print(train_df[<span class="string">"text_len"</span>].describe())</span><br><span class="line">_ = plt.hist(train_df[<span class="string">'text_len'</span>], bins=<span class="number">200</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Text char count'</span>)</span><br><span class="line">plt.title(<span class="string">"Histogram of char count"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/charCount-1595154665274.jpg" alt="charCount"></p>
<h3 id="统计不同种类新闻数量"><a href="#统计不同种类新闻数量" class="headerlink" title="统计不同种类新闻数量"></a>统计不同种类新闻数量</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">"label"</span>].value_counts().plot(kind=<span class="string">"bar"</span>)</span><br><span class="line">plt.title(<span class="string">'News class count'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"category"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/classCount.jpg" alt></p>
<h3 id="统计单词频率"><a href="#统计单词频率" class="headerlink" title="统计单词频率"></a>统计单词频率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">all_lines = <span class="string">" "</span>.join(list(train_df[<span class="string">"text"</span>]))</span><br><span class="line">word_count = Counter(all_lines.split(<span class="string">" "</span>))</span><br><span class="line">word_count = sorted(word_count.items(), key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">print(len(word_count))</span><br><span class="line">print(word_count[<span class="number">-1</span>])</span><br><span class="line">print(word_count[<span class="number">0</span>])</span><br><span class="line">print(word_count[<span class="number">1</span>])</span><br><span class="line">print(word_count[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>打印结果为编码”3750”，”648”，”900”为出现次数最多的字符且每篇文章中出现率均很高，我们有理由猜测这三个（或其中一两个）字符是标点符号，由此我们可以试着统计文章句子长度(假设这三个字符为标点符号)。</p>
<h3 id="统计文章平均句子长度"><a href="#统计文章平均句子长度" class="headerlink" title="统计文章平均句子长度"></a>统计文章平均句子长度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">sent_count = Counter(re.split(<span class="string">'3750|648|900'</span>, all_lines))</span><br><span class="line">sent_count = sorted(sent_count.items(), key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">sent_count = sent_count[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sent_count)):</span><br><span class="line">    count += sent_count[i][<span class="number">1</span>]</span><br><span class="line">print(<span class="string">"Average amount of sentences per article:"</span>, count/<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>结果显示平均每篇文章有78.5个句子。当然这三个编码未必都是标点符号，猜测应该是一个逗号一个句号一个常见字（估计是“的”），因此每篇文章文章句子数应该更少（可以查看上面代码中的第一个sent count，每个分割句都比我们平时遇见的短很多），具体分析哪个编码是表示这个常见字可以对其他新闻文本数据做分析，看看排名前三的关键字是否匹配，这个特征感兴趣的可以自行研究。</p>
<h3 id="统计每类新闻最常出现的n个单词"><a href="#统计每类新闻最常出现的n个单词" class="headerlink" title="统计每类新闻最常出现的n个单词"></a>统计每类新闻最常出现的n个单词</h3><p>另外我们可以统计每类新闻出现次数最多的字符，只需要将前100个文本中每类新闻的编码文本拼接起来进行如上统计即可，因此我们可以写一个函数用于输出出现次数前n的字符，然后传入对应的文本即可，实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TopWord</span><span class="params">(type_, n)</span>:</span></span><br><span class="line">    all_lines = <span class="string">" "</span>.join([train_df[<span class="string">"text"</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_df[<span class="string">"text"</span>])) <span class="keyword">if</span> train_df[<span class="string">"label"</span>][i]==type_])</span><br><span class="line">    word_count = Counter(all_lines.split(<span class="string">" "</span>))</span><br><span class="line">    word_count = sorted(word_count.items(), key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        print(word_count[i])</span><br><span class="line">        </span><br><span class="line">TopWord(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>其中参数type_的取值范围为[0,13]内的整数，意为对应的标签，参数n为打印出现次数前n的单词对应的编码。当然也可以对其制作不同类别的文本出现次数前n的单词的直方图，由于这里不准备对单词频率等特征做过多分析因此略过不提。</p>
<h1 id="基于机器学习的文本分类"><a href="#基于机器学习的文本分类" class="headerlink" title="基于机器学习的文本分类"></a>基于机器学习的文本分类</h1><p>在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。</p>
<p>但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p>
<h2 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h2><p>词嵌入的最简单的方法就是将每一个字转为一个onehot编码，然后对于句子或者文章直接统计每个字出现的次数（词袋模型），例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">句子1：我 爱 北 京 天 安 门</span><br><span class="line">转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</span><br><span class="line"></span><br><span class="line">句子2：我 喜 欢 上 海</span><br><span class="line">转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>
<p>在sklearn中可以直接<code>CountVectorizer</code>来实现这一步骤：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">'This is the first document.'</span>,</span><br><span class="line">    <span class="string">'This document is the second document.'</span>,</span><br><span class="line">    <span class="string">'And this is the third one.'</span>,</span><br><span class="line">    <span class="string">'Is this the first document?'</span>,</span><br><span class="line">]</span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(corpus).toarray()</span><br></pre></td></tr></table></figure>
<p>CountVectorizer的使用及参数表如下，具体可见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" target="_blank" rel="noopener">官方文档</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CountVectorizer(input=<span class="string">'content'</span>, encoding=<span class="string">'utf-8'</span>,  decode_error=<span class="string">'strict'</span>, strip_accents=<span class="literal">None</span>, lowercase=<span class="literal">True</span>, preprocessor=<span class="literal">None</span>, tokenizer=<span class="literal">None</span>, stop_words=<span class="literal">None</span>, </span><br><span class="line">token_pattern=<span class="string">'(?u)\b\w\w+\b'</span>, ngram_range=(<span class="number">1</span>, <span class="number">1</span>), analyzer=<span class="string">'word'</span>, max_df=<span class="number">1.0</span>, min_df=<span class="number">1</span>, max_features=<span class="literal">None</span>, vocabulary=<span class="literal">None</span>, binary=<span class="literal">False</span>, dtype=&lt;<span class="class"><span class="keyword">class</span> '<span class="title">numpy</span>.<span class="title">int64</span>'&gt;)</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数表</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>input</td>
<td>一般使用默认即可，可以设置为”filename’或’file’</td>
</tr>
<tr>
<td>encodeing</td>
<td>使用默认的utf-8即可，分析器将会以utf-8解码raw document</td>
</tr>
<tr>
<td>decode_error</td>
<td>默认为strict，遇到不能解码的字符将报UnicodeDecodeError错误，设为ignore将会忽略解码错误，还可以设为replace，作用尚不明确</td>
</tr>
<tr>
<td>strip_accents</td>
<td>默认为None，可设为ascii或unicode，将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号</td>
</tr>
<tr>
<td>analyzer</td>
<td>一般使用默认，可设置为string类型，如’word’, ‘char’, ‘char_wb’，还可设置为callable类型，比如函数是一个callable类型</td>
</tr>
<tr>
<td>preprocessor</td>
<td>设为None或callable类型</td>
</tr>
<tr>
<td>tokenizer</td>
<td>设为None或callable类型</td>
</tr>
<tr>
<td>ngram_range</td>
<td>词组切分的长度范围，待详解</td>
</tr>
<tr>
<td>stop_words</td>
<td>设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表</td>
</tr>
<tr>
<td>lowercase</td>
<td>将所有字符变成小写</td>
</tr>
<tr>
<td>token_pattern</td>
<td>过滤规则，表示token的正则表达式，需要设置analyzer == ‘word’，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作token</td>
</tr>
<tr>
<td>max_df</td>
<td>可以设置为范围在[0.0 1.0]的float，也可以设置为没有范围限制的int，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的关键词集的时候，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效</td>
</tr>
<tr>
<td>min_df</td>
<td>类似于max_df，不同之处在于如果某个词的document frequence小于min_df，则这个词不会被当作关键词</td>
</tr>
<tr>
<td>max_features</td>
<td>默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集</td>
</tr>
<tr>
<td>vocabulary</td>
<td>默认为None，自动从输入文档中构建关键词集，也可以是一个字典或可迭代对象</td>
</tr>
<tr>
<td>binary</td>
<td>默认为False，一个关键词在一篇文档中可能出现n次，如果binary=True，非零的n将全部置为1，这对需要布尔值输入的离散概率模型的有用的</td>
</tr>
<tr>
<td>dtype</td>
<td>使用CountVectorizer类的fit_transform()或transform()将得到一个文档词频矩阵，dtype可以设置这个矩阵的数值类型</td>
</tr>
</tbody>
</table>
</div>
<p>硬要总结一下词袋模型的话可总结为三部曲：<strong>分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）</strong>。</p>
<h2 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h2><p>转换的结果正是如上面我们人工转换的那样，但正如你现在想到的，有些字比如“的”，“我”等在每篇文章中出现频率很高，但不能体现更多的文章特征，因此我们会对每篇文章中出现次数较高的单词执行降权操作，想要了解的更具体可以参看我的另一篇<a href="https://chenk.tech/posts/dcf8c6f9.html" target="_blank" rel="noopener">学习笔记</a>，这篇文章提到了比较多NLP领域的基础概念，这里简单将其中关于tf-idf的介绍复制过来：</p>
<blockquote>
<p>tf-idf：核心方法论为<strong>单词并不是出现的越多就越重要，并不是出现的越少就越不重要</strong>。计算公式为$tfidf(w)=tf(d,w)*idf(w)$，其中$tf(d,w)$为文档$d$中$w$的词频，$idf(w)=\text{log}\frac{N}{N(w)}$，$N$为语料库中的文档总数，$N(w)$为词语$w$ 出现在多少个文档。 也就是一个单词如果每个文档都出现了，tfidf值会很低，相反一个单词只出现少数几个文档中，在这里个文档中这些个单词的重要性一定是比较高的。</p>
</blockquote>
<h2 id="比较词袋模型与tf-idf"><a href="#比较词袋模型与tf-idf" class="headerlink" title="比较词袋模型与tf-idf"></a>比较词袋模型与tf-idf</h2><p>接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。</p>
<p>首先是词袋模型，部分代码旁边标有注释</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer						<span class="comment"># 构建词袋模型的库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier								<span class="comment"># 岭回归分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score											<span class="comment"># 导入计算F1值的库</span></span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">'train_set.csv'</span>, sep=<span class="string">'\t'</span>, nrows=<span class="number">15000</span>)					<span class="comment"># 读取前15000行数据</span></span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer(max_features=<span class="number">3000</span>)									<span class="comment"># 取前max_features个作为关键词</span></span><br><span class="line">train_test = vectorizer.fit_transform(train_df[<span class="string">'text'</span>])							<span class="comment"># 转化为one-hot向量</span></span><br><span class="line"></span><br><span class="line">clf = RidgeClassifier()															</span><br><span class="line">clf.fit(train_test[:<span class="number">10000</span>], train_df[<span class="string">'label'</span>].values[:<span class="number">10000</span>])					<span class="comment"># 对前10000个学习，后5000个验证</span></span><br><span class="line"></span><br><span class="line">val_pred = clf.predict(train_test[<span class="number">10000</span>:])										<span class="comment"># 对验证集进行预测</span></span><br><span class="line">print(f1_score(train_df[<span class="string">'label'</span>].values[<span class="number">10000</span>:], val_pred, average=<span class="string">'macro'</span>))</span><br></pre></td></tr></table></figure>
<p>其F1得分为0.7416952793751392。</p>
<p>然后我们看tf-idf，代码与前面几乎一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">'../input/train_set.csv'</span>, sep=<span class="string">'\t'</span>, nrows=<span class="number">15000</span>)</span><br><span class="line"></span><br><span class="line">tfidf = TfidfVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>), max_features=<span class="number">3000</span>)</span><br><span class="line">train_test = tfidf.fit_transform(train_df[<span class="string">'text'</span>])</span><br><span class="line"></span><br><span class="line">clf = RidgeClassifier()</span><br><span class="line">clf.fit(train_test[:<span class="number">10000</span>], train_df[<span class="string">'label'</span>].values[:<span class="number">10000</span>])</span><br><span class="line"></span><br><span class="line">val_pred = clf.predict(train_test[<span class="number">10000</span>:])</span><br><span class="line">print(f1_score(train_df[<span class="string">'label'</span>].values[<span class="number">10000</span>:], val_pred, average=<span class="string">'macro'</span>))</span><br></pre></td></tr></table></figure>
<p>其F1得分为0.8721598830546126，明显优于词袋模型。上面的代码需要注意的是ngram_range=(1,3)表示考虑单个编码单词组成的词语的长度，官方解释为：</p>
<blockquote>
<p><strong>ngram_range</strong>tuple (min_n, max_n), default=(1, 1)</p>
<p>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n will be used. For example an <code>ngram_range</code> of <code>(1, 1)</code> means only unigrams, <code>(1, 2)</code> means unigrams and bigrams, and <code>(2, 2)</code> means only bigrams. Only applies if <code>analyzer is not callable</code>.</p>
</blockquote>
<p>我们还可以尝试其他<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text" target="_blank" rel="noopener">sklearn库中的文本分类</a>方法：</p>
<p><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/sklearn.png" alt></p>
<p>呃好像只剩下一个HashingVectorizer，这正是我们接下来要介绍的。</p>
<h2 id="用哈希技巧向量化大文本向量"><a href="#用哈希技巧向量化大文本向量" class="headerlink" title="用哈希技巧向量化大文本向量"></a>用哈希技巧向量化大文本向量</h2><p>以上的向量化情景很简单，但是，事实上这种方式从字符标记到整型特征的目录（vocabulary_属性）的映射都是在内存中进行，在处理大数据集时会出现一些问题：</p>
<ul>
<li>语料库越大，词表就会越大，因此使用的内存也越大</li>
<li>拟合（fitting）需要根据原始数据集的大小等比例分配中间数据结构的大小，构建词映射需要完整的传递数据集，因此不可能以严格在线的方式拟合文本分类器。</li>
<li>pickling和un-pickling vocabulary很大的向量器会非常慢</li>
<li>将向量化任务分隔成并行的子任务很不容易实现，因为vocabulary属性要共享状态有一个细颗粒度的同步障碍：从标记字符串中映射特征索引与每个标记的首次出现顺序是独立的，因此应该被共享，在这点上并行worker的性能收到了损害，使他们比串行更慢。</li>
</ul>
<p>通过同时使用由sklearn.feature_extraction.FeatureHasher类实施的“哈希技巧”（特征哈希）、文本预处理和CountVectorizer的标记特征有可能克服这些限制。简而言之，Hash Trick可以做特征的降维，具体的做法是：假设哈希函数$h$使第$i$个特征哈希到位置$j$即$h(i)=j$，则第$i$个原始特征的词频数值$\phi(i)$将累加到哈希后的第$j$个特征的词频数值$\bar\phi$上：</p>
<script type="math/tex; mode=display">\bar \phi(j) = \sum_{i \text{ in } \mathbb{J};h(i)=j}\phi(i)</script><p>其中$\mathbb{J}$是原始特征的维度。但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数<em>h</em>,我们多了一个一个哈希函数：</p>
<script type="math/tex; mode=display">\eta:\mathbb{N}\rightarrow \pm1</script><script type="math/tex; mode=display">\bar \phi(j) = \sum_{i \text{ in } \mathbb{J};h(i)=j}\eta(i)\phi(i)</script><p>这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。</p>
<p>当然，大家会有疑惑，这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。如果大家对理论上为何这种方法有效，建议参考论文：<a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank" rel="noopener">Feature hashing for large scale multitask learning</a>.这里就不多说了。</p>
<p>在scikit-learn的HashingVectorizer类中，实现了基于signed hash  trick的算法，这里我们就用HashingVectorizer来实践一下Hash  Trick，我们直接用前面的文本数据做分类看看结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> HashingVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line">hashVec = HashingVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">train_test = hashVec.fit_transform(train_df[<span class="string">'text'</span>])</span><br><span class="line"></span><br><span class="line">clf = RidgeClassifier()</span><br><span class="line">clf.fit(train_test[:<span class="number">10000</span>], train_df[<span class="string">'label'</span>].values[:<span class="number">10000</span>])</span><br><span class="line"></span><br><span class="line">val_pred = clf.predict(train_test[<span class="number">10000</span>:])</span><br><span class="line">print(f1_score(train_df[<span class="string">'label'</span>].values[<span class="number">10000</span>:], val_pred, average=<span class="string">'macro'</span>))</span><br></pre></td></tr></table></figure>
<p>这里没有传入max_features参数，因为我们知道哈希技巧本身就是一个降维的算法，计算出来的F1值为0.8891893925390113，明显优于另外两个算法，经过一些调参之后算法准确度还可以提高一些。</p>
<p>这里我们看到虽然我们只用了简单地机器学习算法，但已经取得了挺不错的分类率了，接下来我们将尝试基于深度学习的分类，事实上我们的比赛也从这里才正式开始！</p>
<h1 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h1><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><h3 id="FastText介绍"><a href="#FastText介绍" class="headerlink" title="FastText介绍"></a>FastText介绍</h3><h4 id="fastText模型架构"><a href="#fastText模型架构" class="headerlink" title="fastText模型架构"></a>fastText模型架构</h4><p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</p>
<ol>
<li>fastText在保持高精度的情况下加快了训练速度和测试速度</li>
<li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li>
<li>fastText两个重要的优化：<strong>Hierarchical Softmax、N-gram</strong></li>
</ol>
<p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同。word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，这里的类别数量$|V|$词库大小。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从$|V|$降低到了树的高度</p>
<p>fastText模型架构:其中$x_1,x_2,…,x_{N−1},x_N$表示一个文本中的n-gram向量，每个特征是词向量的平均值。这和前文中提到的CBOW相似，<strong>CBOW用上下文去预测中心词，而此处用全部的n-gram去预测指定类别</strong>。</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>softmax函数常在神经网络输出层充当激活函数，目的就是将输出层的值归一化到</p>
<p>$0-1$区间，将神经元输出构造成概率分布，主要就是起到将神经元输出值进行归一化的作用。在标准的softmax中，计算一个类别的softmax概率时，我们需要对所有类别概率做归一化，在这类别很大情况下非常耗时，因此提出了分层softmax(Hierarchical Softmax),思想是根据类别的频率构造霍夫曼树来代替标准softmax，通过分层softmax可以将复杂度从$N$降低到$logN$，下图给出分层softmax示例：<br><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/huffman.png"></p>
<p>在层次softmax模型中，叶子结点的词没有直接输出的向量，而非叶子节点都有响应的输在在模型的训练过程中，通过Huffman编码，构造了一颗庞大的Huffman树，同时会给非叶子结点赋予向量。我们要计算的是目标词w的概率，这个概率的具体含义，是指从root结点开始随机走，走到目标词w的概率。因此在途中路过非叶子结点（包括root）时，需要分别知道往左走和往右走的概率。例如到达非叶子节点n的时候往左边走和往右边走的概率分别是：</p>
<script type="math/tex; mode=display">p(n, left) = \delta(\theta_n^T·h)</script><script type="math/tex; mode=display">p(n, right) = 1-\sigma(\theta_n^T·h) = \sigma(-\theta_n^T·h)</script><p>以上图中的目标词$w_2$为例：</p>
<script type="math/tex; mode=display">p(w_2) = p(n(w_2, 1), left)·p(n(w_2, 2), left)·p(n(w_2, 3), right)=\sigma(\theta_{n(w_2,1)}^T·h)·\sigma(\theta_{n(w_2,2)}^T·h)·\sigma(-\theta_{n(w_2,3)}^T·h)</script><p>因此目标词为$w$的概率可以表示为：</p>
<script type="math/tex; mode=display">p(w)=\prod_{j=1}^{L(w)-1}\sigma(sign(w,j)·\theta_{n(w,j)}^Th)</script><p>其中$θ_{n(w,j)}$是非叶子结点$n(w,j)$的向量表示（即输出向量）；$h$是隐藏层的输出值，从输入词的向量中计算得来；$sign(x,j)$是一个特殊函数，定义为：</p>
<script type="math/tex; mode=display">sign(w,j)=\begin{cases}1,\text{  若}n(w,j+1)\text{是}n(w,j)\text{的左孩子}\\-1,\text{  若}n(w,j+1)\text{是}n(w,j)\text{的右孩子} \end{cases}</script><p>此外，所有词的概率和为1，即</p>
<script type="math/tex; mode=display">\sum_{i=1}^np(w_i)=1</script><p>最终得到参数更新公式为：</p>
<script type="math/tex; mode=display">\theta_i^{(new)} = \theta_j^{(old)}-\eta(\sigma(\theta_j^Th)-t_j)h</script><h4 id="N-gram特征"><a href="#N-gram特征" class="headerlink" title="N-gram特征"></a>N-gram特征</h4><p>n-gram是基于语言模型的算法，基本思想是将文本内容按照子节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。而且需要额外注意一点是n-gram可以根据粒度不同有不同的含义，有字粒度的n-gram和词粒度的n-gram，正如前面调参时的2-gram和3-gram。</p>
<p>对于文本句子的n-gram来说，如上面所说可以是字粒度或者是词粒度，同时n-gram也可以在字符级别工作，例如对单个单词matter来说，假设采用3-gram特征，那么matter可以表示成图中五个3-gram特征，这五个特征都有各自的词向量，五个特征的词向量和即为matter这个词的向其中“&lt;”和“&gt;”是作为边界符号被添加，来将一个单词的ngrams与单词本身区分开来：<br><img src="/Pic/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/28.png" alt></p>
<p>从上面来看，使用n-gram有如下优点<br>1、为罕见的单词生成更好的单词向量：根据上面的字符级别的n-gram来说，即是这个单词出现的次数很少，但是组成单词的字符和其他单词有共享的部分，因此这一点可以优化生成的单词向量<br>2、在词汇单词中，即使单词没有出现在训练语料库中，仍然可以从字符级n-gram中构造单词的词向量<br>3、n-gram可以让模型学习到局部单词顺序的部分信息, 如果不考虑n-gram则便是取每个单词，这样无法考虑到词序所包含的信息，即也可理解为上下文信息，因此通过n-gram的方式关联相邻的几个词，这样会让模型在训练的时候保持词序信息</p>
<h4 id="其他解决方案"><a href="#其他解决方案" class="headerlink" title="其他解决方案"></a>其他解决方案</h4><p>但正如上面提到过，随着语料库的增加，内存需求也会不断增加，严重影响模型构建速度，针对这个有以下几种解决方案：<br>1、过滤掉出现次数少的单词<br>2、使用hash存储<br>3、由采用字粒度变化为采用词粒度</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>我们先看看<a href="https://github.com/facebookresearch/fastText/tree/master/python" target="_blank" rel="noopener">gituhub官方文档</a>对FastText的定义 ：</p>
<blockquote>
<p><a href="https://fasttext.cc/" target="_blank" rel="noopener">fastText</a> is a library for efficient learning of word representations and sentence classification.</p>
</blockquote>
<p>我们先使用fastText，随后再对其进行详细解析，第一步我们需要安装 fastText库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip instal fastText</span><br></pre></td></tr></table></figure>
<p>或者手动从github下载：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;fastText.git</span><br><span class="line">$ cd fastText</span><br><span class="line">$ sudo pip install .</span><br><span class="line">$ # or :</span><br><span class="line">$ sudo python setup.py install</span><br></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>我们直接使用fastText来实现我们的文本分类，代码一共只有寥寥几行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install fasttext</span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">"train_set.csv"</span>, sep=<span class="string">"\t"</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">train_df[<span class="string">"label_ft"</span>] = <span class="string">"__label__"</span> + train_df[<span class="string">"label"</span>].astype(str)</span><br><span class="line">train_df[[<span class="string">"text"</span>, <span class="string">"label_ft"</span>]].iloc[:<span class="number">-5000</span>].to_csv(<span class="string">"train.csv"</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">model = fasttext.train_supervised(<span class="string">"train.csv"</span>, lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">25</span>, loss=<span class="string">"hs"</span>)</span><br><span class="line"></span><br><span class="line">val_pred = [model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">"__"</span>)[<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[<span class="number">-5000</span>:][<span class="string">"text"</span>]]</span><br><span class="line">print(f1_score(train_df[<span class="string">"label"</span>].values[<span class="number">-5000</span>:].astype(str), val_pred, average=<span class="string">"macro"</span>))</span><br></pre></td></tr></table></figure>
<p>结果很快就输出出来了（比前面的机器学习模型还快），结果为0.8260812453351833，但人不可貌相，我们可以对这个结果做很多优化，比如增加训练集数量，调整参数等。下面先对上面部分参数做了个简单的循环用于查看粗略的训练效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 粗略调参</span></span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">"train_set.csv"</span>, sep=<span class="string">"\t"</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">train_df[<span class="string">"label_ft"</span>] = <span class="string">"__label__"</span> + train_df[<span class="string">"label"</span>].astype(str)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> val_set <span class="keyword">in</span> [<span class="number">2000</span>, <span class="number">3000</span>, <span class="number">4000</span>, <span class="number">5000</span>]:</span><br><span class="line">    print(<span class="string">"The validation set is:"</span>, val_set)</span><br><span class="line">    train_df[[<span class="string">"text"</span>, <span class="string">"label_ft"</span>]].iloc[:-val_set].to_csv(<span class="string">"train.csv"</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> lr_ <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>):</span><br><span class="line">        print(<span class="string">"Learning rate is:"</span>, lr_/<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">for</span> wordGram <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]:</span><br><span class="line">            print(<span class="string">"wordNgrams is:"</span>, wordGram)</span><br><span class="line">            model = fasttext.train_supervised(<span class="string">"train.csv"</span>, lr=lr_/<span class="number">10</span>, wordNgrams=wordGram, verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">25</span>, loss=<span class="string">"hs"</span>)</span><br><span class="line">            val_pred = [model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">"__"</span>)[<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-val_set:][<span class="string">"text"</span>]]</span><br><span class="line">            print(<span class="string">"f1_score is:"</span>, f1_score(train_df[<span class="string">"label"</span>].values[-val_set:].astype(str), val_pred, average=<span class="string">"macro"</span>))</span><br></pre></td></tr></table></figure>
<p>为什么说这是粗略的呢，因为验证集与测试集的划分比较随意，就取了前一部分作为训练集后一部分作为验证集，没有多次独立重复实验取平均值之类的操作，因此实验结果带有较大的偶然性，但这样粗略的实验有助于我们对整体参数有个大概的估计，我们后面的实验可以在更精细的参数范围内做调参等工作。根据上面的粗调参结果我们可以看到几个显然的结论（以下对参数的讨论均限制在上述实验所取范围）：</p>
<ul>
<li>验证集越小效果越好</li>
<li>学习率越高越好</li>
<li>学习率较高（约0.7以上）时，wordNgrams参数为3时效果较好，学习率较低时wordNgrams参数为2时效果较好</li>
<li>最最重要的是，训练数据全加进来效果当然会好很多</li>
</ul>
<p>由于学习率未达到峰值，我们往1的右边继续调高学习率进行实验，最终我们将参数的取值范围大致定在以下范围内，我们后续将对这个范围内的参数做进一步调参：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数名</th>
<th>learning_rate</th>
<th>wordNgrams</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>取值范围</strong></td>
<td>(1.2, 1.4)</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>在精确的参数估计中，我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致，划分代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">label2id &#x3D; &#123;&#125;</span><br><span class="line">for i in range(total):</span><br><span class="line">    label &#x3D; str(all_labels[i])</span><br><span class="line">    if label not in label2id:</span><br><span class="line">        label2id[label] &#x3D; [i]</span><br><span class="line">    else:</span><br><span class="line">        label2id[label].append(i)</span><br></pre></td></tr></table></figure>
<p>通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。嗯想了想前面的HashingVectorizer，不用累死累活去调参就有0.889的准确率（甭跟我扯训练数据的事，两个方法的训练数据都是10000），真是心累阿，不准备接着调参了反正也不是很高，就大概0.85左右这个样，重点还是后面两个方法。</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="词向量训练"><a href="#词向量训练" class="headerlink" title="词向量训练"></a>词向量训练</h3><p>Word2Vec的理论我们在往期博客中已经有详细介绍了（<a href="https://chenk.tech/posts/fb70fd3e.html" target="_blank" rel="noopener">传送门</a>），这里我们将直接对其进行调用，并介绍整个实验流程：</p>
<p>首先导入必要的包，并过滤警告：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<p>然后保险起见我们再瞅一眼我们的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_all = pd.read_csv(<span class="string">"train.csv"</span>, sep=<span class="string">"\t"</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">data_all.head()</span><br></pre></td></tr></table></figure>
<p>大概长这样，嗯没错：</p>
<p><img src="/Pic/NLP/30.png" alt></p>
<p>我们接下来就要构建我们的词表啦，由于训练时通常是所有文档合并起来一起训练，因此这里就直接将所有的编码词合并起来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(data_all)):</span><br><span class="line">    word_list.append(data_all[<span class="string">"text"</span>][i].strip().split(<span class="string">" "</span>))</span><br></pre></td></tr></table></figure>
<p>下一步就将词表传入模型进行训练啦，当然也可以将训练结果进行保存：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2vec_model = Word2Vec(word_list, size=<span class="number">20</span>, iter=<span class="number">10</span>, min_count=<span class="number">20</span>)</span><br><span class="line">word2vec_model.save(<span class="string">'word2vec_model.w2v'</span>)</span><br></pre></td></tr></table></figure>
<p>由于我们是要对文章进行分类，而不是对某个词分类，我们最简单的表示文章的方法就是将文章中所有词向量取平均输出（后面会介绍其他方法）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getVector</span><span class="params">(text, word2vec_model)</span>:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    article_vector = np.zeros( word2vec_model.layer1_size )</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> text:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> word2vec_model:</span><br><span class="line">            article_vector += word2vec_model[word]</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> article_vector / count</span><br><span class="line"></span><br><span class="line">startTime = time.time()</span><br><span class="line">vector_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(data_all)):</span><br><span class="line">    text = data_all[<span class="string">"text"</span>][i].strip().split(<span class="string">" "</span>)</span><br><span class="line">    vector_list.append(getVector(text, word2vec_model))</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"前%d个文本生成词向量花费总时间%.2f秒"</span> %(i, time.time()-startTime))</span><br></pre></td></tr></table></figure>
<p>可以看到生成的结果还是蛮快的：</p>
<p><img src="/Pic/NLP/31.png" alt></p>
<p>于是我们的文本向量也就构建完成了，接下来我们可以套用我们熟知的各种机器学习模型或深度学习模型啦！</p>
<h3 id="利用词向量进行分类"><a href="#利用词向量进行分类" class="headerlink" title="利用词向量进行分类"></a>利用词向量进行分类</h3><p>首先最简单的当然是试探一下逻辑回归是否可行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = np.array(vector_list)</span><br><span class="line">y = np.array(data_all[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">logistic_model = LogisticRegression()</span><br><span class="line">logistic_model.fit(train_X, train_y)</span><br><span class="line">logistic_model.score(test_X, test_y)</span><br></pre></td></tr></table></figure>
<p>结果是0.7783333333333333,不好不差，至少说明了构建的词向量不算离谱hhh~我们接下来将所有能想到的分类的模型全部试一遍，然后选一个效果比较不错的再进行调参，大致是这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier  <span class="comment"># 多层感知机</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment"># K最近邻</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC  <span class="comment"># 支持向量机</span></span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessClassifier  <span class="comment"># 高斯过程</span></span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> RBF  <span class="comment"># 高斯核函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier  <span class="comment"># 决策树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,\</span><br><span class="line">    ExtraTreesClassifier, BaggingClassifier  <span class="comment"># 集成方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB  <span class="comment"># 高斯朴素贝叶斯</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis  <span class="comment"># 判别分析</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier  <span class="comment"># 极端梯度提升（eXtreme Gradient Boosting）</span></span><br><span class="line"></span><br><span class="line">classifiers = [</span><br><span class="line">    (<span class="string">'Logistic Regression'</span>, LogisticRegression()),  <span class="comment"># 逻辑回归</span></span><br><span class="line">    (<span class="string">'Nearest Neighbors'</span>, KNeighborsClassifier(<span class="number">3</span>)),  <span class="comment"># K最近邻</span></span><br><span class="line">    (<span class="string">'Linear SVM'</span>, SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">0.025</span>)),  <span class="comment"># 线性的支持向量机</span></span><br><span class="line">    (<span class="string">'RBF SVM'</span>, SVC(gamma=<span class="number">2</span>, C=<span class="number">1</span>)),  <span class="comment"># 径向基函数的支持向量机</span></span><br><span class="line">    (<span class="string">'Gaussian Process'</span>, GaussianProcessClassifier(<span class="number">1.0</span> * RBF(<span class="number">1.0</span>))),  <span class="comment"># 基于拉普拉斯近似的高斯过程</span></span><br><span class="line">    (<span class="string">'Decision Tree'</span>, DecisionTreeClassifier(max_depth=<span class="number">5</span>)),  <span class="comment"># 决策树</span></span><br><span class="line">    (<span class="string">'Random Forest'</span>, RandomForestClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">10</span>, max_features=<span class="number">1</span>)),  <span class="comment"># 随机森林</span></span><br><span class="line">    (<span class="string">'AdaBoost'</span>, AdaBoostClassifier()),  <span class="comment"># 通过迭代弱分类器而产生最终的强分类器的算法</span></span><br><span class="line">    (<span class="string">'Extra Trees'</span>, ExtraTreesClassifier()),</span><br><span class="line">    (<span class="string">'GradientBoosting'</span>, GradientBoostingClassifier()),  <span class="comment"># 梯度提升树</span></span><br><span class="line">    (<span class="string">'Bagging'</span>, BaggingClassifier()),</span><br><span class="line">    (<span class="string">'Naive Bayes'</span>, GaussianNB()),  <span class="comment"># 朴素贝叶斯</span></span><br><span class="line">    (<span class="string">'QDA'</span>, QuadraticDiscriminantAnalysis()),  <span class="comment"># 二次判别分析</span></span><br><span class="line">    (<span class="string">'LDA'</span>, LinearDiscriminantAnalysis()),  <span class="comment"># 线性判别分析</span></span><br><span class="line">    (<span class="string">'MLP'</span>, MLPClassifier(alpha=<span class="number">1</span>)),  <span class="comment"># 多层感知机</span></span><br><span class="line">    (<span class="string">'XGB'</span>, XGBClassifier()),  <span class="comment"># 极端梯度提升</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>定义两个函数便于调参（构建词向量与训练模型）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testWord2Vec</span><span class="params">(data_all, size_=<span class="number">100</span>, min_count_=<span class="number">5</span>)</span>:</span></span><br><span class="line">    word_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data_all)):</span><br><span class="line">        word_list.append(data_all[<span class="string">"text"</span>][i].strip().split(<span class="string">" "</span>))</span><br><span class="line">    word2vec_model = Word2Vec(word_list, size=size_, iter=<span class="number">10</span>, min_count=min_count_)</span><br><span class="line">    vector_list = []</span><br><span class="line">    startTime = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data_all)):</span><br><span class="line">        text = data_all[<span class="string">"text"</span>][i].strip().split(<span class="string">" "</span>)</span><br><span class="line">        vector_list.append(getVector(text, word2vec_model))</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"前%d个文本生成词向量花费总时间%.2f秒"</span> %(i, time.time()-startTime))</span><br><span class="line">    X = np.array(vector_list)</span><br><span class="line">    y = np.array(data_all[<span class="string">"label"</span>])</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testModel</span><span class="params">(X, y, test_size_=<span class="number">0.2</span>, seed=<span class="number">0</span>)</span>:</span> </span><br><span class="line">    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size_, random_state=seed)</span><br><span class="line">    <span class="keyword">for</span> name, clf <span class="keyword">in</span> classifiers:</span><br><span class="line">        print(name, end=<span class="string">": "</span>)</span><br><span class="line">        clf.fit(train_X, train_y)</span><br><span class="line">        print(clf.score(test_X, test_y))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = testWord2Vec(data_all)</span><br><span class="line">testModel(X, y)</span><br></pre></td></tr></table></figure>
<p>本人训练后结果比较好的是逻辑回归、K最近邻、多层感知机和极端梯度提升，没有进行调参的准确率大约0.85左右，部分结果如下。希望有更高的准确率读者自行调参即可：</p>
<p><img src="/Pic/NLP/32.png" alt></p>
<h3 id="其他方式（Doc2Vec）"><a href="#其他方式（Doc2Vec）" class="headerlink" title="其他方式（Doc2Vec）"></a>其他方式（Doc2Vec）</h3><p>事实上我们不一定要先构建词向量，再取平均得到句子向量，2013 年 Mikolov 提出了 word2vec 来学习单词的向量表示，主要有两种方法，cbow ( continuous bag of words) 和 skip-gram ， 一个是用语境来预测目标单词，另一个是用中心单词来预测语境。</p>
<p>既然可以将 word 表示成向量形式，那么句子／段落／文档是否也可以只用一个向量表示？事实上是可以的，主要有两种方式，第一种方式便是我们 前面所做的，先得到 word 的向量表示，然后用一个简单的平均来代表文档。 第二种方式就是 Mikolov 在 2014 提出的Doc2Vec，也有两种方法来实现：</p>
<p><strong>Distributed Memory Model of Paragraph Vectors(PVDM)</strong></p>
<p>它不是仅是使用一些单词来预测下一个单词,我们还添加了另一个特征向量，即<strong>文档Id</strong>。</p>
<p>因此，当训练单词向量W时，也训练文档向量D，并且在训练结束时，它包含了文档的向量化表示。</p>
<p><img src="/Pic/NLP/29.jpg" alt></p>
<p>调用gensim方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = gensim.models.Doc2Vec(documents,dm = <span class="number">0</span>, alpha=<span class="number">0.1</span>, size= <span class="number">20</span>, min_alpha=<span class="number">0.025</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Distributed Bag of Words version of Paragraph Vector(PV-DBOW)</strong></p>
<p><img src="/Pic/NLP/30.jpg" alt></p>
<p>调用gensim方式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = gensim.models.Doc2Vec(documents,dm = <span class="number">0</span>, alpha=<span class="number">0.1</span>, size= <span class="number">20</span>, min_alpha=<span class="number">0.025</span>)</span><br></pre></td></tr></table></figure>
<p>由上面的调用方式可见，PVDM与PV-DBOW两者在 gensim 实现时的区别是 dm = 0 还是 1。总结一下：</p>
<ul>
<li><p><strong>Doc2Vec 的目的</strong>：获得文档的一个固定长度的向量表达。</p>
</li>
<li><p><strong>数据</strong>：多个文档，以及它们的标签，可以用标题作为标签。</p>
</li>
<li><strong>影响模型准确率的因素</strong>：语料的大小，文档的数量，越多越高；文档的相似性，越相似越好。</li>
</ul>
<p>下面我们动手实现一下Doc2Vec（具体参考<a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py" target="_blank" rel="noopener">文档</a>）结果可能会让你大吃一惊（哭笑）：</p>
<p>首先还是一样导入相关的包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Doc2Vec</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<p>然后读取文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_all = pd.read_csv(<span class="string">"train.csv"</span>, sep=<span class="string">"\t"</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">data_all.head()</span><br></pre></td></tr></table></figure>
<p>用后3000条做验证集（当然也可以搞个N折交叉验证严谨点），这里需要注意的是一定要重置索引，哎说多都是泪：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_all_train = data_all.head(<span class="number">12000</span>)</span><br><span class="line">data_all_val = data_all.tail(<span class="number">3000</span>)</span><br><span class="line">data_all_val = data_all_val.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">data_all_train.head()</span><br></pre></td></tr></table></figure>
<p>然后按格式读进数据，这里定义了一个read_corpus，是根据官方文档进行的修改，读者也可以根据自己的需要对其进行修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_corpus</span><span class="params">(data_all, tokens_only=False)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data_all)):</span><br><span class="line">        tokens = data_all[<span class="string">"text"</span>][i].split(<span class="string">" "</span>)</span><br><span class="line">        <span class="keyword">if</span> tokens_only:</span><br><span class="line">            <span class="keyword">yield</span> tokens</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> gensim.models.doc2vec.TaggedDocument(tokens, [data_all[<span class="string">'label'</span>][i]])</span><br><span class="line">    </span><br><span class="line">train_corpus = list(read_corpus(data_all_train))</span><br><span class="line">val_corpus = list(read_corpus(data_all_val, tokens_only=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>随后定义模型，可以开始训练啦：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = gensim.models.doc2vec.Doc2Vec(vector_size=<span class="number">50</span>, min_count=<span class="number">2</span>, epochs=<span class="number">40</span>)</span><br><span class="line">model.build_vocab(train_corpus)</span><br><span class="line">model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)</span><br><span class="line">vector_train_1 = model.infer_vector(train_corpus[<span class="number">1</span>].words)</span><br><span class="line">print(vector_train_1)</span><br><span class="line">tag_train_1 = train_corpus[<span class="number">1</span>].tags</span><br><span class="line">print(tag_train_1)</span><br></pre></td></tr></table></figure>
<p>打印出训练得到的编码，看起来挺人模人样的：</p>
<p><img src="/Pic/NLP/33.png" alt></p>
<p>然后我们就把模型套进我们的文本中，将训练集和测试集顺便划分好：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_X = []</span><br><span class="line">train_y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_corpus)):</span><br><span class="line">    train_X.append(list(model.infer_vector(train_corpus[i].words)))</span><br><span class="line">    train_y.append(train_corpus[i].tags)</span><br><span class="line">    <span class="keyword">if</span>(i%<span class="number">3000</span>==<span class="number">0</span>):</span><br><span class="line">        print(<span class="string">"Processing train set"</span>, i)</span><br><span class="line">val_X = []</span><br><span class="line">val_y = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(val_corpus)):</span><br><span class="line">    val_X.append(list(model.infer_vector(val_corpus[i])))</span><br><span class="line">    val_y.append([data_all[<span class="string">"label"</span>][i]])</span><br><span class="line">    </span><br><span class="line">train_X = np.array(train_X)</span><br><span class="line">train_y = np.array(train_y)</span><br><span class="line">val_X = np.array(val_X)</span><br><span class="line">val_y = np.array(val_y)</span><br></pre></td></tr></table></figure>
<p>好这一切准备就绪了，我们就套一个逻辑回归吧，咱也甭求多高准确度，百分之八九十就好啦再慢慢调参吧：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">logistic_model = LogisticRegression()</span><br><span class="line">logistic_model.fit(train_X, train_y)</span><br><span class="line">logistic_model.score(val_X, val_y)</span><br></pre></td></tr></table></figure>
<p>看看结果：0.12666666，雾草？？？我不信邪试了试其他的回归模型也差不多这个数，emmm网上搜了搜确实有遇到Doc2Vec效果不稳定的情况，但看原理不是和Word2Vec差不多吗，别以为我是埋伏笔，俺也不知道啊大家清楚的下面留言咯，看看是我代码的问题还是确实如此，又或者是数据集的特点决定的~</p>
<h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p>参考链接：</p>
<ul>
<li><a href="https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification</a></li>
<li><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6688348.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6688348.html</a></li>
<li><a href="https://blog.csdn.net/zhangbaoanhadoop/article/details/79570128" target="_blank" rel="noopener">https://blog.csdn.net/zhangbaoanhadoop/article/details/79570128</a></li>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">https://blog.csdn.net/feilong_csdn/article/details/88655927</a></li>
<li>A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="noopener"><em>Bag of Tricks for Efficient Text Classification</em></a></li>
<li><a href="https://blog.csdn.net/qdhy199148/article/details/51754631" target="_blank" rel="noopener">https://blog.csdn.net/qdhy199148/article/details/51754631</a></li>
<li><a href="https://www.imooc.com/article/41650" target="_blank" rel="noopener">https://www.imooc.com/article/41650</a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>NLP项目Pipeline</title>
    <url>/posts/aefe1ee4.html</url>
    <content><![CDATA[<h1 id="Traditional-NLP-Pipeline"><a href="#Traditional-NLP-Pipeline" class="headerlink" title="Traditional NLP Pipeline"></a>Traditional NLP Pipeline</h1><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>繁杂的数据预处理：分词形式、是否做字符级别的切分词等等</li>
<li>基于word embedding：太大，稀疏，线上部署需要保证内存等</li>
<li>模型能力不够，拟合能力有限，RNN、CNN等很难学习更多东西</li>
<li>训练数据：需要数据集大，人工标注成本大</li>
</ul>
<p>下面我们看看训练流程。</p>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="基于Pytorch"><a href="#基于Pytorch" class="headerlink" title="基于Pytorch"></a>基于Pytorch</h3><h4 id="下载并处理IMDB公开数据集"><a href="#下载并处理IMDB公开数据集" class="headerlink" title="下载并处理IMDB公开数据集"></a>下载并处理IMDB公开数据集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED) <span class="comment">#为CPU设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed(SEED)<span class="comment">#为GPU设置随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用来定义字段的处理方法（文本字段，标签字段）</span></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)<span class="comment">#torchtext.data.Field : </span></span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br></pre></td></tr></table></figure>
<h4 id="切分训练集、测试集和验证集"><a href="#切分训练集、测试集和验证集" class="headerlink" title="切分训练集、测试集和验证集"></a>切分训练集、测试集和验证集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>
<h4 id="读入glove词向量"><a href="#读入glove词向量" class="headerlink" title="读入glove词向量"></a>读入glove词向量</h4><p>目的是把训练集的Token转化为词向量传入模型中，若使用TEXT的api会自动下载词向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>,</span><br><span class="line">                 unk_init=torch.Tensor.normal_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建训练集字典</span></span><br><span class="line">LABEL.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader制作</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"><span class="comment"># 判断使用的是CPU还是GPU</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代器的形式会加速数据传输速度（异步读取）</span></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br></pre></td></tr></table></figure>
<h4 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [sent_len, batch _size, emb_size]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(TEXT.vocab) <span class="comment">#词个数</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span> <span class="comment">#词嵌入维度</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span> <span class="comment">#输出维度</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] <span class="comment">#pad索引</span></span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"></span><br><span class="line">拿到新的词向量，换掉原有的</span><br><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    model.train() <span class="comment">#model.train()代表了训练模式</span></span><br><span class="line">    <span class="comment">#这步一定要加，是为了区分model训练和测试的模式的。</span></span><br><span class="line">    <span class="comment">#有时候训练时会用到dropout、归一化等方法，但是测试的时候不能用dropout等方法。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: <span class="comment">#iterator为train_iterator</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#加这步防止梯度叠加</span></span><br><span class="line"></span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#batch.text 就是上面forward函数的参数text</span></span><br><span class="line">        <span class="comment">#压缩维度，不然跟batch.label维度对不上</span></span><br><span class="line"></span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line"></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#梯度下降</span></span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#loss.item()已经本身除以了len(batch.label)</span></span><br><span class="line">        <span class="comment">#所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。</span></span><br><span class="line"></span><br><span class="line">        epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#（acc.item()：一个batch的正确率） *batch数 = 正确数</span></span><br><span class="line">        <span class="comment">#train_iterator所有batch的正确数累加。</span></span><br><span class="line"></span><br><span class="line">        total_len += len(batch.label)</span><br><span class="line">        <span class="comment">#计算train_iterator所有样本的数量，不出意外应该是17500</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br><span class="line">    <span class="comment">#epoch_loss / total_len ：train_iterator所有batch的损失</span></span><br><span class="line">    <span class="comment">#epoch_acc / total_len ：train_iterator所有batch的正确率</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br><span class="line"></span><br><span class="line"><span class="comment">#import time</span></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>) <span class="comment">#无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#start_time = time.time()</span></span><br><span class="line"></span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#end_time = time.time()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss: <span class="comment">#只要模型效果变好，就存模型</span></span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(f'Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;s')</span></span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="基于Tensorflow2-0"><a href="#基于Tensorflow2-0" class="headerlink" title="基于Tensorflow2.0"></a>基于Tensorflow2.0</h3><h4 id="下载IMDB公开数据集，并处理"><a href="#下载IMDB公开数据集，并处理" class="headerlink" title="下载IMDB公开数据集，并处理"></a>下载IMDB公开数据集，并处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imdb_dir = <span class="string">"./aclImdb"</span></span><br><span class="line">train_dir = os.path.join(imdb_dir, <span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">"neg"</span>, <span class="string">"pos"</span>]:</span><br><span class="line">    dir_name = os.path.join(train_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(dir_name):</span><br><span class="line">        <span class="keyword">if</span> fname [<span class="number">-4</span>:] == <span class="string">".txt"</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">"neg"</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="制作训练集和验证集"><a href="#制作训练集和验证集" class="headerlink" title="制作训练集和验证集"></a>制作训练集和验证集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing. sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">maxlen = <span class="number">100</span> <span class="comment"># cuts off review after 100 words</span></span><br><span class="line">training_samples = <span class="number">200</span> <span class="comment"># Trains on 200 samples</span></span><br><span class="line">validation_samples = <span class="number">10000</span> <span class="comment"># Validates o 10000 samples</span></span><br><span class="line">max_words = <span class="number">10000</span> <span class="comment"># Considers only the top 10000 words in the dataset</span></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">word_index = tokenizer.word_index                   <span class="comment"># Length: 88582</span></span><br><span class="line">print(<span class="string">"Found %s unique tokens."</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">data = pad_sequences(sequences, maxlen=maxlen)</span><br><span class="line"></span><br><span class="line">labels = np.asarray(labels)</span><br><span class="line">print(<span class="string">"Shape of data tensor:"</span>, data.shape)</span><br><span class="line">print(<span class="string">"Shape of label tensor:"</span>, labels.shape)</span><br><span class="line"></span><br><span class="line">indices = np.arange(data.shape[<span class="number">0</span>]) <span class="comment"># Splits data into training and validation set, but shuffles is, since samples are ordered:</span></span><br><span class="line"><span class="comment"># all negatives first, then all positive</span></span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">data = data[indices]</span><br><span class="line">labels = labels[indices]</span><br><span class="line"></span><br><span class="line">x_train = data[:training_samples] <span class="comment"># (200, 100)</span></span><br><span class="line">y_train = labels[:training_samples] <span class="comment"># shape (200,)</span></span><br><span class="line">x_val = data[training_samples:training_samples+validation_samples] <span class="comment"># shape (10000, 100)</span></span><br><span class="line">y_val = labels[training_samples:training_samples+validation_samples] <span class="comment"># shape (10000,)</span></span><br></pre></td></tr></table></figure>
<h4 id="下载glove词向量，并读入"><a href="#下载glove词向量，并读入" class="headerlink" title="下载glove词向量，并读入"></a>下载glove词向量，并读入</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">glove_dir = <span class="string">"./"</span></span><br><span class="line"></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line"></span><br><span class="line">f = open(os.path.join(glove_dir, <span class="string">"glove.6B.50d.txt"</span>), encoding=<span class="string">'utf-8'</span>) <span class="comment">#added , encoding='utf-8'</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">"float32"</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"found %s word vectors."</span> % len (embeddings_index))</span><br></pre></td></tr></table></figure>
<h4 id="读入词向量"><a href="#读入词向量" class="headerlink" title="读入词向量"></a>读入词向量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embedding_dim = <span class="number">50</span> <span class="comment"># GloVe contains 50-dimensional embedding vectors for 400.000 words</span></span><br><span class="line"></span><br><span class="line">embedding_matrix = np.zeros((max_words, embedding_dim)) <span class="comment"># embedding_matrix.shape (10000, 50)</span></span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    <span class="keyword">if</span> i &lt; max_words:</span><br><span class="line">        embedding_vector = embeddings_index.get(word) <span class="comment"># embedding_vector.shape (100,)</span></span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector <span class="comment"># Words not found in the mebedding index will all be zeros</span></span><br></pre></td></tr></table></figure>
<h4 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.models.Sequential() </span><br><span class="line">model.add(keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen))</span><br><span class="line">model.add(keras.layers.Bidirectional(keras.layers.LSTM(<span class="number">64</span>, return_sequences = <span class="literal">True</span>)))</span><br><span class="line">model.add(keras.layers.GlobalMaxPool1D())</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">16</span>, activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation = <span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(loss = <span class="string">'binary_crossentropy'</span>, optimizer = <span class="string">'adam'</span>, metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h4 id="加载预训词向量"><a href="#加载预训词向量" class="headerlink" title="加载预训词向量"></a>加载预训词向量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers[<span class="number">0</span>].set_weights([embedding_matrix])</span><br><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer = <span class="string">"rmsprop"</span>, </span><br><span class="line">              loss = <span class="string">"binary_crossentropy"</span>, <span class="comment"># in a multiclass problem categorical_crossentropy would be used</span></span><br><span class="line">              metrics = [<span class="string">"acc"</span>]) </span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                   epochs = <span class="number">10</span>,</span><br><span class="line">                   batch_size = <span class="number">32</span>,</span><br><span class="line">                   validation_data = (x_val, y_val))</span><br><span class="line">model.save_weights(<span class="string">"pre_trained_glove_model.h5"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="New-NLP-Pipeline"><a href="#New-NLP-Pipeline" class="headerlink" title="New NLP Pipeline"></a>New NLP Pipeline</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>先进的Tokenizer：WordPiece，将词拆成一个一个字，减少了vocab集的大小</li>
<li>模型结构：强大的特征提取器Transformer</li>
<li>预训练任务：解决了数据问题，不需要太多的数据清洗</li>
</ul>
<p>注：生成任务不适合使用Bert</p>
<h2 id="训练流程-1"><a href="#训练流程-1" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="基于Pytorch-1"><a href="#基于Pytorch-1" class="headerlink" title="基于Pytorch"></a>基于Pytorch</h3><h4 id="导入相关的包"><a href="#导入相关的包" class="headerlink" title="导入相关的包"></a>导入相关的包</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 使用transformers包</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<h4 id="种子和参数设定"><a href="#种子和参数设定" class="headerlink" title="种子和参数设定"></a>种子和参数设定</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数</span></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line">TRAIN = <span class="literal">False</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">TEXT = <span class="string">"I like you!"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定模型用种子，便于重复试验</span></span><br><span class="line">random.seed(SEED)</span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="处理Token"><a href="#处理Token" class="headerlink" title="处理Token"></a>处理Token</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 应用transformers中Tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">init_token_id = tokenizer.cls_token_id</span><br><span class="line">eos_token_id  = tokenizer.sep_token_id</span><br><span class="line">pad_token_id  = tokenizer.pad_token_id</span><br><span class="line">unk_token_id  = tokenizer.unk_token_id</span><br><span class="line"></span><br><span class="line">max_input_len = tokenizer.max_model_input_sizes[<span class="string">'bert-base-uncased'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子长度切割成510长，为了加上开头和最后一个token</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_crop</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    tokens = tokenizer.tokenize(sentence)</span><br><span class="line">    tokens = tokens[:max_input_len - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>
<h4 id="加载PyTorch提供的IMDB数据"><a href="#加载PyTorch提供的IMDB数据" class="headerlink" title="加载PyTorch提供的IMDB数据"></a>加载PyTorch提供的IMDB数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    text = data.Field(</span><br><span class="line">        batch_first=<span class="literal">True</span>,</span><br><span class="line">        use_vocab=<span class="literal">False</span>,</span><br><span class="line">        tokenize=tokenize_and_crop,</span><br><span class="line">        preprocessing=tokenizer.convert_tokens_to_ids,</span><br><span class="line">        init_token=init_token_id,</span><br><span class="line">        pad_token=pad_token_id,</span><br><span class="line">        unk_token=unk_token_id</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    label = data.LabelField(dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    train_data, test_data  = datasets.IMDB.splits(text, label)</span><br><span class="line">    train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"training examples count: <span class="subst">&#123;len(train_data)&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"test examples count: <span class="subst">&#123;len(test_data)&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"validation examples count: <span class="subst">&#123;len(valid_data)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    label.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = data.BucketIterator.splits(</span><br><span class="line">        (train_data, valid_data, test_data),</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        device=device</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, valid_iter, test_iter</span><br></pre></td></tr></table></figure>
<h4 id="引入Bert预处理模型"><a href="#引入Bert预处理模型" class="headerlink" title="引入Bert预处理模型"></a>引入Bert预处理模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 看是否有GPU</span></span><br><span class="line">device = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过transformers包，建立BERT模型</span></span><br><span class="line">bert_model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="搭建模型-1"><a href="#搭建模型-1" class="headerlink" title="搭建模型"></a>搭建模型</h4><p>在这里开始做finetune处理了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此处用BERT做为基础模型完成情感分析任务</span></span><br><span class="line"><span class="comment"># 在BERT之上加两层GRU</span></span><br><span class="line"><span class="comment"># 最后接一层线性层用于完成分类任务</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        bert,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">        bidirectional,</span></span></span><br><span class="line"><span class="function"><span class="params">        dropout</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">      </span><br><span class="line">        super(SentimentModel, self).__init__()</span><br><span class="line">    </span><br><span class="line">        self.bert = bert</span><br><span class="line">        embedding_dim = bert.config.to_dict()[<span class="string">'hidden_size'</span>]</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embedding_dim,</span><br><span class="line">            hidden_dim,</span><br><span class="line">            num_layers=n_layers,</span><br><span class="line">            bidirectional=bidirectional,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            dropout=<span class="number">0</span> <span class="keyword">if</span> n_layers &lt; <span class="number">2</span> <span class="keyword">else</span> dropout</span><br><span class="line">            )</span><br><span class="line">        self.out = nn.Linear(hidden_dim * <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> hidden_dim, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            embedded = self.bert(text)[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">        _, hidden = self.rnn(embedded)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> self.rnn.bidirectional:</span><br><span class="line">            hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = self.dropout(hidden[<span class="number">-1</span>,:,:])</span><br><span class="line">    </span><br><span class="line">        output = self.out(hidden)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">model = SentimentModel(</span><br><span class="line">  bert_model,</span><br><span class="line">  HIDDEN_DIM,</span><br><span class="line">  OUTPUT_DIM,!pip list</span><br><span class="line">  N_LAYERS,</span><br><span class="line">  BIDIRECTIONAL,</span><br><span class="line">  DROPOUT</span><br><span class="line">)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<h4 id="一个epoch需要多长时间"><a href="#一个epoch需要多长时间" class="headerlink" title="一个epoch需要多长时间"></a>一个epoch需要多长时间</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>
<h4 id="二分类问题的accuracy"><a href="#二分类问题的accuracy" class="headerlink" title="二分类问题的accuracy"></a>二分类问题的accuracy</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float()</span><br><span class="line">    acc = correct.sum() / len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<h4 id="一个训练步"><a href="#一个训练步" class="headerlink" title="一个训练步"></a>一个训练步</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>
<h4 id="验证模型"><a href="#验证模型" class="headerlink" title="验证模型"></a>验证模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>
<h4 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(model, tokenizer, sentence)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    tokens = tokenizer.tokenize(sentence)</span><br><span class="line">    tokens = tokens[:max_input_len - <span class="number">2</span>]</span><br><span class="line">    indexed = [init_token_id] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_id]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br><span class="line"></span><br><span class="line">train_iter, valid_iter, test_iter = load_data()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss().to(device)</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">best_val_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="comment"># 训练一个epoch</span></span><br><span class="line">    train_loss, train_acc = train(model, train_iter, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'model.pt'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">if</span> TRAIN:</span><br><span class="line">        <span class="comment"># 读取数据</span></span><br><span class="line">        train_iter, valid_iter, test_iter = load_data()</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters())</span><br><span class="line">        criterion = nn.BCEWithLogitsLoss().to(device)</span><br><span class="line">        model = model.to(device)</span><br><span class="line"></span><br><span class="line">        best_val_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">            start_time = time.time()</span><br><span class="line">            <span class="comment"># 训练一个epoch</span></span><br><span class="line">            train_loss, train_acc = train(model, train_iter, optimizer, criterion)</span><br><span class="line">            valid_loss, valid_acc = evaluate(model, valid_iter, criterion)</span><br><span class="line"></span><br><span class="line">            end_time = time.time()</span><br><span class="line"></span><br><span class="line">            epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">                best_valid_loss = valid_loss</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">'model.pt'</span>)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">            print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">            print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">        <span class="comment"># 测试</span></span><br><span class="line">        model.load_state_dict(torch.load(<span class="string">'model.pt'</span>))</span><br><span class="line">        test_loss, test_acc = evaluate(model, test_iter, criterion)</span><br><span class="line">        print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 推理结果</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.load_state_dict(torch.load(<span class="string">'model.pt'</span>, map_location=device))</span><br><span class="line">        sentiment = predict_sentiment(model, tokenizer, TEXT)</span><br><span class="line">        print(sentiment)</span><br></pre></td></tr></table></figure>
<h3 id="基于Tensorflow2-0-1"><a href="#基于Tensorflow2-0-1" class="headerlink" title="基于Tensorflow2.0"></a>基于Tensorflow2.0</h3><p>This is a modification of <a href="https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" target="_blank" rel="noopener">https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb</a> using the Tensorflow 2.0 Keras implementation of BERT from <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">kpe/bert-for-tf2</a> with the original <a href="https://github.com/google-research/bert" target="_blank" rel="noopener">google-research/bert</a> weights.</p>
<p><strong>Predicting Movie Review Sentiment with <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">kpe/bert-for-tf2</a></strong></p>
<h4 id="First-install-some-prerequisites"><a href="#First-install-some-prerequisites" class="headerlink" title="First install some prerequisites:"></a>First install some prerequisites:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>In addition to the standard libraries we imported above, we’ll need to install the <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">bert-for-tf2</a> python package, and do the imports required for loading the pre-trained weights and tokenizing the input text. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bert</span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> BertModelLayer</span><br><span class="line"><span class="keyword">from</span> bert.loader <span class="keyword">import</span> StockBertConfig, map_stock_config_to_params, load_stock_weights</span><br><span class="line"><span class="keyword">from</span> bert.tokenization.bert_tokenization <span class="keyword">import</span> FullTokenizer</span><br></pre></td></tr></table></figure>
<h4 id="Download-the-dataset"><a href="#Download-the-dataset" class="headerlink" title="Download the dataset"></a>Download the dataset</h4><p>First, let’s download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from <a href="https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub" target="_blank" rel="noopener">this Tensorflow tutorial</a>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all files from a directory in a DataFrame.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_directory_data</span><span class="params">(directory)</span>:</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    data[<span class="string">"sentence"</span>] = []</span><br><span class="line">    data[<span class="string">"sentiment"</span>] = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> tqdm(os.listdir(directory), desc=os.path.basename(directory)):</span><br><span class="line">        <span class="keyword">with</span> tf.io.gfile.GFile(os.path.join(directory, file_path), <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data[<span class="string">"sentence"</span>].append(f.read())</span><br><span class="line">            data[<span class="string">"sentiment"</span>].append(re.match(<span class="string">"\d+_(\d+)\.txt"</span>, file_path).group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame.from_dict(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge positive and negative examples, add a polarity column and shuffle.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(directory)</span>:</span></span><br><span class="line">    pos_df = load_directory_data(os.path.join(directory, <span class="string">"pos"</span>))</span><br><span class="line">    neg_df = load_directory_data(os.path.join(directory, <span class="string">"neg"</span>))</span><br><span class="line">    pos_df[<span class="string">"polarity"</span>] = <span class="number">1</span></span><br><span class="line">    neg_df[<span class="string">"polarity"</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> pd.concat([pos_df, neg_df]).sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download and process the dataset files.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_and_load_datasets</span><span class="params">(force_download=False)</span>:</span></span><br><span class="line"><span class="comment">#     dataset = tf.keras.utils.get_file(</span></span><br><span class="line"><span class="comment">#       fname="aclImdb.tar.gz", </span></span><br><span class="line"><span class="comment">#       origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", </span></span><br><span class="line"><span class="comment">#       extract=True)</span></span><br><span class="line">    </span><br><span class="line">    dataset = <span class="string">"./aclImdb"</span></span><br><span class="line">    train_df = load_dataset(os.path.join(os.path.dirname(dataset), </span><br><span class="line">                                       <span class="string">"aclImdb"</span>, <span class="string">"train"</span>))</span><br><span class="line">    test_df = load_dataset(os.path.join(os.path.dirname(dataset), </span><br><span class="line">                                      <span class="string">"aclImdb"</span>, <span class="string">"test"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br></pre></td></tr></table></figure>
<h4 id="Prepare-Encode"><a href="#Prepare-Encode" class="headerlink" title="Prepare/Encode"></a>Prepare/Encode</h4><p>Let’s use the <code>MovieReviewData</code> class below, to prepare/encode the data for feeding into our BERT model, by:</p>
<ul>
<li>tokenizing the text</li>
<li>trim or pad it to a <code>max_seq_len</code> length</li>
<li>append the special tokens <code>[CLS]</code> and <code>[SEP]</code></li>
<li>convert the string tokens to numerical <code>ID</code>s using the original model’s token encoding from <code>vocab.txt</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bert</span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> BertModelLayer</span><br><span class="line"><span class="keyword">from</span> bert.loader <span class="keyword">import</span> StockBertConfig, map_stock_config_to_params, load_stock_weights</span><br><span class="line"><span class="comment"># from bert.tokenization import FullTokenizer</span></span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> bert_tokenization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MovieReviewData</span>:</span></span><br><span class="line">    DATA_COLUMN = <span class="string">"sentence"</span></span><br><span class="line">    LABEL_COLUMN = <span class="string">"polarity"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokenizer: bert_tokenization.FullTokenizer, sample_size=None, max_seq_len=<span class="number">1024</span>)</span>:</span></span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.sample_size = sample_size</span><br><span class="line">        self.max_seq_len = <span class="number">0</span></span><br><span class="line">        train, test = download_and_load_datasets()</span><br><span class="line">        </span><br><span class="line">        train, test = map(<span class="keyword">lambda</span> df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), </span><br><span class="line">                          [train, test])</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">if</span> sample_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> sample_size % <span class="number">128</span> == <span class="number">0</span></span><br><span class="line">            train, test = train.head(sample_size), test.head(sample_size)</span><br><span class="line">            <span class="comment"># train, test = map(lambda df: df.sample(sample_size), [train, test])</span></span><br><span class="line">        </span><br><span class="line">        ((self.train_x, self.train_y),</span><br><span class="line">         (self.test_x, self.test_y)) = map(self._prepare, [train, test])</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"max seq_len"</span>, self.max_seq_len)</span><br><span class="line">        self.max_seq_len = min(self.max_seq_len, max_seq_len)</span><br><span class="line">        ((self.train_x, self.train_x_token_types),</span><br><span class="line">         (self.test_x, self.test_x_token_types)) = map(self._pad, </span><br><span class="line">                                                       [self.train_x, self.test_x])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        x, y = [], []</span><br><span class="line">        <span class="keyword">with</span> tqdm(total=df.shape[<span class="number">0</span>], unit_scale=<span class="literal">True</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">            <span class="keyword">for</span> ndx, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]</span><br><span class="line">                tokens = self.tokenizer.tokenize(text)</span><br><span class="line">                tokens = [<span class="string">"[CLS]"</span>] + tokens + [<span class="string">"[SEP]"</span>]</span><br><span class="line">                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">                self.max_seq_len = max(self.max_seq_len, len(token_ids))</span><br><span class="line">                x.append(token_ids)</span><br><span class="line">                y.append(int(label))</span><br><span class="line">                pbar.update()</span><br><span class="line">        <span class="keyword">return</span> np.array(x), np.array(y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pad</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        x, t = [], []</span><br><span class="line">        token_type_ids = [<span class="number">0</span>] * self.max_seq_len</span><br><span class="line">        <span class="keyword">for</span> input_ids <span class="keyword">in</span> ids:</span><br><span class="line">            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - <span class="number">2</span>)]</span><br><span class="line">            input_ids = input_ids + [<span class="number">0</span>] * (self.max_seq_len - len(input_ids))</span><br><span class="line">            x.append(np.array(input_ids))</span><br><span class="line">            t.append(token_type_ids)</span><br><span class="line">        <span class="keyword">return</span> np.array(x), np.array(t)</span><br></pre></td></tr></table></figure>
<h4 id="A-tweak"><a href="#A-tweak" class="headerlink" title="A tweak"></a>A tweak</h4><p>Because of a <code>tf.train.load_checkpoint</code> limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_ckpt_dir=<span class="string">"./uncased_L-12_H-768_A-12/"</span></span><br><span class="line">bert_ckpt_file = bert_ckpt_dir + <span class="string">"bert_model.ckpt"</span></span><br><span class="line">bert_config_file = bert_ckpt_dir + <span class="string">"bert_config.json"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%time</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert_model_dir="2018_10_18"</span></span><br><span class="line"><span class="comment"># bert_model_name="uncased_L-12_H-768_A-12"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># !mkdir -p .model .model/$bert_model_name</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for fname in ["bert_config.json", "vocab.txt", "bert_model.ckpt.meta", "bert_model.ckpt.index", "bert_model.ckpt.data-00000-of-00001"]:</span></span><br><span class="line"><span class="comment">#   cmd = f"gsutil cp gs://bert_models/&#123;bert_model_dir&#125;/&#123;bert_model_name&#125;/&#123;fname&#125; .model/&#123;bert_model_name&#125;"</span></span><br><span class="line"><span class="comment">#   !$cmd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># !ls -la .model .model/$bert_model_name</span></span><br><span class="line"></span><br><span class="line">bert_ckpt_dir    = os.path.join(<span class="string">".model/"</span>,bert_model_name)</span><br><span class="line">bert_ckpt_file   = os.path.join(bert_ckpt_dir, <span class="string">"bert_model.ckpt"</span>)</span><br><span class="line">bert_config_file = os.path.join(bert_ckpt_dir, <span class="string">"bert_config.json"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h4><p>Now let’s fetch and prepare the data by taking the first <code>max_seq_len</code> tokenens after tokenizing with the BERT tokenizer, und use <code>sample_size</code> examples for both training and testing.</p>
<p>To keep training fast, we’ll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use <code>max_seq_len=512</code>, but on a GPU this would be too slow, and you will have to use a very small <code>batch_size</code>s to fit the model into the GPU memory).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, <span class="string">"vocab.txt"</span>))</span><br><span class="line">data = MovieReviewData(tokenizer, </span><br><span class="line">                       sample_size=<span class="number">10</span>*<span class="number">128</span>*<span class="number">2</span>,<span class="comment">#5000, </span></span><br><span class="line">                       max_seq_len=<span class="number">128</span>)</span><br><span class="line">print(<span class="string">"            train_x"</span>, data.train_x.shape)</span><br><span class="line">print(<span class="string">"train_x_token_types"</span>, data.train_x_token_types.shape)</span><br><span class="line">print(<span class="string">"            train_y"</span>, data.train_y.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"             test_x"</span>, data.test_x.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"        max_seq_len"</span>, data.max_seq_len)</span><br></pre></td></tr></table></figure>
<h4 id="Adapter-BERT"><a href="#Adapter-BERT" class="headerlink" title="Adapter BERT"></a>Adapter BERT</h4><p>If we decide to use <a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener">adapter-BERT</a> we need some helpers for freezing the original BERT layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_layers</span><span class="params">(root_layer)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(root_layer, keras.layers.Layer):</span><br><span class="line">        <span class="keyword">yield</span> root_layer</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> root_layer._layers:</span><br><span class="line">        <span class="keyword">for</span> sub_layer <span class="keyword">in</span> flatten_layers(layer):</span><br><span class="line">            <span class="keyword">yield</span> sub_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">freeze_bert_layers</span><span class="params">(l_bert)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> flatten_layers(l_bert):</span><br><span class="line">        <span class="keyword">if</span> layer.name <span class="keyword">in</span> [<span class="string">"LayerNorm"</span>, <span class="string">"adapter-down"</span>, <span class="string">"adapter-up"</span>]:</span><br><span class="line">            layer.trainable = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> len(layer._layers) == <span class="number">0</span>:</span><br><span class="line">            layer.trainable = <span class="literal">False</span></span><br><span class="line">        l_bert.embeddings_layer.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_learning_rate_scheduler</span><span class="params">(max_learn_rate=<span class="number">5e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   end_learn_rate=<span class="number">1e-7</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   warmup_epoch_count=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   total_epoch_count=<span class="number">90</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lr_scheduler</span><span class="params">(epoch)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> epoch &lt; warmup_epoch_count:</span><br><span class="line">            res = (max_learn_rate/warmup_epoch_count) * (epoch + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+<span class="number">1</span>)/(total_epoch_count-warmup_epoch_count+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> float(res)</span><br><span class="line">    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> learning_rate_scheduler</span><br></pre></td></tr></table></figure>
<h4 id="Creating-a-model"><a href="#Creating-a-model" class="headerlink" title="Creating a model"></a>Creating a model</h4><p>Now let’s create a classification model using <a href="https//arxiv.org/abs/1902.00751">adapter-BERT</a>, which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. <code>adapter_size</code> bellow) in every BERT layer.</p>
<p><strong>N.B.</strong> The commented out code below show how to feed a <code>token_type_ids</code>/<code>segment_ids</code> sequence (which is not needed in our case).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(max_seq_len, adapter_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Creates a classification model."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#adapter_size = 64  # see - arXiv:1902.00751</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create the bert layer</span></span><br><span class="line">    <span class="keyword">with</span> tf.io.gfile.GFile(bert_config_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">        bc = StockBertConfig.from_json_string(reader.read())</span><br><span class="line">        bert_params = map_stock_config_to_params(bc)</span><br><span class="line">        bert_params.adapter_size = adapter_size</span><br><span class="line">        bert = BertModelLayer.from_params(bert_params, name=<span class="string">"bert"</span>)</span><br><span class="line"></span><br><span class="line">    input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype=<span class="string">'int32'</span>, name=<span class="string">"input_ids"</span>)</span><br><span class="line">    <span class="comment"># token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name="token_type_ids")</span></span><br><span class="line">    <span class="comment"># output         = bert([input_ids, token_type_ids])</span></span><br><span class="line">    output         = bert(input_ids)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"bert shape"</span>, output.shape)</span><br><span class="line">    cls_out = keras.layers.Lambda(<span class="keyword">lambda</span> seq: seq[:, <span class="number">0</span>, :])(output)</span><br><span class="line">    cls_out = keras.layers.Dropout(<span class="number">0.5</span>)(cls_out)</span><br><span class="line">    logits = keras.layers.Dense(units=<span class="number">768</span>, activation=<span class="string">"tanh"</span>)(cls_out)</span><br><span class="line">    logits = keras.layers.Dropout(<span class="number">0.5</span>)(logits)</span><br><span class="line">    logits = keras.layers.Dense(units=<span class="number">2</span>, activation=<span class="string">"softmax"</span>)(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)</span></span><br><span class="line">    <span class="comment"># model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])</span></span><br><span class="line">    model = keras.Model(inputs=input_ids, outputs=logits)</span><br><span class="line">    model.build(input_shape=(<span class="literal">None</span>, max_seq_len))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load the pre-trained model weights</span></span><br><span class="line">    load_stock_weights(bert, bert_ckpt_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freeze weights if adapter-BERT is used</span></span><br><span class="line">    <span class="keyword">if</span> adapter_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        freeze_bert_layers(bert)</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=keras.optimizers.Adam(),</span><br><span class="line">                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[keras.metrics.SparseCategoricalAccuracy(name=<span class="string">"acc"</span>)])</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">adapter_size = <span class="literal">None</span> <span class="comment"># use None to fine-tune all of BERT</span></span><br><span class="line">model = create_model(data.max_seq_len, adapter_size=adapter_size)</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line"></span><br><span class="line">log_dir = <span class="string">".log/movie_reviews/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%s"</span>)</span><br><span class="line">tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">total_epoch_count = <span class="number">50</span></span><br><span class="line"><span class="comment"># model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,</span></span><br><span class="line">model.fit(x=data.train_x, y=data.train_y,</span><br><span class="line">          validation_split=<span class="number">0.1</span>,</span><br><span class="line">          batch_size=<span class="number">48</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          epochs=total_epoch_count,</span><br><span class="line">          callbacks=[create_learning_rate_scheduler(max_learn_rate=<span class="number">1e-5</span>,</span><br><span class="line">                                                    end_learn_rate=<span class="number">1e-7</span>,</span><br><span class="line">                                                    warmup_epoch_count=<span class="number">20</span>,</span><br><span class="line">                                                    total_epoch_count=total_epoch_count),</span><br><span class="line">                     keras.callbacks.EarlyStopping(patience=<span class="number">20</span>, restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                     tensorboard_callback])</span><br><span class="line"></span><br><span class="line">model.save_weights(<span class="string">'./movie_reviews.h5'</span>, overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line"></span><br><span class="line">_, train_acc = model.evaluate(data.train_x, data.train_y)</span><br><span class="line">_, test_acc = model.evaluate(data.test_x, data.test_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train acc"</span>, train_acc)</span><br><span class="line">print(<span class="string">"test acc"</span>, test_acc)</span><br></pre></td></tr></table></figure>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>To evaluate the trained model, let’s load the saved weights in a new model instance, and evaluate.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time </span><br><span class="line"></span><br><span class="line">model = create_model(data.max_seq_len, adapter_size=<span class="literal">None</span>)</span><br><span class="line">model.load_weights(<span class="string">"movie_reviews.h5"</span>)</span><br><span class="line"></span><br><span class="line">_, train_acc = model.evaluate(data.train_x, data.train_y)</span><br><span class="line">_, test_acc = model.evaluate(data.test_x, data.test_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train acc"</span>, train_acc)</span><br><span class="line">print(<span class="string">" test acc"</span>, test_acc)</span><br></pre></td></tr></table></figure>
<h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special <code>[CLS]</code> and <code>[SEP]</code> token at begin and end of the token sequence, and pad to match the model input shape.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_sentences = [</span><br><span class="line">  <span class="string">"That movie was absolutely awful"</span>,</span><br><span class="line">  <span class="string">"The acting was a bit lacking"</span>,</span><br><span class="line">  <span class="string">"The film was creative and surprising"</span>,</span><br><span class="line">  <span class="string">"Absolutely fantastic!"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, <span class="string">"vocab.txt"</span>))</span><br><span class="line">pred_tokens    = map(tokenizer.tokenize, pred_sentences)</span><br><span class="line">pred_tokens    = map(<span class="keyword">lambda</span> tok: [<span class="string">"[CLS]"</span>] + tok + [<span class="string">"[SEP]"</span>], pred_tokens)</span><br><span class="line">pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))</span><br><span class="line"></span><br><span class="line">pred_token_ids = map(<span class="keyword">lambda</span> tids: tids +[<span class="number">0</span>]*(data.max_seq_len-len(tids)),pred_token_ids)</span><br><span class="line">pred_token_ids = np.array(list(pred_token_ids))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'pred_token_ids'</span>, pred_token_ids.shape)</span><br><span class="line"></span><br><span class="line">res = model.predict(pred_token_ids).argmax(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text, sentiment <span class="keyword">in</span> zip(pred_sentences, res):</span><br><span class="line">  print(<span class="string">" text:"</span>, text)</span><br><span class="line">  print(<span class="string">"  res:"</span>, [<span class="string">"negative"</span>,<span class="string">"positive"</span>][sentiment])</span><br></pre></td></tr></table></figure>
<h3 id="一些Trick"><a href="#一些Trick" class="headerlink" title="一些Trick"></a>一些Trick</h3><ul>
<li>Multi-task学习</li>
<li>对抗训练</li>
<li>Domain data再训练</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>Bert</title>
    <url>/posts/1424e830.html</url>
    <content><![CDATA[<h1 id="自然语言处理通用解决方案"><a href="#自然语言处理通用解决方案" class="headerlink" title="自然语言处理通用解决方案"></a>自然语言处理通用解决方案</h1><ol>
<li>需要熟悉 word2vec，RNN 网络模型，了解词向量如何建模</li>
<li>重点在于 Transform 网络架构，BERT训练方法，实际应用</li>
<li>开源项目都是现成的，提供预训练模型，基本任务拿过来直接用就成</li>
</ol>
<h1 id="Bert-核心原理"><a href="#Bert-核心原理" class="headerlink" title="Bert 核心原理"></a>Bert 核心原理</h1><p>Transformer结构是谷歌大脑在2017年底发表的一篇论文 Attention is All You Need 中提出的一种语言结构，是一种 Seq2Seq 的模型，Bert 就是从 Transformer 中衍生出来的一种预训练的语言模型。</p>
<p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为Decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<h2 id="1-Transformer-直观理解"><a href="#1-Transformer-直观理解" class="headerlink" title="1 Transformer 直观理解"></a>1 Transformer 直观理解</h2><hr>
<h3 id="1-1-Transformer做了啥"><a href="#1-1-Transformer做了啥" class="headerlink" title="1.1 Transformer做了啥"></a>1.1 Transformer做了啥</h3><ol>
<li>其基本组成仍是机器翻译中常见的Seq2Seq网络，也就是类似输入一串中文，输出对应的英文。</li>
<li>传统架构中，中间的网络设计就是一个RNN，但是RNN不能并行计算，比如输入$x_0,x_1,…x_n$一串字符串，RNN会处理$x_0$得到一个当前输出$h_0$和一个隐藏特征，然后$x_1$与$x_0$不是独立的，会用到前面一个隐藏特征，也就是每个下一步都会用到前面的中间结果，因此我们在计算过程中不能独立计算。这就是RNN的问题，没办法做并行计算。Transformer 能做并行计算。</li>
<li>实际任务中我们对每个词不是一视同仁的，对重要程度不够高的词随便看看就好，对重要程度较高的词需要多关注一下 —— Transformer结构中使用 <strong>Self-Attention</strong> 机制。其输出结果是同时被计算出来的，而不是RNN中一步输出一个结果。</li>
<li>传统的 word2vec表达向量时在不同语境下相同的词无法改变，也就是预训练好的向量永久不变，但实际任务中会需要用到一个词的不同语义表示。</li>
<li>Transformer使用位置嵌入(positional encoding)来理解语言的顺序</li>
</ol>
<hr>
<h3 id="1-2-Attention是啥意思呢？"><a href="#1-2-Attention是啥意思呢？" class="headerlink" title="1.2 Attention是啥意思呢？"></a>1.2 Attention是啥意思呢？</h3><p>比较两个句子：</p>
<ol>
<li>The animal didn’t cross the street because it was too tired.</li>
<li>The animal didn’t cross the street because it was too narrow.</li>
</ol>
<p>我们要让计算机识别这两个it具体指代的是animal还是street，我们考虑对it编码时，前后文各个词在该编码中所占的比例，也就是要把上下文语境融入到当前计算的词向量中，这就是self-attention。</p>
<p>大概长得像这样：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153124996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>
</blockquote>
<hr>
<h2 id="2-Transformer-架构分析"><a href="#2-Transformer-架构分析" class="headerlink" title="2 Transformer 架构分析"></a>2 Transformer 架构分析</h2><p>我们从整体层面来看，输入一个句子，经过Transformer处理后，输出另一个句子：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153137132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p>把Transformer稍微拆开一点，我们看到了句子输入后先由Encoder处理，经过某些转换，再由Decoder输出：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153146802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p>事实上，中间的Encoder-Decoder可能是由更多的Encoders-Decoders拼接起来的，看起来像这样：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153155158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p>每一个Encoder都是独立的，训练的时候权重参数可能完全不同，每一个Encoder都被分为两个子模块 ——— Self-Attention和前馈神经网络。</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153203901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p>使用Embedding Algorithm将每个单词转化为词向量：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153212169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p>然后把词向量依次流入两个组件中，输出向量作为下一个Encoder的输入向量</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153240878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200601153240708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="3-细看Self-Attention机制"><a href="#3-细看Self-Attention机制" class="headerlink" title="3 细看Self-Attention机制"></a>3 细看Self-Attention机制</h2><p><strong>第一步</strong>：对每个单词进行计算得到Queries，Keys，Values三个向量</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153310517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p><strong>第二步</strong>：使用单词A的Queries与单词A、C、D……的Keys计算出单词A与单词A、B、C、D……的相关程度，这个相关程度决定了我们编码该单词时需要考虑其他单词的比例：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153325830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p><strong>第三步</strong>：将上面得到的Score除以向量的维度（为了让梯度更稳定），Softmax规范化转化为概率：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153335268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<p><strong>第四步</strong>：将该单词对应的每个单词的Softmax值分别乘上每个单词的Value，加和得到最终结果：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153343433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<h2 id="4-一点说明"><a href="#4-一点说明" class="headerlink" title="4 一点说明"></a>4 一点说明</h2><p><strong>ADDITION1</strong>：事实上上面的操作我们可以向量化表示，这样我们就实现了前面说的效率更高的并行计算：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153353255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>ADDITION2</strong>：我们可以用一个公式概括上面的步骤：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153401474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>ADDITION3</strong>： Mulit-head attention：</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153411869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>ADDITION4</strong>：总结图</p>
<p><img src="https://img-blog.csdnimg.cn/20200601153420715.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>JAVA基础（二）—— HashMap（更新中）</title>
    <url>/posts/ece897f8.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title>JAVA基础（一）—— String</title>
    <url>/posts/3b6698e9.html</url>
    <content><![CDATA[<p>几乎所有JAVA面试都是以String开始的，因此掌握String知识点是至关重要的，下面我们以问题的形式与大家一起逐步解析String源码。</p>
<h1 id="String是如何实现的，它有哪些重要的方法？"><a href="#String是如何实现的，它有哪些重要的方法？" class="headerlink" title="String是如何实现的，它有哪些重要的方法？"></a>String是如何实现的，它有哪些重要的方法？</h1><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><p>以主流的JDK版本1.8来说，String内部实际存储结构为参数组，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">Comparable</span>&lt;<span class="title">String</span>&gt;, <span class="title">CharSequence</span></span>&#123;</span><br><span class="line">    <span class="comment">// 用于存储字符串的值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> value[];</span><br><span class="line">    <span class="comment">// 缓存字符串的hash code</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> hash; <span class="comment">//Default to 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="重要方法"><a href="#重要方法" class="headerlink" title="重要方法"></a>重要方法</h2><h3 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h3><p>String的构造方法有以下四种，其中最容易被忽略的是后两种：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// String 为参数的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(String original)</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.value = original.value;</span><br><span class="line">    <span class="keyword">this</span>.hash = original.hash;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// char[] 为参数的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(<span class="keyword">char</span> value)</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.value = Array.copyOf(value, value.length);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StringBuffer 为参数的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(StringBuffer buffer)</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(buffer)&#123;</span><br><span class="line">        <span class="keyword">this</span>.value = Arrays.copyOf(buffer.getValue(), buffer.length());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StringBuilder 为参数的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">(StringBuilder builder)</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.value = Arrays.copyOf(builder.getValue(),builder.length());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="equals比较字符串是否相等"><a href="#equals比较字符串是否相等" class="headerlink" title="equals比较字符串是否相等"></a>equals比较字符串是否相等</h3><p>源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object anObject)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 对象引用相同时直接返回true</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span> == anObject)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断需要对比的值是否为String类型，如果不是直接返回false</span></span><br><span class="line">    <span class="keyword">if</span>(anObject <span class="keyword">instanceof</span> String)&#123;</span><br><span class="line">        String anotherString = (String)anObject;</span><br><span class="line">        <span class="keyword">int</span> n = value.length;</span><br><span class="line">        <span class="keyword">if</span>(n == anotherString.value.length)&#123;</span><br><span class="line">            <span class="keyword">char</span> v1[] = value;</span><br><span class="line">            <span class="keyword">char</span> v2[] = anotherString value;</span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(n != <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(v1[i]!=v2[i])&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="instanceof判断类型"><a href="#instanceof判断类型" class="headerlink" title="instanceof判断类型"></a>instanceof判断类型</h2><p>instanceof 的使用如下：<br>当判断参数为String类型后，会循环对比两个字符串中的每一个字符，当所有字符都相等时返回true,否则返回false</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Object oString = <span class="string">"123"</span></span><br><span class="line">Object oint = <span class="number">123</span>;</span><br><span class="line">System.out.println(oString <span class="keyword">instanceof</span> String); <span class="comment">// 返回true</span></span><br><span class="line">System.out.println(oint <span class="keyword">instanceof</span> String); <span class="comment">//返回false</span></span><br></pre></td></tr></table></figure>
<h2 id="compareto-比较两个字符串"><a href="#compareto-比较两个字符串" class="headerlink" title="compareto() 比较两个字符串"></a>compareto() 比较两个字符串</h2><p>返回值为int类型，正数，负数或0，compareTo()会循环对比所有字符，当两个字符串有一个字符不相等时，返回char1-char2。源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(String anotherString)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len1 = value.length;</span><br><span class="line">    <span class="keyword">int</span> len2 = anotherString.value.length;</span><br><span class="line">    <span class="comment">// 获取到连个字符串长度最短的那个int值</span></span><br><span class="line">    <span class="keyword">int</span> lim = Math.min(len1, len2);</span><br><span class="line">    <span class="keyword">char</span> v1[] = value;</span><br><span class="line">    <span class="keyword">char</span> v2[] = anotherString.value;</span><br><span class="line">    <span class="keyword">int</span>  k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(k&lt;lim)&#123;</span><br><span class="line">        <span class="keyword">char</span> c1 = v1[k];</span><br><span class="line">        <span class="keyword">char</span> c2 = v2[k];</span><br><span class="line">        <span class="keyword">if</span>(c1 != c2)&#123;</span><br><span class="line">            <span class="comment">// 有字符串不相等则返回差值</span></span><br><span class="line">            <span class="keyword">return</span> c1 - c2;</span><br><span class="line">        &#125;</span><br><span class="line">        k++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> len1-len2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>compareTo和Equal两个方法都可以使用ignoreCase忽略大小写,这两个方法都是用于比较两个字符串的，但他们有两点不同：</p>
<ul>
<li>equals()可以接收一个Object类型的参数，但compareTo()只能接收一个String类型的参数</li>
<li>equals()返回之为Boolean，而compareTo返回值则为int</li>
</ul>
<p>当euqals()方法返回true时，或是compareTo()方法返回0时，则表示两个字符串完全相同</p>
<h2 id="其他常用方法"><a href="#其他常用方法" class="headerlink" title="其他常用方法"></a>其他常用方法</h2><ul>
<li>indexOf()：查询字符串首次出现 的下标位置</li>
<li>lastIndexOf()：查询字符串最后出现的下标位置</li>
<li>contains()：查询字符串中是否包含另一字符串</li>
<li>toLowerCase()：把字符串全部转换成小写</li>
<li>toUpperCase()：把字符串全部转化成大写</li>
<li>length()：查询字符串的长度</li>
<li>trim()：去掉字符串首尾空格 </li>
<li>repplace()：替换字符串中某些字符</li>
<li>split()：把字符串分割并返回字符串类型</li>
<li>join()：把字符串数组转为字符串</li>
</ul>
<h1 id="String关联的其他问题："><a href="#String关联的其他问题：" class="headerlink" title="String关联的其他问题："></a>String关联的其他问题：</h1><h2 id="为什么String类型要用final修饰？"><a href="#为什么String类型要用final修饰？" class="headerlink" title="为什么String类型要用final修饰？"></a>为什么String类型要用final修饰？</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br></pre></td></tr></table></figure>
<p>为什么这里要使用final类呢？<br>Java语言之父的回答是：他更倾向使用final，因为它能够缓存结果，当你在传参时不需要考虑谁会修改它的值。如果是可变类的话，则更有可能 要重新拷贝出一个新值进行传参，这样在 性能上 就有一些损失。迫使String类设计成不可变的另一个原因是安全，如果是可变类，有可能在校验之后类型就改变了，有可能 会引起严重的系统崩溃问题。<br>总结来说：<strong>安全+高效</strong>是使String类设置为不可变的一个重要原因</p>
<h2 id="和equal的区别是什么？"><a href="#和equal的区别是什么？" class="headerlink" title="$==$和equal的区别是什么？"></a>$==$和equal的区别是什么？</h2><p>$==$对于基本数据类型来说，是用于比较”值“是否相等的，而对于引用类型来说，是用于比较引用地址是否相同的，查看源码可以发现equal方法是源于Object的，对于Object类型来说，equal的内部实现就是$==$：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">this</span>==obj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而String重写了Object类型的equal方法，用于比较两个字符串的值是否相等，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object anObject)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>==anObject)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(anObject <span class="keyword">instanceof</span> String)&#123;</span><br><span class="line">        String anotherString = (String)anObject;</span><br><span class="line">        <span class="keyword">int</span> n=value.length;</span><br><span class="line">        <span class="keyword">if</span>(n == anotherString.value.length)&#123;</span><br><span class="line">            <span class="keyword">char</span> v1[] = value;</span><br><span class="line">            <span class="keyword">char</span> v2[] = anotherString.value;</span><br><span class="line">            <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">            <span class="comment">// 循环比对两个字符串的每一个字符</span></span><br><span class="line">            <span class="keyword">while</span>(n--!= <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="comment">// 如果其中有一个字符不相等就return false，否则继续比对</span></span><br><span class="line">                <span class="keyword">if</span>(v1[i]!=v2[i])&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="String和StringBuffer、StringBuilder有什么区别？"><a href="#String和StringBuffer、StringBuilder有什么区别？" class="headerlink" title="String和StringBuffer、StringBuilder有什么区别？"></a>String和StringBuffer、StringBuilder有什么区别？</h2><p>String类型是不可变的，因此在字符串拼接时使用String的话性能 会降低，因此需要使用另一个数据类型 StringBuffer，它提供了append和insert用于字符串的拼接，它使用synchronized来保证线程安全，如下源码所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> StringBuffer <span class="title">append</span><span class="params">(Object obj)</span></span>&#123;</span><br><span class="line">    toStringCache = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">super</span>.append(String.valueOf(obj));</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> StringBuffer <span class="title">append</span><span class="params">(String str)</span></span>&#123;</span><br><span class="line">    toStringCache = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">super</span>.append(str);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但又因为使用了synchroniized，导致性能不是很高，因此在JDK1.5时有了StringBuilder，它未使用synchroniized来修饰，因此性能上优于StringBuffer，因此在非并发情况下 可以使用StringBuilder来进行 字符串的拼接</p>
<h2 id="String的intern-方法有什么含义？String在JVM中是如何存储的？"><a href="#String的intern-方法有什么含义？String在JVM中是如何存储的？" class="headerlink" title="String的intern()方法有什么含义？String在JVM中是如何存储的？"></a>String的intern()方法有什么含义？String在JVM中是如何存储的？</h2><p>String常见的常见方式有以下两种：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="string">"java"</span>;</span><br><span class="line">String s2 = <span class="keyword">new</span> String(<span class="string">"java"</span>);</span><br></pre></td></tr></table></figure>
<p>这两者在JVM的存储区域截然不同，在JDK1.8中，变量s1会先去字符串常量池中找字符串”java”，如果有相同的字符，则直接返回常量句柄，如果没有该字符串，则会在常量池中创建该字符串，然后再返回常量句柄。而变量s2直接在堆上创建一个变量，如果调用intern方法，才会把字符串保存在常量池中，如下代码所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="string">"java"</span>;</span><br><span class="line">String s2 = <span class="keyword">new</span> String(<span class="string">"java"</span>);</span><br><span class="line">String s3 = s2.intern();</span><br><span class="line">System.out.println(s2==s3);</span><br><span class="line">System.out.println(s1==s3);</span><br></pre></td></tr></table></figure>
<p>从JDK1.7后把永生代换成元空间，把字符串常量池从方法区移到了Java堆上</p>
]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title>逆波兰表达式</title>
    <url>/posts/93bfc68e.html</url>
    <content><![CDATA[<h1 id="运算表达式"><a href="#运算表达式" class="headerlink" title="运算表达式"></a>运算表达式</h1><p>一个表达式由操作数、操作符和分界符组成，算术表达式：</p>
<ul>
<li>中缀表达式(infix)：A+B</li>
<li>前缀表达式(prefix)：+AB</li>
<li>后缀表达示(postfix)：AB＋</li>
</ul>
<p><strong>表达式中相邻两个操作符的计算次序为</strong>：</p>
<ul>
<li>优先级高的先计算</li>
<li>优先级相同的自左向右计算</li>
<li>当使用括号时从最内层括号开始计算</li>
</ul>
<h2 id="中缀表达式"><a href="#中缀表达式" class="headerlink" title="中缀表达式"></a>中缀表达式</h2><p>中缀表达式需使用括号表示优先级，而后缀表达式不用，本身已含有优先级的信息。考虑下面例子：</p>
<blockquote>
<p>a b c d - * + e f / -</p>
</blockquote>
<p>从左向右，a，b，c，d分别压入堆栈，看到 “-“，将c与d提取出来做减法运算，得到rst1压入堆栈，看到”*“，将rst1与b做乘法运算，得到rst2压入堆栈，看到”+”，将rst2与a提取出来做加法，得到rst3压入堆栈，再将e与f分别压入堆栈，看到”/“，将e与f做除法，得到rst4压入堆栈，看到”-“，将rst3与rst4提取出来做减法，得到rst5压入堆栈，结束。</p>
<p>总结一下中缀表达式的计算顺序：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(is number)&#123;</span><br><span class="line">	push(number)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line">	pop(top element)</span><br><span class="line">    calculate(top element, number)</span><br><span class="line">    push(result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="中缀表达式转后缀表达式"><a href="#中缀表达式转后缀表达式" class="headerlink" title="中缀表达式转后缀表达式"></a>中缀表达式转后缀表达式</h2><p>由于后缀表达式计算方便，我们需要考虑将中缀表达式转为后缀表达式，具体方式为：</p>
<ul>
<li>操作符栈初始化，将结束符”；“进栈，然后读入中缀表达式字符流 的首字符ch。</li>
<li>重复执行以下步骤，直到ch=”；”，同时栈顶的操作符也是”；”，停止循环。<ul>
<li>若ch是操作符直接输出，读入下一个字符ch；</li>
<li>若ch是操作符，判断ch的优先级icp和位于栈顶的操作符op的优先级isp：<ul>
<li>若icp(ch) &gt; isp(op)，令ch进栈，读入下一个字符ch；</li>
<li>若ip(ch) &lt; isp(op)，退栈并输出;</li>
<li>若icp(ch) == isp(op)，退栈但不输出，若输出的是左括号则读入下一个字符ch。</li>
</ul>
</li>
<li>算法结束，输出序列即为所需的后缀表达式。</li>
</ul>
</li>
</ul>
<p>转换时的操作符的优先级如下：</p>
<p><img src="/posts/Cpp/stack1.png" alt></p>
<p>看一个具体的例子：</p>
<p><img src="/posts/Cpp/stack2.png" alt></p>
<p><img src="/posts/Cpp/stack3.png" alt></p>
<h1 id="具体代码实现"><a href="#具体代码实现" class="headerlink" title="具体代码实现"></a>具体代码实现</h1><h2 id="中缀表达式直接求值"><a href="#中缀表达式直接求值" class="headerlink" title="中缀表达式直接求值"></a>中缀表达式直接求值</h2><p>下面是中缀表达式直接求值的C++实现（在定义了栈的相关操作的基础上实现）：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Judge</span><span class="params">(<span class="keyword">char</span> c1,<span class="keyword">char</span> c2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> a1,a2;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'+'</span>==c1||<span class="string">'-'</span>==c1) a1 = <span class="number">3</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'*'</span>==c1||<span class="string">'/'</span>==c1)a1 = <span class="number">5</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'('</span>==c1) a1 = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">')'</span>==c1) a1 = <span class="number">7</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'#'</span>==c1) a1 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'+'</span>==c2||<span class="string">'-'</span>==c2)a2 = <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'*'</span>==c2||<span class="string">'/'</span>==c2)a2 = <span class="number">4</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'('</span>==c2) a2 = <span class="number">6</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">')'</span>==c2) a2 = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">if</span>(<span class="string">'#'</span>==c2) a2 = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">if</span>(a1&gt;a2) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">if</span>(a1==a2) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">if</span>(a1&lt;a2) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">run</span><span class="params">(<span class="keyword">char</span> c ,<span class="keyword">double</span> d1,<span class="keyword">double</span> d2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">switch</span> (c)</span><br><span class="line">	&#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="string">'+'</span>:</span><br><span class="line">		<span class="keyword">return</span> d1+d2;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">case</span> <span class="string">'-'</span>:</span><br><span class="line">		<span class="keyword">return</span> d1-d2;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">case</span> <span class="string">'*'</span> :</span><br><span class="line">		<span class="keyword">return</span> d1*d2;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">case</span> <span class="string">'/'</span>:</span><br><span class="line">		<span class="keyword">return</span> d1/d2;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">default</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0.0</span>;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">infix</span><span class="params">(<span class="built_in">string</span> str)</span></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> * op = (<span class="keyword">char</span>*)<span class="string">"+-*/()#"</span>;</span><br><span class="line">    str.append(<span class="number">1</span>, <span class="string">'#'</span>);</span><br><span class="line">    SeqStack&lt;<span class="keyword">char</span>&gt; oper;</span><br><span class="line">    SeqStack&lt;<span class="keyword">double</span>&gt; num;</span><br><span class="line">    <span class="keyword">int</span> a=<span class="number">-1</span>;</span><br><span class="line">    oper.Push(<span class="string">'#'</span>);</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">		<span class="keyword">int</span> b = a+<span class="number">1</span>;</span><br><span class="line">		a = str.find_first_of(op, a+<span class="number">1</span>);</span><br><span class="line">		<span class="keyword">if</span>(a==<span class="built_in">string</span>::npos) <span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">if</span>(a!=b)&#123;</span><br><span class="line">			<span class="function"><span class="built_in">string</span> <span class="title">ss</span><span class="params">(str,b,a-b)</span></span>;</span><br><span class="line">			<span class="keyword">double</span> d=atof(ss.c_str());</span><br><span class="line">			<span class="comment">//数据先入栈</span></span><br><span class="line">			num.Push(d);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">int</span> ju = Judge(oper.GetTop(),str[a]);</span><br><span class="line">		<span class="keyword">if</span>(<span class="number">-1</span>==ju)&#123;</span><br><span class="line">			oper.Push(str[a]);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(<span class="number">0</span>==ju)&#123;</span><br><span class="line">			oper.Pop();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(<span class="number">1</span>==ju)&#123;</span><br><span class="line">			<span class="keyword">double</span> d1 = num.GetTop();</span><br><span class="line">			num.Pop();</span><br><span class="line">			<span class="keyword">double</span> d2 = num.GetTop();</span><br><span class="line">			num.Pop();</span><br><span class="line">			d1 = run(oper.GetTop(),d2,d1);</span><br><span class="line">			num.Push(d1);</span><br><span class="line">			oper.Pop();</span><br><span class="line">			a--;</span><br><span class="line">	 	&#125;</span><br><span class="line">    &#125;</span><br><span class="line">	str.erase(str.length()<span class="number">-1</span>,<span class="number">1</span>);</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;str&lt;&lt;<span class="string">" = "</span>&lt;&lt;num.GetTop()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序设计思路如下：</p>
<ol>
<li>首先定义了Judge函数，用于判断运算符的优先级次序</li>
<li>定义run函数，统一运算符的操作模式，避免写整段的运算符判断语句</li>
<li>定义infix中缀表达式主函数，具体实现步骤为<ul>
<li>定义运算符，目的是方便后续找到表达式字符串中的运算符，另外在运算符栈中先插入“#“终止字符。</li>
<li>循环，使用表达式：a = str.find_first_of(op, a+1)不断找到下一个位置的运算符，将两个运算符（或最开始）的连续的数字字符串通过atof函数转化为一个数字，然后将数字压入num栈中，并根据Judge函数比较得到的下一个运算符与运算符栈顶元素的优先级，分别采取压入栈、弹出栈及取数运算操作，取数运算通过将数字栈中弹出的两个数字以及对应的运算符通过run函数实现。需要注意的是，由于此时数字栈顶被压入了新的元素，需在最后使用a—。</li>
</ul>
</li>
</ol>
<h2 id="中缀表达式转后缀表达式求值"><a href="#中缀表达式转后缀表达式求值" class="headerlink" title="中缀表达式转后缀表达式求值"></a>中缀表达式转后缀表达式求值</h2><p>这里采用与中缀表达式直接求值不同的实现方式，在程序函数的调用方面有些许区别，事实上也可以直接通过中缀表达式定义好的函数实现这一步骤，读者可以自行完成。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">IsOperator</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> ops[] = <span class="string">"+-*/"</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">sizeof</span>(ops) / <span class="keyword">sizeof</span>(<span class="keyword">char</span>); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (ch == ops[i])</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Precedence</span><span class="params">(<span class="keyword">char</span> op1, <span class="keyword">char</span> op2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (op1 == <span class="string">'('</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (op1 == <span class="string">'+'</span> || op1 == <span class="string">'-'</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (op2 == <span class="string">'*'</span> || op2 == <span class="string">'/'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (op1 == <span class="string">'*'</span> || op1 == <span class="string">'/'</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (op2 == <span class="string">'+'</span> || op2 == <span class="string">'-'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">in2postfix</span><span class="params">(<span class="built_in">string</span> s)</span></span>&#123;</span><br><span class="line">	<span class="built_in">string</span> postFix;</span><br><span class="line">	<span class="keyword">int</span> j = <span class="number">0</span>, len;</span><br><span class="line">    <span class="keyword">char</span> c;</span><br><span class="line">    SeqStack&lt;<span class="keyword">char</span>&gt; st;</span><br><span class="line">    len = s.length();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">        c = s[i];</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">'('</span>)&#123;</span><br><span class="line">			st.Push(c);</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">')'</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span> (st.GetTop() != <span class="string">'('</span>)&#123;</span><br><span class="line">                postFix[j++] = st.GetTop();</span><br><span class="line">                st.Pop();</span><br><span class="line">            &#125;</span><br><span class="line">            st.Pop();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (!IsOperator(c))&#123;</span><br><span class="line">				st.Push(c);</span><br><span class="line">			&#125; </span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">while</span> (st.Empty() == <span class="literal">false</span> &amp;&amp; Precedence(st.GetTop(), c) &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">                    postFix[j++] = st.GetTop();</span><br><span class="line">                    st.Pop();</span><br><span class="line">                &#125;</span><br><span class="line">                st.Push(c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (st.Empty() == <span class="number">0</span>)&#123;</span><br><span class="line">        postFix[j++] = st.GetTop();</span><br><span class="line">        st.Pop();</span><br><span class="line">    &#125;</span><br><span class="line">    postFix[j] = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">return</span> postFix;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">postFixEval</span><span class="params">(<span class="keyword">char</span>* postFix)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    SeqStack&lt;<span class="keyword">char</span>&gt; st;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">strlen</span>(postFix);</span><br><span class="line">    <span class="keyword">char</span> c;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        c = postFix[i];</span><br><span class="line">        <span class="keyword">if</span> (IsOperator(c) == <span class="literal">false</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            st.Push(c - <span class="string">'0'</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">char</span> op1, op2;</span><br><span class="line">            <span class="keyword">int</span> val;</span><br><span class="line"></span><br><span class="line">            op1 = st.GetTop();</span><br><span class="line">            st.Pop();</span><br><span class="line">            op2 = st.GetTop();</span><br><span class="line">            st.Pop();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">switch</span> (c)</span><br><span class="line">            &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'+'</span>:</span><br><span class="line">                val = op1 + op2;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'-'</span>:</span><br><span class="line">                val = op2 - op1;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'*'</span>:</span><br><span class="line">                val = op1 * op2;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'/'</span>:</span><br><span class="line">                val = op2 / op1;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            st.Push(val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> st.GetTop();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="栈的定义与实现代码"><a href="#栈的定义与实现代码" class="headerlink" title="栈的定义与实现代码"></a>栈的定义与实现代码</h2><p>最后附上上述代码中需使用到的栈结构的定义：</p>
<p>这是头函数SeqStack.h文件：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> SEQSTACK_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SEQSTACK_H</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    DataType data;</span><br><span class="line">    Node&lt;DataType&gt; *next;  <span class="comment">//此处&lt;T&gt;也可以省略</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;       //定义模板类<span class="title">SeqStack</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkStack</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkStack() ;            <span class="comment">//构造函数，栈的初始化</span></span><br><span class="line">	~LinkStack();            <span class="comment">//析构函数</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Push</span><span class="params">(DataType x)</span></span>;          <span class="comment">//将元素x入栈</span></span><br><span class="line">    <span class="function">DataType <span class="title">Pop</span><span class="params">()</span></span>;                <span class="comment">//将栈顶元素弹出</span></span><br><span class="line">    <span class="function">DataType <span class="title">GetTop</span><span class="params">()</span></span>;	         <span class="comment">//取栈顶元素（并不删除）</span></span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">Empty</span><span class="params">()</span></span>;           <span class="comment">//判断栈是否为空</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node&lt;DataType&gt; *top;                <span class="comment">//栈顶指针，指示栈顶元素在数组中的下标</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p>
<p>这是stack.cpp文件：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"SeqStack.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">SeqStack</span>&lt;DataType&gt;:</span>:SeqStack()&#123;</span><br><span class="line">	top=<span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">SeqStack</span>&lt;DataType&gt;:</span>:~SeqStack()&#123;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt; </span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">SeqStack</span>&lt;DataType&gt;:</span>:Push(DataType x)&#123;</span><br><span class="line">    <span class="keyword">if</span> (top==StackSize<span class="number">-1</span>) <span class="keyword">throw</span> <span class="string">"error"</span>;</span><br><span class="line">    top++;</span><br><span class="line">    data[top]=x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqStack</span>&lt;DataType&gt;:</span>:Pop()&#123; </span><br><span class="line">    DataType x;</span><br><span class="line">    <span class="keyword">if</span> (top==<span class="number">-1</span>) <span class="keyword">throw</span> <span class="string">"error"</span>;</span><br><span class="line">    x=data[top--];</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt; </span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqStack</span>&lt;DataType&gt;:</span>:GetTop()&#123;</span><br><span class="line">	<span class="keyword">if</span> (top!=<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> data[top];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt; </span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">SeqStack</span>&lt;DataType&gt;:</span>:Empty()&#123;</span><br><span class="line">	<span class="keyword">if</span>(top==<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt; </span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">SeqStack</span>&lt;DataType&gt;:</span>:NotEmpty()&#123;</span><br><span class="line">	<span class="keyword">if</span>(top==<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">to_Xhex</span><span class="params">(<span class="keyword">int</span> data, <span class="keyword">int</span> tohex)</span></span>&#123;</span><br><span class="line">    SeqStack&lt;<span class="keyword">int</span>&gt; S;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> t = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(data&gt;=tohex)&#123;</span><br><span class="line">        S.Push(data%tohex);</span><br><span class="line">        data/=tohex;</span><br><span class="line">    &#125;</span><br><span class="line">    S.Push(data);</span><br><span class="line">    <span class="keyword">while</span>(S.NotEmpty())&#123;</span><br><span class="line">        count += t * S.GetTop();</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;S.Pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title>C++指针详解</title>
    <url>/posts/2df51814.html</url>
    <content><![CDATA[<p>C++指针大概是许多初学者最不愿意接触的 东西，但是通过指针，可以简化一些 C++ 编程任务的执行，还有一些任务，如动态内存分配，没有指针是无法执行的，因此指针始终是学习C++无法绕过的一个弯，那么什么是指针？指针是一个变量，其值为另一个变量的地址，即，内存位置的直接地址。就像其他变量或常量一样，您必须在使用指针存储其他变量地址之前，对其进行声明。</p>
<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>这一个例子是作为备忘，以防啥时候忘了咋使用指针或引用便于随时翻看。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span> j = <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">int</span>* ip = &amp;i;</span><br><span class="line">	<span class="keyword">int</span>* jp = &amp;j;</span><br><span class="line">	<span class="keyword">int</span>** ipp = &amp;ip;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"address of i:%p\r\n"</span>, &amp;i);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"==========================\r\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"value of ip:%p\r\n"</span>, ip);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"address of ip:%p\r\n"</span>, &amp;ip);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"content of ip:%d\r\n"</span>, *ip);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"==========================\r\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"value of ipp:%p\r\n"</span>, ipp);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"address of ipp:%p\r\n"</span>, &amp;ipp);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"content of ip:%p\r\n"</span>, *ipp);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"==========================\r\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"value of i:%d\r\n"</span>, **ipp);</span><br><span class="line">	*ipp = &amp;j;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"value of j:%d\r\n"</span>, **ipp);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码运行结果是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">address of i:000000000061fe14</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">value of ip:000000000061fe14</span><br><span class="line">address of ip:000000000061fe08</span><br><span class="line">content of ip:1</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">value of ipp:000000000061fe08</span><br><span class="line">address of ipp:000000000061fe00</span><br><span class="line">content of ip:000000000061fe14</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">value of i:1</span><br><span class="line">value of j:2</span><br></pre></td></tr></table></figure>
<p>In brief，在变量前（可以是指针变量）加一个&amp;，就可以找到该变量的地址！对指针加一个*，就可以找到该（指针对应的）地址对应的值，下面我们开始深入理解指针~</p>
<h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1><p>我们先给出一个断言：物理内存的单位是字节,一个字节8个位，一个字节的表示范围为0000 0000~1111 1111(oxff)，实际上往往还会多出一个位用于校验。<br>由下图可见，我们每个（不同类型）变量占的字节大小不同</p>
<p><img src="/posts/Cpp/memory.png" alt></p>
<p>首先我们定义两个变量：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">char</span> a;</span><br></pre></td></tr></table></figure>
<p>创建两个变量后，变量的空间占用如下（以下所有的地址都只是简化版本，实际上为8个位的字节）</p>
<p><img src="/posts/Cpp/memory2.PNG" alt></p>
<p>当我们对变量进行赋值操作：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">i = <span class="number">30</span>;</span><br><span class="line">a = ’t’;</span><br></pre></td></tr></table></figure>
<p>这时可以理解为：</p>
<p><img src="/posts/Cpp/memory3.PNG" alt></p>
<p>下面我们定义一个指针：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *pi;</span><br></pre></td></tr></table></figure>
<p>它会在内存中体现为：</p>
<p><img src="/posts/Cpp/memory4.PNG" alt></p>
<p>那么给这个变量赋值，赋的值是变量i的地址：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pi = &amp;i;</span><br></pre></td></tr></table></figure>
<p><img src="/posts/Cpp/memory5.PNG" alt></p>
<p>注意看这个变量值为6，而变量i的内存地址也为6，也就是说将i的内存地址保存为pi变量的值，这是不是有点静态链表的内味了。</p>
<p>总而言之：指针变量所存的内容就是内存的地址编号。</p>
]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title>k8s 核心技术</title>
    <url>/posts/44e24bf5.html</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer">

<h1 id="认识-kubernetes-架构及应用场景"><a href="#认识-kubernetes-架构及应用场景" class="headerlink" title="认识 kubernetes 架构及应用场景"></a>认识 kubernetes 架构及应用场景</h1><h2 id="kubernetes（k8s-在企业中的应用场景"><a href="#kubernetes（k8s-在企业中的应用场景" class="headerlink" title="kubernetes（k8s) 在企业中的应用场景"></a>kubernetes（k8s) 在企业中的应用场景</h2><h3 id="构建自动化运维平台"><a href="#构建自动化运维平台" class="headerlink" title="构建自动化运维平台"></a>构建自动化运维平台</h3><p>（1） 中小型企业，使用 k8s 构建一套自动化运维平台（降本增效）<br>（2） 大型互联网公司更要使用，实现更高效的运作方式</p>
<h3 id="充分利用服务器资源"><a href="#充分利用服务器资源" class="headerlink" title="充分利用服务器资源"></a>充分利用服务器资源</h3><p>怎么理解充分利用服务器资源这一条呢，我们举个例子：假设并发量200请求，服务器是两核心CPU，4G内存，其中静态请求150个，比如访问CDN、nginx、缓存等，动态请求50个，主要是访问数据库，把数据读入内存，我们来估算服务器资源（只考虑内存，不考虑程序员RT与CPU切换时间）：</p>
<p>静态+动态 = 150*2M + 50 *10M = 800M<br>也就是说200个请求占用了800M内存</p>
<p>我们可以估算这台服务器的并发能力QPS为 200*4 = 8000，但实际上 800 QPS 是无法达到的，我们需要考虑 response time，cpu切换时间，内存等等，因此实际上我们会给它安排 300 QPS，此时先然无法充分利用服务器资源，有很多资源被浪费掉了。而 k8s 使用类似 Docker 的容器化方式，也就是我们在其中部署多个容器，将服务器资源做隔离，容器中部署的就是 web 服务。</p>
<h3 id="服务无缝衔接"><a href="#服务无缝衔接" class="headerlink" title="服务无缝衔接"></a>服务无缝衔接</h3><p>那么服务的无缝迁移又该怎么理解呢？首先我们开发时应该有三种环境，分别是是开发环境、测试环境和生产环境。在开发 web 项目时有许多配套服务，在测试环境中这些环境也必须存在，测试完之后产品就可以上线，但这时会出现一个问题：项目在测试时没有毛病，但一旦上市生产就出现了很大的问题，这经常是环境的不一致性造成的，这时候往往就会花费大量精力去调试环境，非常麻烦，而容器化方式可以做到服务无缝迁移。也就是说我们可以把 JDK、MQ、ES、MySQL 等做成一个个镜像，这些镜像可以脱离我们的依赖环境，因此这些镜像可以做到无缝迁移。</p>
<h2 id="服务部署模式变迁-amp-服务部署模式变化的思考"><a href="#服务部署模式变迁-amp-服务部署模式变化的思考" class="headerlink" title="服务部署模式变迁&amp;服务部署模式变化的思考"></a>服务部署模式变迁&amp;服务部署模式变化的思考</h2><p>我们考虑下面几个问题：</p>
<h3 id="服务部署模式是怎么变迁的。"><a href="#服务部署模式是怎么变迁的。" class="headerlink" title="服务部署模式是怎么变迁的。"></a>服务部署模式是怎么变迁的。</h3><ol>
<li>物理机部署：就是直接把服务部署到物理机上（不能充分利用物理机资源）</li>
<li>虚拟化（虚拟机）方式，也就是通过虚拟机将物理资源进行隔离部署服务，将服务部署到虚拟机上，但虚拟机本身就非常占用资源，因此我们寻求一种更好的方式</li>
<li>使用容器化方式进行部署（容器更轻量级，运行更快）</li>
</ol>
<h3 id="服务部署模式变化，带来哪些问题？"><a href="#服务部署模式变化，带来哪些问题？" class="headerlink" title="服务部署模式变化，带来哪些问题？"></a>服务部署模式变化，带来哪些问题？</h3><ol>
<li><p>前提条件：SOA架构，微服务架构的模式下，项目拆分越来越多，服务越来越多，这么多服务我们是怎么管理？</p>
<ul>
<li><p>虚拟机服务部署方式（openstack）</p>
</li>
<li><p>容器化部署模式（k8s）</p>
<p>容器我们可以认为是一个更轻量级的虚拟机，使用了与虚拟机不同的技术，因此与openstack用于管理虚拟机类似， k8s 就是用来管理容器的。</p>
</li>
</ul>
</li>
<li><p>面临问题：</p>
<ul>
<li><p>如何对服务进行横向扩展（不能 简单地加机器，会影响服务）</p>
</li>
<li><p>容器宕机如何解决，数据怎么恢复</p>
</li>
<li><p>重新发布新的版本如何在线上快速更新，更新后不影响业务（k8s可以做滚动更新）</p>
</li>
<li><p>如何监控容器（容器出现问题怎么办）</p>
</li>
<li><p>容器如何调度创建</p>
</li>
<li><p>数据安全性如何保证</p>
</li>
</ul>
</li>
</ol>
<h2 id="云架构-amp-云原生"><a href="#云架构-amp-云原生" class="headerlink" title="云架构 &amp; 云原生"></a>云架构 &amp; 云原生</h2><h3 id="云和-k8s-是什么关系"><a href="#云和-k8s-是什么关系" class="headerlink" title="云和 k8s 是什么关系"></a>云和 k8s 是什么关系</h3><ul>
<li>云就是使用容器构建的一套服务集群网络，云由大量容器构成，不同容器有不同功能</li>
<li>k8s 就是用来管理云中的容器</li>
</ul>
<h3 id="云架构"><a href="#云架构" class="headerlink" title="云架构"></a>云架构</h3><h4 id="iaas-基础设施即服务"><a href="#iaas-基础设施即服务" class="headerlink" title="iaas 基础设施即服务"></a>iaas 基础设施即服务</h4><pre><code>用户：可以租用（购买|分配权限）云主机，用户就不需要考虑网络，DNS，硬件环境方面的问题。
运营商：（私有云或公有云平台）提供网络，存储，DNS（基础设施服务）
</code></pre><h4 id="paas-平台即服务"><a href="#paas-平台即服务" class="headerlink" title="paas 平台即服务"></a>paas 平台即服务</h4><pre><code>MYSQL\ES\R等服务都由平台提供了
</code></pre><h4 id="saas-软件即服务（目前很多系统都是该系统）"><a href="#saas-软件即服务（目前很多系统都是该系统）" class="headerlink" title="saas 软件即服务（目前很多系统都是该系统）"></a>saas 软件即服务（目前很多系统都是该系统）</h4><pre><code>钉钉：给每个公司提供一个系统，每个公司使用独立一套功能
财务管理软件：维护交给运营商维护，用户只需要使用其中的功能即可
</code></pre><h4 id="serverless-无服务"><a href="#serverless-无服务" class="headerlink" title="serverless 无服务"></a>serverless 无服务</h4><pre><code> 站在用户角度：不需要服务器，用户只需要使用云服务器即可，在云服务器所有基础环境 ，软件环境都不需要 用户自己安装
 未来：服务开发都是 serverless，企业都构建了自己的私有云环境，或者使用公有云环境（阿里云）
     阿里将所有服务部署到云端之后，效率提升了60%
</code></pre><h3 id="云原生"><a href="#云原生" class="headerlink" title="云原生"></a>云原生</h3><p>就是为了让应用程序（项目、服务软件）都 运行在云上的解决方案，这样的方案叫做云原生。<br><strong>特点</strong>：<br>（1）容器化 —— 所有服务部署都必须部署在容器中<br>（2）微服务 —— web 服务架构、微服务架构<br>（3）CI、CD —— 可持续交互与可持续部署<br>（4）DevOps —— 开发与运维密不可分</p>
<h2 id="kubernetes-架构原理"><a href="#kubernetes-架构原理" class="headerlink" title="kubernetes 架构原理"></a>kubernetes 架构原理</h2><p>1）kubernetes 是 Google 使用 go 语言开发，原来的系统是 borg 系统（也是云平台管理工具），Docker后来自己开发了容器管理平台 Docker Swarm，Google 表示不服，因此参照 borg 架构开发了 k8s 架构</p>
<p>2）k8s 架构</p>
<p><img src="/posts/k8s/arch1.jpg" alt></p>
<p>关系：一个 master 对应多个 node 节点</p>
<p><strong>master 节点</strong></p>
<ol>
<li>api server：k8s 网关，所有指令请求都必须经过 api server</li>
<li>scheduler：调度器，使用调度算法，将请求资源调度给某一个 node 节点</li>
<li>controller 控制器：维护 k8s 资源对象（添加、删除、更新、修改）</li>
<li>etcd：存储资源对象，服务的注册与发现</li>
</ol>
<p><strong>node 节点</strong></p>
<ol>
<li>docker：运行容器的基础环境，容器引擎</li>
<li>kuberlet：在每个 node 节点都存在一份，在 node 节点上的资源操作指令由其执行</li>
<li>kube-proxy：代理服务，负载均衡</li>
<li>fluentd：日志收集服务</li>
<li>pod：是 k8s 管理的基本单位，内部是容器，也即是 k8s 不直接管理容器，而是管理 pod</li>
</ol>
<h1 id="深入认识-kubernetes-核心组件原理"><a href="#深入认识-kubernetes-核心组件原理" class="headerlink" title="深入认识 kubernetes 核心组件原理"></a>深入认识 kubernetes 核心组件原理</h1><h2 id="pod的核心原理"><a href="#pod的核心原理" class="headerlink" title="pod的核心原理"></a>pod的核心原理</h2><p><strong>k8s的作用</strong>：k8s是用来管理容器的，但不直接操作容器，最小操作单元是pod（间接地管理容器）<br><strong>k8s的特点</strong>：</p>
<ol>
<li>一个master有一群node节点与之对应</li>
<li>master节点不存储容器，只负责调度、网关、控制器、资源对象存储</li>
<li>容器是存储在node节点（容器是存储在pod内部）</li>
<li>pod内部都可以有一个容器，或者是多个容器</li>
<li>kubelet负责本地的pod维护</li>
<li>kube-proxy负责在多个pod之间做负载均衡</li>
</ol>
<p><strong>pod是什么呢</strong>:pod也是一个容器，但这个容器中装的是docker创建的容器，也就是pod是用来封装容器的容器，pod是一个虚拟化分组（pod有自己的地址，主机名），相当于一台独立的沙箱环境（主机），可以封装一个容器或多个容器</p>
<p><strong>pod用来干什么</strong>：通常情况下，在服务部署时使用pod来管理一组相关服务（一个pod要么部署一个服务，要么部署一组相关的服务），所谓的一组相关的服务，即为链式调用的调用链路上的服务。</p>
<p><img src="/posts/k8s/pod1.png" alt></p>
<p><strong>web服务集群如何实现</strong>：只需要复制多方pod的副本即可，这也是k8s管理的先进之处，k8s如果继续扩容、缩容，只需要控制pod的数量即可</p>
<p><strong>pod底层网络，数据存储是如何进行的</strong>：pod内部容器创建之前必须先创建pause容器，服务容器之间的访问使用localhost访问，性能非常高，实际就像访问本地服务一样</p>
<p><img src="/posts/k8s/pod2.png" alt></p>
<h2 id="ReplicaSet副本控制器"><a href="#ReplicaSet副本控制器" class="headerlink" title="ReplicaSet副本控制器"></a>ReplicaSet副本控制器</h2><p><strong>什么叫做副本控制器</strong>：<strong>用于控制pod副本的数量</strong>，使副本数量与预期数量保持一致。例如，我们提前设置replicas=3（有三个副本），因此创建三个pod。当有一个pod宕机之后，k8s会立刻创建一个新的，保证副本数量等于三个，这就是副本控制器的作用——永远保证副本数量为设定值。</p>
<p>副本控制器能通过<strong>标签选择器</strong>选择维护一组相关的服务（它自己的服务），那么它要怎么判断是自己的服务呢？这里就通过标签选择，比如</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">selector:</span><br><span class="line">	app &#x3D; web</span><br><span class="line">    release &#x3D; stable</span><br></pre></td></tr></table></figure>
<p><strong>ReplicationController和ReplicaSet两个副本控制器有什么区别</strong>：</p>
<ol>
<li>ReplicaSet<ul>
<li>单选</li>
<li>复合选择</li>
</ul>
</li>
<li>ReplicationController<ul>
<li>单选</li>
</ul>
</li>
</ol>
<p>在新版的k8s中，推荐使用Replicaset作为副本控制器（功能更强大），ReplicationController不再使用</p>
<h2 id="Deployment资源部署对象"><a href="#Deployment资源部署对象" class="headerlink" title="Deployment资源部署对象"></a>Deployment资源部署对象</h2><ul>
<li>服务部署结构模型</li>
<li>滚动更新</li>
</ul>
<p>虽然在企业中我们采用ReplicaSet作为副本控制器，但在实际中项目不断更新，项目的版本将会不停的发版，版本的变化如何做到服务的更新呢？我们做的是滚动更新，如下图，每当发布一个新的版本，每更新一个POD，就要干掉原有的POD。</p>
<p><img src="/posts/k8s/Deployment.png" alt></p>
<p>那么滚动更新是由谁实现的呢？这就涉及到部署模型：因为事实上ReplicaSet是不支持滚动更新的，滚动更新是由Deployment支持的，通常两者一起使用，因此部署模型为如下结构：</p>
<p><img src="/posts/k8s/dep2.png" alt></p>
<p>如果后一个版本出现问题，k8s也可以支持向前滚动。</p>
<h2 id="StatefulSet部署有状态服务"><a href="#StatefulSet部署有状态服务" class="headerlink" title="StatefulSet部署有状态服务"></a>StatefulSet部署有状态服务</h2><p>StatefulSet和Deployment类似，区别就是StatefulSet是为了解决有状态服务容器化部署问题而产生的。</p>
<p>思考：MySQL使用容器化部署，存在怎样的问题？</p>
<ul>
<li>容器是有生命周期的，一旦宕机，数据丢失</li>
<li>pod部署：pod有生命周期，但重启pod集群副本时数据可能丢失</li>
</ul>
<p>因此容器是不太适合部署数据这样的有状态服务的，对于k8s而言，不能使用Deployment模型来部署有状态服务，通常情况下Deployment用于部署无状态服务，对于有状态服务的部署使用StatefulSet。</p>
<p>什么是有状态什么是无状态呢？</p>
<ul>
<li>有状态服务<ul>
<li>有实时的的数据需要存储</li>
<li>有状态服务集群中，把某一个服务抽离出去，一段时间后再加入机器网络，如果集群网络无法使用就被称为有状态服务</li>
</ul>
</li>
<li>无状态服务<ul>
<li>无实时的的数据需要存储</li>
<li>有状态服务集群中，把某一个服务抽离出去，一段时间后再加入机器网络，对集群网络没有影响</li>
</ul>
</li>
</ul>
<p>底层的数据存储借助PVC文件系统，而StatefulSet会保证POD重新建立后，hostname不会发生变化，POD就可以通过hostname来关联数据</p>
<h1 id="kubernetes-的服务的注册与发现（核心）"><a href="#kubernetes-的服务的注册与发现（核心）" class="headerlink" title="kubernetes 的服务的注册与发现（核心）"></a>kubernetes 的服务的注册与发现（核心）</h1><h2 id="pod在生产环境中的访问流程"><a href="#pod在生产环境中的访问流程" class="headerlink" title="pod在生产环境中的访问流程"></a>pod在生产环境中的访问流程</h2><p><strong>pod的结构</strong>：根据前面所说，pod相当于一个容器，有独立的ip地址，也有自己的hostname，利用namespace进行资源隔离，独立沙箱环境。同时pod内部封装的是容器，可以封装一个，也可以封装一组相关的容器。</p>
<p><strong>pod网络</strong>：有自己独立的ip地址，pod内部容器之间访问采用localhost访问。</p>
<h3 id="pod如何对外网提供服务"><a href="#pod如何对外网提供服务" class="headerlink" title="pod如何对外网提供服务"></a>pod如何对外网提供服务</h3><ul>
<li>前提思考：pod有PODID和hostname，pod是虚拟的资源对象（进程），没有对应的实体（物理机，物理网卡），单独的POD不能直接对外提供访问，对外提供访问一定要有物理机，通过端口访问</li>
<li>解决方案：POD如果想要对外提供访问，必须绑定物理机的端口，（在物理及上开启端口，让这个端口和POD的端口建立映射），这样就可以通过物理机进行数据包的转发</li>
<li>总结为步骤：①先通过物理机IP+port进行访问NODE节点；②数据包转发</li>
</ul>
<h3 id="pod如何实现负载均衡访问"><a href="#pod如何实现负载均衡访问" class="headerlink" title="pod如何实现负载均衡访问"></a>pod如何实现负载均衡访问</h3><p>一组相同的副本直接POD如何实现负载均衡访问，思考nginx能否做负载均衡：事实上pod是一个进程，是有生命周期的（宕机，版本更新），都会创建新的pod（ip地址发生变化，hostname发生变化）,nginx做负载均衡不太合适，因为nginx不能识别出hostname的变化，因此在pod动态变化的前提下（且由于经常做滚动更新，变化速度比较快），nginx不能发现我们的服务。因此我们使用service VIP实现负载均衡。</p>
<h4 id="利用service来实现负载均衡"><a href="#利用service来实现负载均衡" class="headerlink" title="利用service来实现负载均衡"></a>利用service来实现负载均衡</h4><ul>
<li>POD IP：pod的ip地址</li>
<li>NODE IP：物理机的IP地址</li>
<li>cluster IP：虚拟化IP，是由k8s抽象出的service对象，这个service对象是一个VIP的资源对象</li>
</ul>
<p>Kubernetes 这样定义Service ：逻辑上的一组 Pod，一种可以访问Pod的策略，通常称之为微服务。 这一组 Pod 能够被 Service 访问到，通常是通过selector实现的。当我们调用某个服务时并不关心调用了哪个Pod，对外提供服务一组的 Pod 实际上可能会发生变化(是否能提供服务，或者在销毁中，或者在创建中)，而Service 能够解耦这种关联。</p>
<p>在 Kubernetes 集群中，每个Node运行一个 kube-proxy代理进程。kube-proxy 负责为 Service实现了一种VIP(虚拟 IP)。</p>
<p>以下为Service资源对象:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-svc</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">       targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br></pre></td></tr></table></figure>
<h4 id="Service如何实现负载均衡"><a href="#Service如何实现负载均衡" class="headerlink" title="Service如何实现负载均衡"></a>Service如何实现负载均衡</h4><p>我们知道了一组相关的port做负载均衡会使用虚拟IP来做数据包转发，但service是节点（资源对象）是怎么实现数据包的转发呢？</p>
<p><strong>我们已知的</strong>：</p>
<ol>
<li>service和pod都是一个进行，因此service也不能对外网提供服务</li>
<li>service和pod之间可以直接进行通信，它们的通信属于局域网通信</li>
<li>把请求交给service后，service使用(ipstables,ipvs)来做数据包分发</li>
</ol>
<p><strong>访问步骤</strong>：</p>
<ol>
<li>在物理机上绑定端口</li>
<li>通过ip:port访问</li>
<li>访问完成之后将请求转交给service</li>
<li>service将数据包分发给相应的pod</li>
</ol>
<p><strong>service对象是如何与pod建立关联的？</strong></p>
<p>每一组相同的pod（副本）会有相同的标签，通过标签选择器（selector），service对一组相同的副本提供服务，如果是需要访问另一组，则需在创建一个service。因此不同的业务会有不同的service。然后service将对应的POD的IP地址存储到endpoints中，由此将service和相应的pod关联起来了。</p>
<p><strong>当pod宕机或者发布了新的版本，service怎么发现pod发生了变化？</strong></p>
<p>主要是依靠kube-proxy组件，k8s安装后每个节点都运行着这个组件。kube-proxy进程将监听所有的pod，一旦发现pod有变化，就会更新service中endpoint中的映射关系。</p>
]]></content>
      <categories>
        <category>Structure</category>
      </categories>
  </entry>
  <entry>
    <title>数据结构之排序技术（五）—— 分配排序</title>
    <url>/posts/b57883cc.html</url>
    <content><![CDATA[<p>分配排序与我们前面的所有其他排序都不一样，无论是选择排序还是插入排序，都需要比较关键码然后再进行排序，而分配排序只需要先<strong>将数据分配到不同的桶中，再将桶中的数据收集到一起</strong>，其时间复杂度可以达到线性阶。分配排序主要有两种方法：桶式排序和链式基数排序，其中 桶式排序主要是对单关键字进行排序，而链式基数排序则是对多关键字进行排序，下面分别进行介绍</p>
<h1 id="桶式排序"><a href="#桶式排序" class="headerlink" title="桶式排序"></a>桶式排序</h1><h2 id="箱排序"><a href="#箱排序" class="headerlink" title="箱排序"></a>箱排序</h2><p>主要流程：假设待排序的记录的值在0到m-1之间，设置m个桶，依次扫描待排序的记录，R[1]，…，R[n-1]，把关键字等于k的记录全都装入到第k个箱子里(分配)，然后按序号依次将各非空的箱子首尾连接起来(收集)。例如要将一副混洗的52张扑克牌按点数A&lt;2&lt;…&lt;J&lt;Q&lt;K排序，需设置13个”箱子”，排序时依次将每张牌按点数放入相应的箱子里，然后依次将这些箱子首尾相接，就得到了按点数递增序排列的一副牌。</p>
<p>有几个需要注意的点：</p>
<ul>
<li><p>由于一般情况下每个箱子中存放多少个关键字相同的记录是无法预料的，故箱子的类型应设计成链表为宜。</p>
</li>
<li><p>为保证排序是稳定的，分配过程中装箱及收集过程中的连接必须按先进先出原则进行。有两个实现方法：</p>
<ul>
<li>每个箱子设为一个链队列。当一记录装入某箱子时，应做入队操作将其插入该箱子尾部；而收集过程则是对箱子做出队操作，依次将出队的记录放到输出序列中。</li>
<li>若输入的待排序记录是以链表形式给出时，出队操作可简化为是将整个箱子链表链接到输出链表的尾部。这只需要修改输出链表的尾结点中的指针域，令其指向箱子链表的头，然后修改输出链表的尾指针，令其指向箱子链表的尾即可。</li>
</ul>
</li>
<li>时间复杂度：分配过程的时间是O(n)；收集过程的时间为O(m) （采用链表来存储输入的待排序记录）或O(m+n)。因此，箱排序的时间为O(m+n)。若箱子个数m的数量级为O(n)，则箱排序的时间是线性的，即O(n)。</li>
</ul>
<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>桶排序的思想是把[0，1)划分为n个大小相同的子区间，每一子区间是一个桶。然后将n个记录分配到各个桶中。因为关键字序列是均匀分布在[0，1)上的，所以一般不会有很多个记录落入同一个桶中。由于同一桶中的记录其关键字不尽相同，所以必须采用关键字比较的排序方法(通常用插入排序)对各个桶进行排序，然后依次将各非空桶中的记录连接(收集)起来即可。</p>
<p>这种排序思想基于以下假设：<strong>假设输入的n个关键字序列是随机分布在区间[0，1)之上</strong>。若关键字序列的取值范围不是该区间，只要其取值均非负，我们总能将所有关键字除以某一合适的数，将关键字映射到该区间上。但要保证映射后的关键字是均匀分布在[0，1)上的。</p>
<p>伪代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BucketSon</span><span class="params">(R)</span></span></span><br><span class="line"><span class="function">    </span>&#123; <span class="comment">//对R[0..n-1]做桶排序，其中0≤R[i].key&lt;1(0≤i&lt;n)</span></span><br><span class="line">      <span class="keyword">for</span>(i=<span class="number">0</span>，i&lt;n;i++) <span class="comment">//分配过程．</span></span><br><span class="line">        将R[i]插入到桶B[「n(R[i].key)」]中； <span class="comment">//可插入表头上</span></span><br><span class="line">      <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++) <span class="comment">//排序过程</span></span><br><span class="line">        当B[i]非空时用插人排序将B[i]中的记录排序；</span><br><span class="line">      <span class="keyword">for</span>(i=<span class="number">0</span>，i&lt;n；i++) <span class="comment">//收集过程</span></span><br><span class="line">        若B[i]非空，则将B[i]中的记录依次输出到R中；</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<p>注：有些数据结构书对上面两者不做区分，事实上也差不多，下面直接默认桶排序为箱排序。</p>
<p>桶排序的存储结构为，其中QueueNode为桶的存储结构：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> key;</span><br><span class="line">    <span class="keyword">int</span> next;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">QueueNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> front;</span><br><span class="line">    <span class="keyword">int</span> rear;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>分配操作与收集操作的实现：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Distribute</span><span class="params">(Node r[], <span class="keyword">int</span> n, QueueNode q[], <span class="keyword">int</span> m, <span class="keyword">int</span> first)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = first;</span><br><span class="line">    <span class="keyword">while</span>(r[i].next!=<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> k = r[i].key;</span><br><span class="line">        <span class="keyword">if</span>(q[k].front != <span class="number">-1</span>)&#123;</span><br><span class="line">            q[k].front = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            r[q[k].rear].next = i;</span><br><span class="line">        &#125;</span><br><span class="line">        q[k].rear = i;</span><br><span class="line">        i = r[i].next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Collect</span><span class="params">(Node r[], <span class="keyword">int</span> n, QueueNode q[], <span class="keyword">int</span> m, <span class="keyword">int</span> first)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(q[k].front!=<span class="number">-1</span>)&#123;</span><br><span class="line">        k++;</span><br><span class="line">    &#125;</span><br><span class="line">    first = q[k].front;</span><br><span class="line">    <span class="keyword">int</span> last = q[k].rear;</span><br><span class="line">    <span class="keyword">while</span>(k&lt;m)&#123;</span><br><span class="line">        k++;</span><br><span class="line">        <span class="keyword">if</span>(q[k].front!=<span class="number">-1</span>)&#123;</span><br><span class="line">            r[last].next = q[k].front;</span><br><span class="line">            last = q[k].rear;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    r[last].next = <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>桶式排序代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BucketSort</span><span class="params">(Node r[], <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        r[i].next = i+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    r[n<span class="number">-1</span>].next = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> first = <span class="number">0</span>;</span><br><span class="line">    QueueNode q[<span class="number">100</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">-0</span>;i&lt;m;i++)&#123;</span><br><span class="line">        q[i].front = q[i].rear = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Distribute(r,n,q,m,first);</span><br><span class="line">    Collect(r,n,q,m,first);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h1><p>基数排序是借助多关键码进行桶式排序的思想对单关键码进行排序</p>
<p>参考链接：</p>
<ul>
<li><a href="https://www.cnblogs.com/ziyiFly/archive/2008/09/10/1288508.html" target="_blank" rel="noopener">https://www.cnblogs.com/ziyiFly/archive/2008/09/10/1288508.html</a></li>
<li>数据结构（第2版 王红梅）</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>动态规划（一）</title>
    <url>/posts/aa65faec.html</url>
    <content><![CDATA[<p>我们拿到一个动态规划问题，一定要考虑三个方面：</p>
<ul>
<li>问题目标</li>
<li>状态的定义：$opt[n]$</li>
<li>状态转移方程：$opt[n] = \text{best_of}(opt[n-1], opt[n-2])$</li>
</ul>
<p>下面我们举几个例子（Leetcode上的题目）：</p>
<h1 id="最大子序和"><a href="#最大子序和" class="headerlink" title="最大子序和"></a>最大子序和</h1><p><strong>目标</strong>：求得数组中和最大的一个子序列</p>
<p><strong>思路</strong>：</p>
<ul>
<li>首先对数组进行遍历，当前最大连续子序列和为sum，结果为results</li>
<li>如果sum &gt; 0，则说明sum对结果有增益效果，则sum保留并加上当前遍历数字</li>
<li>如果sum &lt;= 0，则说明sum对结果无增益效果，需要舍弃，则sum直接更新为当前遍历数字</li>
<li>每次比较sum 和 results的大小，将最大值置为results，遍历结束后返回结果results</li>
<li>时间复杂度为O（n）</li>
</ul>
<p>我们可以用数学符号表示为：</p>
<p>目标：</p>
<script type="math/tex; mode=display">\text{Max}\sum_{l=i}^jA[l]</script><p>子问题：</p>
<script type="math/tex; mode=display">M[j] = \text{max sum over all windows ending at j} = \max\{M[j-1]+A[j],A[j]\}</script><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums)==<span class="number">1</span>:</span><br><span class="line">    	<span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line">    max_ret = nums[<span class="number">0</span>]</span><br><span class="line">    cur_max = last_max = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">        <span class="keyword">if</span> last_max+nums[i]&lt;nums[i]:</span><br><span class="line">            cur_max = nums[i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur_max = last_max+nums[i]</span><br><span class="line">        <span class="keyword">if</span> cur_max &gt; max_ret:</span><br><span class="line">            max_ret = cur_max</span><br><span class="line">        last_max = cur_max</span><br><span class="line">    <span class="keyword">return</span> max_ret</span><br></pre></td></tr></table></figure>
<h1 id="最长上升子序列"><a href="#最长上升子序列" class="headerlink" title="最长上升子序列"></a>最长上升子序列</h1><p>目标是求一个数列$\{A_1,A_2,…A_n\}$中最长的子串，同时需要满足这个子串是递增的。同样我们可以定义$L(j)$为$j$位置结尾处最长的上升子序列，由此有$L(j)=max_{i&lt;j\text{ and }A[i]&lt;A[j]}\{L(i)\}+1$。下面我们看代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(len(nums)&lt;=<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">return</span> len(nums)</span><br><span class="line">    mem = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(nums))]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, j):</span><br><span class="line">            <span class="keyword">if</span> nums[i] &lt; nums[j]:</span><br><span class="line">                mem[j] = max(mem[j], mem[i]+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> max(mem)</span><br></pre></td></tr></table></figure>
<h1 id="零钱兑换"><a href="#零钱兑换" class="headerlink" title="零钱兑换"></a>零钱兑换</h1><p><strong>问题描述</strong>：给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。</p>
<p>如果这个题我们使用贪心算法有时候会失效，比如我们的目标是凑6元零钱，而有1元、3元、4元三种货币，如果用贪心算法最后组合为1个4元2个1元，共 3枚银币，而我们全局最优的解是两个3元硬币。</p>
<p>为了求解这个问题，我们依然设计一个数组$M[j]$来维护需要凑够$j$元的零钱所需的最小硬币数量，我们有如下递推式：</p>
<script type="math/tex; mode=display">M[j]=\min_i\{M[j-v_i]\}+1</script><p>其中$v_i$为硬币种类。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coinChange</span><span class="params">(self, coins, amount)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> amount == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> len(coins) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> len(coins) == <span class="number">1</span> <span class="keyword">and</span> coins[<span class="number">0</span>] &gt; amount:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    mem = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(amount+<span class="number">1</span>)]</span><br><span class="line">    mem[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, amount + <span class="number">1</span>):</span><br><span class="line">        cur_min = amount + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">if</span> c &lt;= i:</span><br><span class="line">                cur_min = mem[i-c] <span class="keyword">if</span> mem[i-c] &lt; cur_min <span class="keyword">else</span> cur_min</span><br><span class="line">        mem[i] = cur_min + <span class="number">1</span> <span class="keyword">if</span> cur_min&lt;amount+<span class="number">1</span> <span class="keyword">else</span> amount+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> mem[<span class="number">-1</span>] == amount + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> mem[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<h1 id="0-1背包问题"><a href="#0-1背包问题" class="headerlink" title="0-1背包问题"></a>0-1背包问题</h1><p>你只有一个容量有限的背包，总容量为c，有n个可待选择的物品，每个物品只有一件，它们都有各自的重量和价值，你需要从中选择合适的组合来使得你背包中的物品总价值最大。例如下图的对应关系：</p>
<p><img src="/posts/Alg/1.png" alt></p>
<p>我们定义$M[i,j]$为可选物品为前$i$件时且背包容量为$j$时的物品最大价值。由此我们可以导出状态转移方程：</p>
<script type="math/tex; mode=display">M[i, j] = \max\{M[i-1,j], M[i-1,j-S_i]+V_i\}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knapsack</span><span class="params">(w, v, c)</span>:</span></span><br><span class="line">    mem = np.zeros((len(w)+<span class="number">1</span>, c+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(w)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, c+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> w[i<span class="number">-1</span>]&lt;=j:</span><br><span class="line">                mem[i,j] = max(mem[i, j], mem[i<span class="number">-1</span>,j], mem[i<span class="number">-1</span>,j-w[i<span class="number">-1</span>]]+v[i<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mem[i,j] = mem[i<span class="number">-1</span>,j]</span><br><span class="line">    <span class="keyword">return</span> mem</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（九）—— KMP算法</title>
    <url>/posts/700f7e13.html</url>
    <content><![CDATA[<h1 id="KMP算法详解"><a href="#KMP算法详解" class="headerlink" title="KMP算法详解"></a>KMP算法详解</h1><p>KMP算法解决的是字符串间匹配的问题，我们以Leetcode上第28题为例来理解具体的KMP算法的实现过程与原理，原题如下：</p>
<p><img src="/posts/Alg/28.png" alt></p>
<p>字符串匹配问题的暴力解法自然很容易想到，也就是逐字搜索，当发现搜索的主串无法匹配子串时，返回搜索的初始位置的下一位继续匹配，具体C++实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">BF</span><span class="params">(<span class="keyword">char</span> *s1, <span class="keyword">char</span>*s2)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> len = <span class="built_in">strlen</span>(s1);</span><br><span class="line">	<span class="keyword">int</span> i=<span class="number">0</span>;<span class="keyword">int</span> j=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(s1[i]!=<span class="string">'\0'</span> &amp;&amp; s2[j]!=<span class="string">'\0'</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(s1[i]==s2[j])&#123;</span><br><span class="line">			i++;</span><br><span class="line">			j++;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span>&#123;</span><br><span class="line">			i=i-j+<span class="number">1</span>;</span><br><span class="line">			j=<span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span>(s2[j]==<span class="string">'\0'</span>) <span class="keyword">return</span>(i-j+<span class="number">1</span>);</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用这个方法自然也可以把这个题做出来，但这个算法的复杂度比较高，原因是每一轮匹配（失败）结束之后，主串又返回当前匹配位置的下一位开始匹配，于是损失了前面主串与字串匹配的所有信息，KMP算法的核心就是如何利用这些信息，是一个用空间换取时间的算法，它将要利用的信息存储在部分匹配表（PMT）中，也就是说将模式串的起始位置相对于主串直接向右滑动一段距离。那么问题的关键就是这段距离的长度怎么确定？就是要使用PMT表，下面具体解释这个表。</p>
<h2 id="PMT表"><a href="#PMT表" class="headerlink" title="PMT表"></a>PMT表</h2><p><strong>PMT中的值是字符串的前缀集合与后缀集合的交集中最长元素的长度</strong>：首先介绍前缀集合与后缀集合：例如，对于”aba”，它的前缀集合为{”a”, ”ab”}，后缀 集合为{”ba”, ”a”}。两个集合的交集为{”a”}，那么长度最长的元素就是字符串”a”了，长度为1，所以对于”aba”而言，它在PMT表中对应的值就是1。再比如，对于字符串”ababa”，它的前缀集合为{”a”, ”ab”, ”aba”, ”abab”}，它的后缀集合为{”baba”, ”aba”, ”ba”, ”a”}， 两个集合的交集为{”a”, ”aba”}，其中最长的元素为”aba”，长度为3。因此PMT的值即为<strong>前缀集与后缀集交集中最长的元素</strong></p>
<p>得到的表大概长这样：</p>
<p><img src="/posts/Alg/28_1.png" alt></p>
<p>那么我们应该怎么用这个表呢？如图你所示，要在主字符串”ababababca”中查找模式字符串”abababca”。如果在 j 处字符不匹配，那么由于前边所说的模式字符串 PMT 的性质，主字符串中 i 指针之前的 PMT[j −1] 位就一定与模式字符串的第 0 位至第 PMT[j−1] 位是相同的。这是因为主字符串在 i 位失配，也就意味着主字符串从 i−j 到 i 这一段是与模式字符串的 0 到 j 这一段是完全相同的。而我们上面也解释了，模式字符串从 0 到 j−1 ，在这个例子中就是”ababab”，其前缀集合与后缀集合的交集的最长元素为”abab”， 长度为4。所以就可以断言，主字符串中i指针之前的 4 位一定与模式字符串的第0位至第 4 位是相同的，即长度为 4 的后缀与前缀相同。这样一来，我们就可以将这些字符段的比较省略掉。具体的做法是，保持i指针不动，然后将j指针指向模式字符串的PMT[j −1]位即可。总结：PMT系数帮助我们快速定位J，避免i回指，同时减小J回指的长度。</p>
<p><strong>注意：PMT表是根据子串建立的</strong></p>
<p>有了上面的思路，我们就可以使用PMT加速字符串的查找了。我们看到如果是在 j 位 失配，那么影响 j 指针回溯的位置的其实是第 j −1 位的 PMT 值，所以为了编程的方便， 我们不直接使用PMT数组，而是将PMT数组向后偏移一位。我们把新得到的这个数组称为next数组。下面给出根据next数组进行字符串匹配加速的字符串匹配程序。其中要注意的一个技巧是，在把PMT进行向右偏移时，第0位的值，我们将其设成了-1，这只是为了编程的方便，并没有其他的意义。在本节的例子中，next数组如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>char</th>
<th>a</th>
<th>b</th>
<th>a</th>
<th>b</th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>a</th>
</tr>
</thead>
<tbody>
<tr>
<td>index</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
</tr>
<tr>
<td>pmt</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>next</td>
<td>-1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<h2 id="next数组求法"><a href="#next数组求法" class="headerlink" title="next数组求法"></a>next数组求法</h2><p>现在，我们再看一下如何编程快速求得next数组。其实，求next数组的过程完全可以看成字符串匹配的过程，即以模式（就是target）字符串为主字符串，以模式字符串（就是target）的前缀为目标字符串，一旦字符串匹配成功，那么当前的next值就是匹配成功的字符串的长度。</p>
<p>具体来说，就是从模式字符串的第一位(注意，不包括第0位)开始对自身进行匹配运算（因为i=0时，pmt系数为0）。 在任一位置，能匹配的最长长度就是当前位置的next值。如下图所示。</p>
<p><img src="/posts/Alg/28_2.png" alt></p>
<p><img src="/posts/Alg/28_3.png" alt></p>
<p><img src="/posts/Alg/28_4.png" alt></p>
<p><img src="/posts/Alg/28_5.png" alt></p>
<p><img src="/posts/Alg/28_6.png" alt></p>
<h1 id="完整代码-cpp"><a href="#完整代码-cpp" class="headerlink" title="完整代码(cpp)"></a>完整代码(cpp)</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">KMP</span><span class="params">(<span class="keyword">char</span> * t, <span class="keyword">char</span> * p)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> next[<span class="built_in">strlen</span>(p)];</span><br><span class="line">	getNext(p, next);</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> (i &lt; <span class="built_in">strlen</span>(t) &amp;&amp; j &lt; <span class="built_in">strlen</span>(p))</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span> (j == <span class="number">-1</span> || t[i] == p[j])</span><br><span class="line">		&#123;</span><br><span class="line">			i++;</span><br><span class="line">			j++;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">			j = next[j];</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (j == <span class="built_in">strlen</span>(p))</span><br><span class="line">		<span class="keyword">return</span> i - j;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getNext</span><span class="params">(<span class="keyword">char</span> *p, <span class="keyword">int</span> *next)</span></span>&#123;</span><br><span class="line">	next[<span class="number">0</span>] = <span class="number">-1</span>;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span>(i &lt; <span class="built_in">strlen</span>(p))&#123;</span><br><span class="line">		<span class="keyword">if</span>(j == <span class="number">-1</span> || p[i] == p[j])&#123;</span><br><span class="line">			++i;</span><br><span class="line">			++j;</span><br><span class="line">			next[i] = j;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span>&#123;</span><br><span class="line">			j = next[j];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>部分引用自知乎: <a href="https://www.zhihu.com/question/21923021/answer/281346746" target="_blank" rel="noopener">https://www.zhihu.com/question/21923021/answer/281346746</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（七）—— 手推SVM</title>
    <url>/posts/d0757ece.html</url>
    <content><![CDATA[<p>SVM有三宝：<strong>间隔，对偶，核技巧</strong>，下面一一阐述：</p>
<h1 id="间隔"><a href="#间隔" class="headerlink" title="间隔"></a>间隔</h1><h2 id="什么是线性可分"><a href="#什么是线性可分" class="headerlink" title="什么是线性可分"></a>什么是线性可分</h2><p><img src="/posts/Alg/svm_1.jpg" alt></p>
<p>上面的二维界面上，两类点被一条直线完全分开，叫做线性可分。</p>
<p>严格的数学定义是：$D_0$和$D_1$是$n$维欧氏空间中的两个点集。如果存在$n$维向量$w$和实数$b$，使得所有属于$D_0$的点$x_i$都有$wx_i+b\gt0$，而对于所有属于$D_1$的点$X_i$则有$wx_j+b\lt0$。则我们称$D_0$和$D_1$线性可分。</p>
<h2 id="什么是超平面"><a href="#什么是超平面" class="headerlink" title="什么是超平面"></a>什么是超平面</h2><p>上面提到的，将$D_0$和$D_1$完全正确地划分开的$wx+b=0$，就是一个超平面。以最大间隔把两类样本分开的超平面是最佳超平面，也称之为最大间隔超平面。<br>特征：</p>
<ul>
<li>两类样本分别分隔在该超平面的两侧</li>
<li>两侧距离超平面最近的样本点到超平面的距离被最大化了</li>
</ul>
<h2 id="什么是支持向量"><a href="#什么是支持向量" class="headerlink" title="什么是支持向量"></a>什么是支持向量</h2><p><img src="/posts/Alg/svm_2.jpg" alt></p>
<p>在蓝色样本中存在一些距离我们的超平面最近的一些点，这些点叫做支撑向量。</p>
<h2 id="svm的最优化问题是什么"><a href="#svm的最优化问题是什么" class="headerlink" title="svm的最优化问题是什么"></a>svm的最优化问题是什么</h2><p>首先我们想要最优化的是各类样本点到超平面的距离最远（其实也就是找到最大间隔超平面），任意一个超平面可以用下面这个线性方程来描述：</p>
<script type="math/tex; mode=display">w^Tx+b = 0</script><p><strong>n维空间距离又是怎么算的呢？</strong></p>
<p>我们看二维空间点$(x,y)$到直线$Ax+By+C=0$的的距离计算公式是：</p>
<script type="math/tex; mode=display">\frac{|Ax+By+C|}{\sqrt{A^2+B^2}}</script><p>拓展到$n$维也是同样的，点$x$到直线$w^Tx+b=0$的距离为：</p>
<script type="math/tex; mode=display">\frac{|w^Tx+b|}{||w||}</script><p>其中$||w||=\sqrt{w_i^2+…+w_d^2}$</p>
<p><img src="/posts/Alg/svm_3.jpg" alt></p>
<p>支撑向量是样本中离超平面最近的点，所以所有的其他的红色点距离超平面的距离一定大于$d$，那么我们有这样一个公式：</p>
<script type="math/tex; mode=display">\left\{\begin{array}{ll}
\frac{w^{T} x_{i}+b}{\|w\|} \geq d & y_{i}=1 \\
\frac{w^{T} x_{i}+b}{\|w\|} \leq-d & y_{i}=-1
\end{array}\right.</script><p>稍作转化可以得到：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{ll}
\frac{w^{T} x_{i}+b}{\|w\| d} \geq 1 & y_{i}=1 \\
\frac{w^{T} x_{i}+b}{\|w\| d} \leq-1 & y_{i}=-1
\end{array}\right.</script><p>由于$||w_i||d$为常数，可在所有系数上同时消去之，然后有（仍以原变量名命名）：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{ll}
w^{T} x_{i}+b \geq 1 & y_{i}=1 \\
w^{T} x_{i}+b \leq-1 & y_{i}=-1
\end{array}\right.</script><p>对于上面这个方程我们还能简写一下：</p>
<script type="math/tex; mode=display">
y_{i}\left(w^{T} x_{i}+b\right) \geq 1</script><p>超平面方程为：</p>
<script type="math/tex; mode=display">
w^{T} x_{i}+b = \pm1</script><p>有下图：<br><img src="/posts/Alg/svm_4.jpg" alt></p>
<p>并且每个支撑向量到超平面的距离可以写为：</p>
<script type="math/tex; mode=display">d=\frac{|w^Tx_i+b|}{||w||}</script><p>我们要最大化这个距离，也就是</p>
<script type="math/tex; mode=display">max\frac{|w^Tx_i+b|}{||w||}</script><p>在样本点确定以后，$|w^Tx_i+b|$是一个常数，所以这个式子就变成了：</p>
<script type="math/tex; mode=display">max\frac{1}{||w||}</script><p>也就是：</p>
<script type="math/tex; mode=display">min||w||</script><p>为了方便，我们取</p>
<script type="math/tex; mode=display">min\frac12||w||^2</script><p>所以得到最后的优化问题是：</p>
<script type="math/tex; mode=display">min\frac12||w||^2</script><script type="math/tex; mode=display">s.t. y_i(w^Tx_i+b)\ge1</script><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><h2 id="约束条件下的目标函数如何求解最优化问题"><a href="#约束条件下的目标函数如何求解最优化问题" class="headerlink" title="约束条件下的目标函数如何求解最优化问题"></a>约束条件下的目标函数如何求解最优化问题</h2><p>可以使用拉格朗日乘子法进行求解，将原本有约束的优化问题转化为对拉格朗日函数的无约束优化问题。<br>拉格朗日乘子法的定义（以三维空间为例）：</p>
<script type="math/tex; mode=display">L(x,y,\lambda)=f(x,y)+\lambda g(x,y)</script><p>其中$\lambda$是拉格朗日乘子</p>
<p>拉格朗日函数把原本的目标函数和其限制条件整合成了一个函数，这样子约束问题就不存在了，我<br>们可以直接对该目标函数求解其极值。</p>
<h2 id="怎么理解对偶问题"><a href="#怎么理解对偶问题" class="headerlink" title="怎么理解对偶问题"></a>怎么理解对偶问题</h2><p>首先，原问题是：</p>
<script type="math/tex; mode=display">min_{w,b}\frac12||w||^2</script><script type="math/tex; mode=display">s.t. g_i(w,b)=1-y_i(wx_i+b)\le0,i=1,2,...,m</script><p>拉格朗日乘子法转化后，变成了：</p>
<script type="math/tex; mode=display">L(w,b,\lambda)=\frac12||w||^2+\sum_{i=1}^m\lambda_i[1-y_i(wx_i+b)]</script><p>于是原问题变成：</p>
<script type="math/tex; mode=display">min_{w,b}max_\lambda L(w,b,\lambda)</script><script type="math/tex; mode=display">s.t. \lambda_i\ge0</script><p>我们首先直观的看一下，为什么转化后的下面的式子，可以替代上面的式子。<br>由于$\lambda_i\ge 0$<br>当$1-y_i(wx_i+b)\gt0$时，$max_\lambda L(w,b,\lambda)$是无穷，无意义<br>当$1-y_i(wx_i+b)\le0$时，$max_\lambda L(w,b,\lambda)$是$\frac12||w||^2$<br>所以$min(\infty,\frac12 ||w||^2) = \frac12||w||^2$</p>
<p>所以转化后的式子实际上和原来想表达的是的一样的。</p>
<h2 id="什么是对偶问题"><a href="#什么是对偶问题" class="headerlink" title="什么是对偶问题"></a>什么是对偶问题</h2><p>对偶问题实际上就是将</p>
<script type="math/tex; mode=display">min_{w,b}max_\lambda L(w,b,\lambda)</script><script type="math/tex; mode=display">s.t. \lambda_i \ge 0</script><p>变成了</p>
<script type="math/tex; mode=display">min_\lambda max_{w,b} L(w,b,\lambda)</script><script type="math/tex; mode=display">s.t. \lambda_i \ge 0</script><p>我们假设有一个函数$f$，则一定有：</p>
<script type="math/tex; mode=display">min maxf\ge max minf</script><p>也就是说，最大的里面挑出来的最小的，也要比最小的里面挑出来的最大的要大。这个关系实际上就是弱对偶关系，那什么是强对偶关系呢？</p>
<script type="math/tex; mode=display">min maxf =  max minf</script><p>此即为强对偶的关系。</p>
<p>至于为什么我们这里可以使用强对偶关系，是由于：</p>
<p>如果主问题是凸优化问题，也就是说当：</p>
<ol>
<li>拉格朗日函数中的$f(x)$和$g_i(x)$都是凸函数；</li>
<li>$h_i(x)$是仿射函数；</li>
<li>主问题可行域中至少有一点使得不等式约束严格成立。即存在$x$，对所有$j$，均有$g_i(x)\lt 0$。</li>
</ol>
<p>当1、2、3同时成立时，强对偶性成立。</p>
<h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（六）—— 决策树</title>
    <url>/posts/16501e9e.html</url>
    <content><![CDATA[<h1 id="决策树的直观理解"><a href="#决策树的直观理解" class="headerlink" title="决策树的直观理解"></a>决策树的直观理解</h1><p>顾名思义，决策树就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如CART只能是二叉树），也可以是多叉树（比如ID3、C4.5可以是多叉树或二叉树）。根节点包含整个样本集，每个叶节点都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。</p>
<p><img src="/posts/Alg/jcs.jpg" alt></p>
<p>就像上面这个例子，训练集由三个特征：outlook(天气)，humidity（湿度），windy（是否有风）。那么我们该如何选择特征对训练集进行划分那？连续型特征（比如湿度）划分的阈值又是如何确定的呢？决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种：</p>
<ol>
<li>当前节点包含的样本属于同一类别，无需划分</li>
<li>当前属性集为空，或所有样本在属性集上取值相同，无法划分</li>
<li>当前节点包含样本集合为空，无法划分</li>
</ol>
<h1 id="ID3、C4-5、CART"><a href="#ID3、C4-5、CART" class="headerlink" title="ID3、C4.5、CART"></a>ID3、C4.5、CART</h1><p>这三个是非常著名的决策树算法。简单粗暴来说，ID3使用<strong>信息增益</strong>作为选择特征的准则；C4.5使用<strong>信息增益比</strong>作为选择特征的准则；CART使用<strong>Gini指数</strong>作为选择特征的准则。</p>
<h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><p>熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。</p>
<p>信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。</p>
<p>注意：<strong>ID3仅仅能够处理离散属性。</strong></p>
<p>信息熵：</p>
<script type="math/tex; mode=display">H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{C_k}{D}</script><p>条件熵</p>
<script type="math/tex; mode=display">H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) = \sum_{i=1}^n\frac{|D_i|}{|D|}(-\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|})</script><p><img src="/posts/Alg/jcs_2.jpg" alt></p>
<p>在上面的问题中</p>
<script type="math/tex; mode=display">H(D) = -\frac35log_2\frac35-\frac25log_2\frac25 = 0.97</script><p>根据年龄进行划分：</p>
<script type="math/tex; mode=display">H(D|年龄) = \frac15(-0)+\frac45(-\frac24log_2\frac24-\frac24log_2\frac24)=0.8</script><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5克服了ID3仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 /划分前熵 选择信息增益比最大的作为最优特征。</p>
<p>C4.5处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。（事实上ID3也可以，不过最开始设计者提出来的时候并没有提出这个Idea，于是就认为没有）C4.5更主要还是能够对冲掉特征较多的问题，也就是ID3会更倾向于划分子类更多的特征，而C4.5解决了这个问题（增加惩罚项——取值熵）。</p>
<p>信息增益比：</p>
<script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><p>其中分母是取值熵：</p>
<script type="math/tex; mode=display">H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}</script><p>我们计算长相的取值熵：</p>
<script type="math/tex; mode=display">H_{长相}(D)=\frac15log_215-\frac35log_2\frac35-\frac15log_2\frac15=1.371</script><p>那么，特征长相的信息增益比是：</p>
<script type="math/tex; mode=display">g_R(D,长相) = \frac{0.42}{1.371} = 0.306</script><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART与ID3，C4.5不同之处在于CART生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。CART的全称是分类与回归树。从这个名字中就应该知道，CART既可以用于分类问题，也可以用于回归问题。</p>
<p>回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。<br>分类树中使用Gini指数最小化准则来选择特征并进行划分；Gini指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。</p>
<p>我们通过百面书上的例子来说明一下Gini系数的计算过程：</p>
<p><strong>Gini纯度公式</strong>：</p>
<script type="math/tex; mode=display">Gini(D)=1-\sum_{k=1}^n(\frac{|C_k|}{|D|})^2</script><p>按特征A切成两份后的Gini指数公式：</p>
<script type="math/tex; mode=display">Gini(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}Gini(D_i)</script><p>大家可以根据上面的例子自己做尝试，可以参考西瓜书这一节内容。</p>
<p><strong>Gini系数相比起熵不需要对数运算，更加高效，并且Gini指数更偏向于连续属性，熵更偏向于离散属性</strong></p>
<h1 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h1><p>决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛化性能的方法。剪枝分为预剪枝与后剪枝。</p>
<ul>
<li>预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。</li>
<li>后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。</li>
</ul>
<p>那么怎么来判断是否带来泛化性能的提升呢？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。</p>
<p>我们讲述一下百面书上的代价复杂剪枝：<br>女孩需要对80个人进行见或不见的分类。假设根据某种规则，已经得到了一棵CART决策树$T_0$：</p>
<p>从$T_0$开始，裁剪$T_i$中关于训练数据集合误差增加最小的分支以得到$T_{i+1}$。具体地，当一棵树$T$在结点$t$处剪枝时，它的误差增加可以用$R(t)-R(T_t)$表示，其中表示，其中$R(t)$进行剪枝之后的该结点误差，$R(T_t)$表示未进行剪枝时子树$T_t$的误差。考虑到树的复杂性因素，我们用$|L(T_t)|$表示子树$T_t$的叶子结点个数，则树在结点$t$处剪枝后的误差增加率为：</p>
<script type="math/tex; mode=display">\alpha = \frac{R(t)-R(T_t)}{|L(T_t)|-1}</script><p><img src="/posts/Alg/jcs_3.jpg" alt></p>
<p>在$t_3$处剪枝，剪枝之前误差是1+2（类别中较少的样本数），剪枝之后误差是4。子树叶节点个数为2。误差增加率为$\alpha(t_3)=\frac{4-(1+2)}{2-1}=1$</p>
<p>其他的误差增加率依次计算。</p>
<p><strong>问</strong>：决策树中连续值和缺失值特征是如何处理的？<br><strong>答</strong>：决策树中，对于连续属性，假设有n个样本，那么首先按照取值从小到大进行排序。取每两个值的中值作为候选<br>的划分点进行划分。n个样本，对应有n-1个区间，也就是n-1个候选划分点。尝试所有划分点之后，分别计算信息增<br>益，选取信息增益最大的划分点即可。对于属性有缺失值的情况，划分过程中计算属性信息增益的时候，只使用属性<br>没有缺失值的样本进行信息增益的计算。确定好分类之后，对于在该属性值有缺失的样本，将被归入所有的分支节<br>点。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（五）—— 线性回归与逻辑回归</title>
    <url>/posts/46dadf6.html</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>我们常说的变量关系有两种——函数关系和变量关系。线性回归是回归分析中的一个特例，回归分析研究的是变量间的相互关系，我们下面简要说明线性回归的推导。</p>
<p>首先设$Y$与$X$的关系为：$Y=\theta^{T} X+\epsilon$，上式中，$\epsilon$是$m×1$维向量，代表$m$个样本相对于线性回归方程的上下浮动程度。$\epsilon$是独立同分布的，由中心极限定理，$\epsilon$分布服从均值为0，方差为$\sigma^2$的正态分布。</p>
<p>结合上面的公式，对每个样本来说，有：$\varepsilon^{(j)}=y^{(j)}-\theta^{T} x^{(j)}$<br>上式中， $j \in(1,2, \cdots, m)$，$\varepsilon$分布服从均值为0，方差为$\sigma^{2}$的正态分布，所以:</p>
<script type="math/tex; mode=display">f\left(\varepsilon^{(j)}\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(\epsilon^{(j)})^{2}\right.}{2 \sigma^{2}}}</script><p>将 $\varepsilon^{(j)}=y^{(j)}-\theta^{T} x^{(j)}$ 代入上式, 有:</p>
<script type="math/tex; mode=display">f(y^{(j)} | x^{(j)} ; \theta)=\frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{(y^{(j)}-\theta^Tx^{(j)})^2}{2 \sigma^{2}}}</script><p>下面的公式推导用到了如下对数转换公式：</p>
<script type="math/tex; mode=display">\log a+\log b=\log a b</script><script type="math/tex; mode=display">\log a b=\log a+\log b</script><p>似然函数：</p>
<script type="math/tex; mode=display">L(\theta)=\prod_{j=1}^{m} f\left(y^{(j)} | x^{(j)} ; \theta\right)=\prod_{j=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y^{(j)}-\theta^Tx^{(j)})}{2 \sigma^{2}}^2}</script><p>两边取对数, $\Leftrightarrow l(\theta)=\log L(\theta)$</p>
<script type="math/tex; mode=display">\begin{array}{l}
l(\theta)=\log \prod_{j=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y^{(j)}-g^{T} r^{(1)}\right)^{2}}{2 \sigma^{2}}} =\sum_{j=1}^{m} \log \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{\left(y^{(j)}-b^{T} z^{(j)}\right)^{2}}{2 \sigma^{2}}} \\
l_{\theta}(\theta)=m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{j=1}^{m}\left(y^{(j)}-\theta^{T} x^{(j)}\right)^{2}
\end{array}</script><p>上式中，去掉常数项, 去掉负号，貝将求极大似然函数最大值转换为求成本函数最小值：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2} \sum_{j=1}^{m}\left(y^{(j)}-\theta^{T} x^{(j)}\right)^{2}</script><p>即：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2} \sum_{j=1}^{m}\left(y^{(j)}-h_{\theta}\left(x^{(j)}\right)\right)^{2}</script><p>这不就是最小二乘法吗~要解之获得最小值，即求导置0即可，当数据量大时需采用梯度下降法等算法以解之：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
J(\theta)=\frac{1}{2}(X \theta-Y)^{T}(X \theta-Y) \\
J(\theta)=\frac{1}{2}\left(\theta^{T} X^{T}-Y^{T}\right)(X \theta-Y) \\
J(\theta)=\frac{1}{2}\left(\theta^{T} X^{T} X \theta-\left(\theta^{T} X^{T} Y-Y^{T} X \theta+Y^{T} Y\right)\right.
\end{array}</script><p>对上式求导数：</p>
<script type="math/tex; mode=display">
\nabla J(\theta)=\frac{1}{2}\left(2 X^{T} X \theta-X^{T} Y-\left(Y^{T} X\right)^{T}\right)</script><script type="math/tex; mode=display">
\begin{array}{l}
\nabla J(\theta)=\frac{1}{2}\left(2 X^{T} X \theta-X^{T} Y-X^{T} Y\right) \\
\nabla J(\theta)=\frac{1}{2}\left(2 X^{T} X \theta-2 X^{T} Y\right) \\
\nabla J(\theta)=X^{T} X \theta-X^{T} Y
\end{array}</script><p>令上式 $=0$，即：</p>
<script type="math/tex; mode=display">\nabla J(\theta)=X^{T} X \theta-X^{T} Y=0</script><p>可求得：</p>
<script type="math/tex; mode=display">\theta=\left(X^{T} X\right)^{-1} X^{T} Y</script><p>以上就是线性回归，下面我们介绍逻辑回归。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归虽然名字有回归二字，但其重点是在解决二分类问题，即找一条决策边界来完成分类的决策，我们熟知的线性回归的决策函数是：</p>
<script type="math/tex; mode=display">h_\theta(x) = \theta^Tx</script><p>而逻辑回归则是把线性回归的决策函数做一个softmax输出：</p>
<script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>这个决策函数可以将$(-\infty,+\infty)$的数映射到$(0,1)$之间，成为一个概率输出。</p>
<p>除了这一点（决策函数）和线性回归不一样，逻辑回归的本质还是线性回归。</p>
<p>逻辑回归自然是能实现多分类的，实现方式分为一对一和一对多两种：</p>
<p>其中一对一分类是每两个类之间构建一个分类器，共需要$\frac{N(N-1)}{2}$个分类器；一对多顾名思义需要$N$个分类器</p>
<p>逻辑回归的优点：可解释性高，工业中可控度高。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（四）—— 滑动窗口</title>
    <url>/posts/465ba12d.html</url>
    <content><![CDATA[<p>我们首先看LeetCode中的一道题：</p>
<p><img src="/posts/Alg/209.png" alt></p>
<p>我们很容易想到这个算法的暴力解法，有$O(n^3)$的时间复杂度。我们可以先试着优化之使之称为具有$O(n^2)$时间复杂度的算法：我们首先维护一个数组，这个数组中记录了该位置之前所有数之和，当我们需要计算滑动窗口时，我们只需要将两个数相减即可，此时具有$O(n^2)$的时间复杂度。我们再思考一下原来暴力解的问题出在哪，其实我们在计算出i到j所有元素之和后，要继续计算i到j+1所有元素之和，是只需要利用前一个结果加上第j+1个元素即可的。因此这些个子数组之间是存在大量重复计算的。我们可以设计一个滑动窗口的思路.</p>
<p>首先我们定义两个指针left和right，两者通过分别向右滑动，前者能够使滑动窗口的和减小，后者能使和增大，开始时两者重合，窗口的和就是重合点所在的数。</p>
<p>下面我们分三个步骤：</p>
<ol>
<li>right向右滑动，使和变大，当恰好 大于等于s时，记录滑窗所包括的子数组长度ans，若ans已有数值，需判断新值是否小于旧值，若是，更新ans。</li>
<li>left向右滑动，判断是否仍大于等于s</li>
<li>若是，重复步骤2，若否，更新ans并转步骤1，直到有边框达到最右边</li>
</ol>
<p>下面我们看代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minSubArrayLen</span><span class="params">(self, s, nums)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type s: int</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        i, j, r = <span class="number">0</span>, <span class="number">0</span>, len(nums)+<span class="number">1</span></span><br><span class="line">        sums = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> sums:</span><br><span class="line">                sums.append(num)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sums.append(sums[<span class="number">-1</span>]+num)</span><br><span class="line">        <span class="keyword">while</span> i &lt; len(nums) <span class="keyword">and</span> j &lt; len(nums):</span><br><span class="line">            <span class="keyword">if</span> sums[j] - sums[i] + nums[i] &lt; s:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> j + <span class="number">1</span> - i &lt; r:</span><br><span class="line">                    r = j + <span class="number">1</span> - i</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> r != len(nums) + <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> r</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/posts/Alg/209_sol.png" alt></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（三）—— 双指针</title>
    <url>/posts/3defe89c.html</url>
    <content><![CDATA[<p>双指针算法是数组问题中常常用到的算法，主要分为对撞指针和快慢指针两种.</p>
<h1 id="对撞指针"><a href="#对撞指针" class="headerlink" title="对撞指针"></a>对撞指针</h1><p>我们先看Leetcode的第167题：</p>
<p><img src="/posts/Alg/167.png" alt></p>
<p>最直接的思考：暴力解法，双层遍历，令i和j同时遍历，看nums[i]+nums[j]是否等于target，此时时间复杂度是$O(n^2)$，事实上我们提交此算法到Leetcode，会提示Time Exceed，因此我们必须想出更高效的思路。</p>
<p>事实上，我们很容易看出，这个暴力解法忽略了这个数组一个很重要的特征——这个数组是有序的。当我们想到这一点，我们就知道在搜索时我们可以使用二分搜索的方法，时间复杂度就降为了$O(nlogn)$，此时算法比前面的暴力解法自然高效多了。</p>
<p>对这个问题，实际上存在一种时间复杂度为$O(n)$的解答：</p>
<p>初始情况下，我们将左指针放在数组的最左边，右指针放在数组的最右边，计算nums[i]+nums[j]是否等于target。如果是则返回之，若小于target，左边指针向右移动（增大数值），若大于target，则右指针向左移动（减小数值），好了，那我们直接来实现吧~</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, numbers: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        i, j = <span class="number">0</span>, len(numbers)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>((numbers[i]+numbers[j])!=target <span class="keyword">and</span> i&lt;j):</span><br><span class="line">            <span class="keyword">if</span> numbers[i]+numbers[j]&lt;target:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> [i+<span class="number">1</span>,j+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="/posts/Alg/167_sol1.png" alt></p>
<p>其他用到对撞指针的问题：</p>
<blockquote>
<p>125 Valid Palindrome<br>344 Reverse String<br>345 Reverse Vowels of a String<br>11  Container With Most Water(难度较高)</p>
</blockquote>
<h1 id="快慢指针"><a href="#快慢指针" class="headerlink" title="快慢指针"></a>快慢指针</h1><p>我们看到Leetcode第142题：</p>
<p><img src="/posts/Alg/142.png" alt></p>
<p>如果我们用一个Set保存已经访问过的节点，我们可以遍历整个列表并返回第一个出现重复的节点。这可以很好地解决这道题，但若我们增加一个条件：空间复杂度为$)(1)$，那么我们就不能使用这种算法，接下来我们介绍快慢指针的思想：</p>
<p>当一个跑的快的人与一个跑的慢的人在一条赛道上赛跑（无止尽），在某一个时刻，跑的快的人一定会从后面赶上跑得慢的人。类比之，我们定义快指针fast一次走两步，慢指针slow一次走一步，最终他们会在环中的某个位置相遇（最小公倍数）。</p>
<p><strong>快慢指针的步骤</strong>：</p>
<ol>
<li>fast走到链表末，返回null</li>
<li>fast=slow，两只真第一次相遇，slow指针同时从相遇的位置和最开始的位置出发，相遇的位置即为环入口。</li>
</ol>
<p>证明：</p>
<ul>
<li>a = 从开始位置到环入口的步数</li>
<li>b = 一环步数</li>
<li>fast = 2 * slow</li>
<li>fast = slow + n*b</li>
<li>=&gt; slow = n*b</li>
</ul>
<p>下面我们来看代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def detectCycle(self, head: ListNode) -&gt; ListNode:</span><br><span class="line">        fast, slow &#x3D; head, head</span><br><span class="line">        while fast !&#x3D; None:</span><br><span class="line">            fast &#x3D; fast.next</span><br><span class="line">            if fast &#x3D;&#x3D; None:</span><br><span class="line">                break</span><br><span class="line">            fast &#x3D; fast.next</span><br><span class="line">            slow &#x3D; slow.next</span><br><span class="line">            if slow &#x3D;&#x3D; fast:</span><br><span class="line">                break</span><br><span class="line">        if fast &#x3D;&#x3D; None:</span><br><span class="line">            return None</span><br><span class="line">        p1, p2&#x3D; slow, head</span><br><span class="line">        while p1 !&#x3D; p2:</span><br><span class="line">            p1 &#x3D; p1.next</span><br><span class="line">            p2 &#x3D; p2.next</span><br><span class="line">        return p1</span><br></pre></td></tr></table></figure>
<p>结果为</p>
<p><img src="/posts/Alg/142_sol.png" alt></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（二）——堆排序算法</title>
    <url>/posts/fda12dda.html</url>
    <content><![CDATA[<h1 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h1><p>二叉堆本质上是一种<strong>完全二叉树</strong>，它分为两个类型：最大堆和最小堆。</p>
<ol>
<li>最大堆：最大堆任何一个父节点的值，都大于等于它左右孩子节点的值</li>
<li>最小堆：最小堆任何一个父节点的值，都小于等于它左右孩子节点的值。</li>
</ol>
<p>二叉堆的根节点叫做<strong>堆顶</strong>，最大堆的堆顶是最大元素，最小堆的堆顶是最小元素。</p>
<h1 id="堆的自我调整"><a href="#堆的自我调整" class="headerlink" title="堆的自我调整"></a>堆的自我调整</h1><h2 id="插入节点"><a href="#插入节点" class="headerlink" title="插入节点"></a>插入节点</h2><p>我们首先有一个最大堆，我们希望给这个堆插入一个元素，我们首先直接将这个新元素放置到堆的最下部，此时发现最下面的子堆不满足最大堆的定义，依次<strong>向上调整</strong>：首先交换80和45，然后交换80和72，最终满足条件，此时插入节点完毕。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy82NjI5MDgwLWJiZjQ4M2U4YTcyYzZjNzkucG5n?x-oss-process=image/format,png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h2><p>首先我们要删除最小堆（如下图）的最顶端元素 0，我们首先将 0 与 3 进行交换，将最下面的元素交换到最上面。<br><img src="https://img-blog.csdn.net/20160521003432801?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center#pic_center" alt="在这里插入图片描述"><br>交换结果如下图，此时发现最上面的子堆不满足最小堆的定义：</p>
<p><img src="https://img-blog.csdn.net/20160521003454410?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center#pic_center" alt="在这里插入图片描述"><br>交换元素 1 和 3：</p>
<p><img src="https://img-blog.csdn.net/20160521003517583?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center#pic_center" alt="在这里插入图片描述"><br>一直向下调整，直到满足所有子堆最小堆的要求，得到最终结果：</p>
<p><img src="https://img-blog.csdn.net/20160521003536802?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center#pic_center" alt></p>
<h2 id="构建二叉堆"><a href="#构建二叉堆" class="headerlink" title="构建二叉堆"></a>构建二叉堆</h2><p>构建二叉堆也就是将一个无序的二叉树调整为二叉堆，本质上就是让所偶非叶子节点依次下沉，有两种方式——自下而上和自上而下。</p>
<p>自下而上就是首先看有叶子节点的最后一个堆，将其调整为一个最小堆（或最大堆），实际上就是和删除节点或插入节点时有类似的步骤，大家可以自己推一推。</p>
<h1 id="堆的实现"><a href="#堆的实现" class="headerlink" title="堆的实现"></a>堆的实现</h1><p>二叉堆虽然是一棵完全二叉树，但它的存储方式并不是链式存储，二十顺序存储，也就是说二叉堆的所有节点都存储在数组当中。</p>
<p>我们经过观察很容易发现，所有子堆的子节点和父节点满足 {2n+1&amp;2n+2 : n} 的关系，因此我们可以很容易得到一个节点的子节点或者父节点。因此数组和二叉树可以完全一一对应。</p>
<h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><p><strong>思想</strong>：我们删除一个最大堆的堆顶（替换到最后面），经过自我调整，第二大的元素就会被交换上来，成为最大堆的新堆顶。<br><strong>步骤</strong>：</p>
<ol>
<li>把无序数组构建成二叉堆（时间复杂度为 $O(n)$）</li>
<li>循环删除对顶元素，移到集合尾部，调节堆产生新的堆顶（时间复杂度为 $O(nlogn)$）</li>
</ol>
<h1 id="堆排序的应用"><a href="#堆排序的应用" class="headerlink" title="堆排序的应用"></a>堆排序的应用</h1><p>我们还是看到 Leetcode 的 215 题：<br><img src="https://img-blog.csdnimg.cn/2020051122003223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Leetcode"><br>我们直接看代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findKthLargest</span><span class="params">(self, nums, k)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._k = len(nums) - k</span><br><span class="line">        <span class="keyword">return</span> self.heapsort(nums)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">heapsort</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="comment"># 第一步：建堆</span></span><br><span class="line">        self.build_heap(nums)</span><br><span class="line">        <span class="comment"># 第二部：循环交换位置 最上面的节点与最下面的节点交换位置，最上面的节点向下调整</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>, self._k<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">            nums[i], nums[<span class="number">0</span>] = nums[<span class="number">0</span>], nums[i]</span><br><span class="line">            self.max_heapify(nums, <span class="number">0</span>, i)</span><br><span class="line">        <span class="keyword">return</span> nums[self._k]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_heap</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        length = len(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((length<span class="number">-1</span>) // <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            self.max_heapify(nums, i, length)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_heapify</span><span class="params">(self, nums, i, length)</span>:</span></span><br><span class="line">        <span class="comment"># 找到左右孩子节点</span></span><br><span class="line">        left = i * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        right = i * <span class="number">2</span> + <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> left &lt; length <span class="keyword">and</span> nums[left] &gt; nums[i]:</span><br><span class="line">            largest = left</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            largest = i</span><br><span class="line">        <span class="keyword">if</span> right &lt; length <span class="keyword">and</span> nums[right] &gt; nums[largest]:</span><br><span class="line">            largest = right</span><br><span class="line">            <span class="comment"># 若最大元素不是父节点则需要交换</span></span><br><span class="line">        <span class="keyword">if</span> largest != i:</span><br><span class="line">            nums[i], nums[largest] = nums[largest], nums[i]</span><br><span class="line">            self.max_heapify(nums, largest, length)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>算法复现（一）—— 快速排序算法</title>
    <url>/posts/f019e32d.html</url>
    <content><![CDATA[<p>快速排序是我们在面试时常常遇到的算法，我们接下来首先介绍快速排序的基本思想，然后手撸一遍快速排序算法，最后我们介绍一些特殊情景的应用。</p>
<h1 id="快速排序介绍"><a href="#快速排序介绍" class="headerlink" title="快速排序介绍"></a>快速排序介绍</h1><p>快速排序算法是冒泡排序算法的一种改进，其主要思想是通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据比另一部分所有数据小，整个过程可以递归进行，最终使整个数据变成有序序列。但快速排序是一种不稳定的排序算法，即相同元素不能保序，因此在一些实际场景中不能进行使用。</p>
<h1 id="快排实现步骤"><a href="#快排实现步骤" class="headerlink" title="快排实现步骤"></a>快排实现步骤</h1><ol>
<li>在数据集之中，选择一个元素作为“基准”（pivot）<br>· 可以选择第一个元素或随机选择一个元素，但最好随机选择<br>· 算法最坏情况复杂度为 $O(n^2)$，平均复杂度为$O(nlogn)$，其中 $n$ 为 partition 操作，$logn$ 为树的深度</li>
<li>所有小于“基准”的元素，都移到“基准”的左边；所有大于“基准”的元素，都移到“基准”的右边。这个操作称为”分区“（partition）<br>·    分区操作结束后，基准元素所处的位置就是最终排序后它的位置<br>·    partition 操作有两种方法：挖坑法和指针交换法</li>
<li>对“基准”左边和右边两个子集，不断重复第一步和第二步，直到所有子集只剩下一个元素为止</li>
</ol>
<h1 id="Partition-操作"><a href="#Partition-操作" class="headerlink" title="Partition 操作"></a>Partition 操作</h1><p>下图是 Partition 操作的主要方式，下面分别介绍两种方法 —— 挖坑法和指针交换法。</p>
<p><img src="https://img-blog.csdn.net/20180826153323609?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hX2hhbnFpYW5uYW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center" alt="挖坑法"></p>
<h3 id="挖坑法"><a href="#挖坑法" class="headerlink" title="挖坑法"></a>挖坑法</h3><p>首先我们的数组元素排序如下：<br><img src="https://img-blog.csdn.net/20180826155543836?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hX2hhbnFpYW5uYW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center" alt="数组元素"></p>
<p>首先我们选定基准元素Pivot，并记住这个位置index，这个位置相当于一个“坑”，并且设置两个指针 left 和 right ，指向数列的最左和最右两个元素。</p>
<p><img src="https://img-blog.csdn.net/20180826160755113?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hX2hhbnFpYW5uYW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center" alt="第一步"></p>
<p>接下来，从right指针开始，把指针所指向的元素和基准元素做比较。如果比pivot大，则right指针向左移动；如果比pivot小，则把right所指向的元素填入坑中。两个指针接触时算法停止。</p>
<p>4&gt;2，所以4的位置不变，将right指针左移继续比较，right右边黄色的区域代表着大于基准元素的区域。</p>
<p>1&lt;2，所以把1填入基准元素所在位置，也就是坑的位置。这时候，元素1本来所在的位置成为了新的坑。同时，left向右移动一位。</p>
<p>此时，left左边绿色的区域代表着小于基准元素的区域，接下来，我们切换到left指针进行比较。如果left指向的元素小于pivot，则left指针向右移动；如果元素大于pivot，则把left指向的元素填入坑中。</p>
<p><img src="https://img-blog.csdn.net/20180826163431457?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hX2hhbnFpYW5uYW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center" alt="第二步"></p>
<p>下面是挖坑法另一个例子，来自<a href="https://www.cnblogs.com/qq931399960/p/9550026.html" target="_blank" rel="noopener">这篇博客</a></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvNTYyMDMwLzIwMTgxMi81NjIwMzAtMjAxODEyMDYxNzIxMzM5MDctMTkzNDE2NzExMi5wbmc?x-oss-process=image/format,png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="指针交换法"><a href="#指针交换法" class="headerlink" title="指针交换法"></a>指针交换法</h3><p>指针交换法的思想是同时遍历左右边，交换不满足条件的两个元素，下面的例子同样来自<a href="https://www.cnblogs.com/qq931399960/p/9550026.html" target="_blank" rel="noopener">这篇博客</a>：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvNTYyMDMwLzIwMTgxMi81NjIwMzAtMjAxODEyMDYxODIzMjU5MDAtMTU3MTgyMjY5MS5wbmc?x-oss-process=image/format,png#pic_center" alt="在这里插入图片描述"></p>
<p>由于上面的图片讲的已经很清楚了，这里就不再进行讲解，下面直接上代码。</p>
<h1 id="快速排序代码实现"><a href="#快速排序代码实现" class="headerlink" title="快速排序代码实现"></a>快速排序代码实现</h1><p>下面提供一个最简单的实现，想要寻求刺激的可以直接看下一节。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quickSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(arr) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    pivot = arr[random.randint(<span class="number">0</span>,<span class="number">9</span>)]</span><br><span class="line">    left, right = [], []</span><br><span class="line">    arr.remove(pivot)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> item &gt;= pivot:</span><br><span class="line">            right.append(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left.append(item)</span><br><span class="line">    <span class="keyword">return</span> quickSort(left) + pivot + quickSort(right)</span><br></pre></td></tr></table></figure>
<h1 id="快速排序的应用"><a href="#快速排序的应用" class="headerlink" title="快速排序的应用"></a>快速排序的应用</h1><p>我们首先看到 leetcode 的215题：</p>
<p><img src="https://img-blog.csdnimg.cn/2020051122003223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Leetcode"></p>
<p>这个题我们可以用快速排序的思维来解题。我们在进行快速排序时，每进行一次 Partition，我们就能知道该基准元素的最终位置，因此我们只需要找到排名第二的元素即可，我们来看一下这个题的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findKthLargest</span><span class="params">(self, nums, k)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._k = len(nums) - k</span><br><span class="line">        <span class="keyword">return</span> self.quickSort(nums, <span class="number">0</span>, len(nums)<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quickSort</span><span class="params">(self, nums, left, right)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> left == right:</span><br><span class="line">            <span class="keyword">return</span> nums[left]</span><br><span class="line">        pivot = self.partition(nums, left, right)</span><br><span class="line">        <span class="keyword">if</span> pivot == self._k:</span><br><span class="line">            <span class="keyword">return</span> nums[pivot]</span><br><span class="line">        <span class="keyword">elif</span> pivot &lt; self._k:</span><br><span class="line">            <span class="keyword">return</span> self.quickSort(nums, pivot+<span class="number">1</span>, right)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.quickSort(nums, left, pivot<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(self, nums, left, right)</span>:</span></span><br><span class="line">        pivot = nums[left]</span><br><span class="line">        i, j = left, right</span><br><span class="line">        <span class="keyword">while</span> i &lt; j:</span><br><span class="line">            <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[j] &gt;= pivot:</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; j:</span><br><span class="line">                nums[i] = nums[j]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[i] &lt;= pivot:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; j:</span><br><span class="line">                nums[j] = nums[i]</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">        nums[i] = pivot</span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>头疼的Borel</title>
    <url>/posts/c649d14b.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="0eb29b3883ea72c2ea5b622beea2066bfc5a2313891254b25f78ea5de038d33e">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
  </entry>
  <entry>
    <title>数理统计笔记_假设检验</title>
    <url>/posts/55ea3ea0.html</url>
    <content><![CDATA[<h1 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h1><p><strong>含义</strong>：提出一个看法，判断一个看法（假设）是否成立，即为假设检验问题。</p>
<p><strong>检验</strong>：关于参数的判断，称为参数检验问题，不涉及参数的检验称为非参数检验，检验的基本思想是概率意义下的反证法。</p>
<p><strong>小概率原理</strong>：概率很小的时间在一次实验中，实际上几乎不会发生</p>
<p>假设检验也叫显著性检验，是以小概率反证法的逻辑推理，判断假设是否成立的统计方法，它首先假设样本对应的总体参数（或分布）与某个已知总体参数（或分布）相同，然后根据统计量的分布规律来分析样本数据，利用样本信息判断是否支持这种假设，并对检验假设做出取舍抉择，做出的结论是概率性的，不是绝对的肯定或者否定。</p>
<p>假设检验有两种错误，第一类型错误和第二类型错误。第一类型错误，是零假设成立的情况下拒绝零假设。第二类型错误，是备选假设成立的情况下接受零假设。</p>
<p>假设检验保证，第一类型错误概率不超过α。这是因为在零假设成立的情况下，统计量 st 值落在区域 I 的可能性小于等于α。对于第二类错误，假设检验就没有什么控制了。为什么假设检验控制第一类错误，而忽略第二类错误呢？一般情况下，零假设代表无效、无作用或者无影响，而备选假设代表有效、有作用或者有影响。这时候第一类错误的危害比第二类错误的大。比如在验证新算法有效性实验中，新算法实际无效但被认为有效的第一类型错误，会让大家错误地使用这个算法。新算法有效而被认为无效的第二类错误，只是让自己发不出论文，对大众没有影响。</p>
<h2 id="假设检验的步骤"><a href="#假设检验的步骤" class="headerlink" title="假设检验的步骤"></a>假设检验的步骤</h2><ol>
<li>建立检验假设和确定检验水准</li>
<li>选定检验方法和计算检验统计量</li>
<li>确定P值和做出推断结论</li>
</ol>
<h3 id="建立检验假设和确定检验水准"><a href="#建立检验假设和确定检验水准" class="headerlink" title="建立检验假设和确定检验水准"></a>建立检验假设和确定检验水准</h3><h4 id="建立检验假设"><a href="#建立检验假设" class="headerlink" title="建立检验假设"></a>建立检验假设</h4><p>在均数的比较中，检验假设是针对总体特征而言，包括相互对立的两个方面：</p>
<ul>
<li>原假设、零假设$H_0$：它是要否定的假设</li>
<li>备择假设$H_1$：它是$H_0$的对立面</li>
</ul>
<p>二者是从反证法的思想提出的，$H_0$和$H_1$是相互联系又相互对立的假设。</p>
<p>研究者可能有两种目的：</p>
<ol>
<li>推断两个总体均数有无明显差别。不管是病人某体征高于正常人还是低于正常人，两种可能性都存在，研究者同等关心，则应使用双侧检验。</li>
<li>根据专业知识，已知病人不会低于正常人，或是研究者只关心病人该体征是否高于正常人，应当使用单侧检验。</li>
</ol>
<h4 id="确定检验水准"><a href="#确定检验水准" class="headerlink" title="确定检验水准"></a>确定检验水准</h4><p>建设检验还需根据不同研究目的事先设置是否拒绝原假设的判断标准，即检验水准。检验水准也称为显著性水准，它指无效假设$H_0$为真，但被错误地拒绝的一个小概率值，一般取$\alpha=0.05$</p>
<p>原假设、备择假设和显著性水准的确定，以及单侧检验或双侧检验的选择，都应该结合研究设计，在未获得样本结果之前决定，而不受样本结果的影响。</p>
<h3 id="选定检验方法和计算检验统计量"><a href="#选定检验方法和计算检验统计量" class="headerlink" title="选定检验方法和计算检验统计量"></a>选定检验方法和计算检验统计量</h3><p>要根据研究设计的类型和统计推断的目的选用不同的检验方法。如成组设计的两样本均数的比较用t检验，多个样本均数的比较用F检验。</p>
<p>检验统计量时用于抉择是否拒绝原假设的统计量，其统计分布在统计推断中是至关重要的，不同的检验方法要用不同的方式计算现有样本的检验统计量值。</p>
<h3 id="确定P值和做出推断结论"><a href="#确定P值和做出推断结论" class="headerlink" title="确定P值和做出推断结论"></a>确定P值和做出推断结论</h3><p>P值是指由原假设成立时的检验统计量出现在由样本计算出来的检验统计量的末端或更末端处的概率值。</p>
<p><strong>$P≤α$时，结论为按所取检验水准拒原假设，接受备择假设。</strong>这样做出结论的理由是：在原假设成立的条件下，出现等于及大于现有检验统计量值的概率小于$α$（也即发生一类错误的概率），是小概率事件，这在一次抽样中是不大可能发生的，现有样本信息不支持原假设，因此拒绝他。</p>
<p><strong>如果$P&gt;α$，即样本信息支持原假设，就没有理由拒绝它，因此只好接受原假设。</strong></p>
<p>假设检验的结论是具有概率性的，不管是拒绝还是接受原假设，都有可能发生错误。拒绝原假设不等于原假设肯定不成立，因为小概率事件仍有可能发生只是可能性很小而已；同理，不拒绝原假设也不等于原假设肯定成立。</p>
<h2 id="均值的比较：-t-检验"><a href="#均值的比较：-t-检验" class="headerlink" title="均值的比较：$t$检验"></a>均值的比较：$t$检验</h2><p>$t$检验也称学生$t$检验，$t$检验是用于<strong>两个样本（或样本与群体）平均值差异程度</strong>的检验方法。它是用$T$分布理论来推断差异发生的概率，从而判定两个平均数的差异是否显著。</p>
<p>T检验的适用条件为<strong>样本分布符合正态分布</strong>。</p>
<p><strong>t检验的应用条件</strong>：</p>
<ol>
<li>当样本数较小时，要求样本取自正态总体</li>
<li>做两样本均数比较时，要求两样本的总体方差相同（如果方差不同也可进行t检验）</li>
</ol>
<h3 id="t-检验的类型"><a href="#t-检验的类型" class="headerlink" title="$t$检验的类型"></a>$t$检验的类型</h3><p>$t$检验有多种类型，可以分为只有一组样本的单体检验和有两组样本的双体检验。单体检验用于检验样本的分布期望是否等于某个值。双体检验用于检验两组样本的分布期望是否相等，又分为配对双体检验和非配对双体检验。配对双体检验的两组样本数据是一一对应的，而非配对双体检验的两组数据则是独立的。比如药物实验中，配对双体检验适用于观察同一组人服用药物之前和之后，非配对双体检验适用于一组服用药物而一组不服用药物。</p>
<h4 id="单体检验"><a href="#单体检验" class="headerlink" title="单体检验"></a>单体检验</h4><p>单体检验是针对一组样本的假设检验。零假设为$H_0:μ=μ_0$。统计量服从自由度为$n-1$的$T$分布:</p>
<script type="math/tex; mode=display">t = \frac{\bar x-\mu_0}{\frac{s}{\sqrt{n}}}</script><p>其中$\bar x$为样本均值，$s$为样本标准差，$n$为样本数。</p>
<h4 id="配对双体检验"><a href="#配对双体检验" class="headerlink" title="配对双体检验"></a>配对双体检验</h4><p>配对样本应用场景：同一个样本在两个时间点的检测数据、同一个样本用两种检测方式的检测数据。</p>
<p><strong>前提：两组独立样本必须都符合正态分布！！</strong></p>
<p>配对双体检验针对配对的两组样本。配对双体检验假设两组样本之间的差值服从正态分布。如果该正态分布的期望为零，则说明这两组样本不存在显著差异。零假设为$H_0:\mu=\mu_0$，统计量服从自由度$n-1$的$T$分布：</p>
<script type="math/tex; mode=display">t = \frac{d-\mu_0}{\frac{s}{\sqrt{n}}}</script><p>其中$d$是差值的平均值，$s$是差值的样本标准差。</p>
<blockquote>
<p>配对设计的t检验研究的是差值均数（样本均数）与理论上的差值总体均数的比较。<br>首先计算出各对差值$d$的均数。当两种处理结果无差别或某种处理不起作用时，理论上差值d的总体均数$μ_d=0$</p>
</blockquote>
<h4 id="非配对双体检验"><a href="#非配对双体检验" class="headerlink" title="非配对双体检验"></a>非配对双体检验</h4><p>非配对双体检验针对独立的两组样本。非配对双体检验假设两组样本是从不同的正态分布采样出来的。<strong>根据两个正态分布的标准差是否相等</strong>，非配对双体检验又可以分两类。</p>
<p>一种是分布标准差相等的情况（<strong>方差齐性</strong>）。零假设是两组样本的分布期望相等，统计量$T$服从自由度为$n_1+n_2-2$的$T$分布：</p>
<script type="math/tex; mode=display">t = \frac{\bar x_1-\bar x_2}{s_{x_1,x_2}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}</script><script type="math/tex; mode=display">s_{x_1x_2} = \sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}</script><p>另一种是分布标准差不相等的情况（方差非齐性），零假设也是两组样本的分布期望相等，统计量$T$服从$T$分布：</p>
<script type="math/tex; mode=display">t = \frac{\bar x_1 - \bar x_2}{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}</script><h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2><p>$\chi^2$概率分布主要用于检查实际结果与期望结果之间何时存在显著差别，该概率分布使用检验统计量$X^2$进行检验：</p>
<script type="math/tex; mode=display">X^2 = \sum{\frac{(O-E)^2}{E}}</script><p>其中$X^2$~$\chi^2$，$O$代表观察值，$E$代表期望值，$v$表示自由度（即用于检验统计量$\chi_2$的独立变量的数目，或者说是独立信息段的数目），一般来说$v = $组数-限制数</p>
<p>检验统计量的值越大，则说明观察值与期望结果之间存在的差异越明显，因此当检验统计量的结果大于查表得到的拒绝域时，就认为实际观察值与期望值不符，需要拒绝原假设。</p>
<h3 id="卡方检验的两个主要用途"><a href="#卡方检验的两个主要用途" class="headerlink" title="卡方检验的两个主要用途"></a>卡方检验的两个主要用途</h3><h4 id="用于检验拟合优度"><a href="#用于检验拟合优度" class="headerlink" title="用于检验拟合优度"></a>用于检验拟合优度</h4><p>也就是可以检验一组给定的数据与指定分布的吻合程度，例如，用于检验给定数据的出现频率是否与理论频率相吻合。</p>
<p>$\chi^2$拟合优度检验对相当多的概率分布都有效，只要你得到一组观察频数，且能算出期望频数，就可以使用$\chi^2$分布检验任何概率分布的拟合优度。而最大的困难在于计算自由度$v$（在查表确定拒绝域的时候需要自由度）</p>
<h4 id="用于检验两个变量的独立性"><a href="#用于检验两个变量的独立性" class="headerlink" title="用于检验两个变量的独立性"></a>用于检验两个变量的独立性</h4><p>通过这个方法可以检查变量之间是否存在某种关联。</p>
<h2 id="方差的比较：F检验"><a href="#方差的比较：F检验" class="headerlink" title="方差的比较：F检验"></a>方差的比较：F检验</h2><p>F检验又叫方差齐性检验，用于检验两组服从正态分布的样本是否具有相同的总体方差，即方差齐性。</p>
<p>非参数检验</p>
<p>参数检验是在假设总体分布已知的情况下进行的，但在实际生活中，那种对总体的分布的假定并不是能随便做出的。数据并不是来自所假定分布的总体，或者，数据根本不是来自一个增提；还有可能数据因为种种原因被污染。在这种情形下，在假定总体分布已知的情况下进行推断的做法就可能产生错误甚至导致灾难性的结论。于是，人们希望在不对整体分布做出假定的情况下，尽量从数据本身来获得所需要的信息，这就是分参数统计推断的总宗旨。</p>
<p>参数检验中的重点是t检验，不同的t检验方法适用于不同的分析场景，在不满足t检验的条件的时候就需要非参数检验了，下表整理了不同场景下的检验方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>分析方法</th>
<th>功能</th>
<th>正态性（前提）</th>
<th>不服从正态时</th>
<th>方差齐性</th>
</tr>
</thead>
<tbody>
<tr>
<td>单样本T检验</td>
<td>与某数字对比差异</td>
<td>服从正态分布</td>
<td>单样本Wilcoxon检验</td>
<td>-</td>
</tr>
<tr>
<td>配对样本T检验</td>
<td>配对数据差异</td>
<td>差值服从正态分布</td>
<td>配对Wilcoxon检验</td>
<td>无要求</td>
</tr>
<tr>
<td>独立样本T检验</td>
<td>两组数据差异</td>
<td>两组数据均服从正态分布</td>
<td>Mann-Whitney检验</td>
<td>要求方差齐</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>本文转载自CSDN博客 <a href="https://blog.csdn.net/qq_26822029/article/details/103632054?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1" target="_blank" rel="noopener">https://blog.csdn.net/qq_26822029/article/details/103632054?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1</a> ，用于本人资料查询，不做商用</p>
</blockquote>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
  </entry>
  <entry>
    <title>数理统计笔记_参数估计</title>
    <url>/posts/c100533b.html</url>
    <content><![CDATA[<h1 id="数理统计"><a href="#数理统计" class="headerlink" title="数理统计"></a>数理统计</h1><p>核心内容是统计推断，基本理论和方法有两部分：参数估计和假设检验，有两大应用方法：方差分析和回归分析。</p>
<h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p><strong>实际问题里常碰到两类问题</strong>：<br>（1）$X$ ~ $F(x;\theta)$，分布形式已知，但 $\theta$ 未知（ $\theta$ 可以是一个向量）<br>（2）$X$ 的分布自由，尽管新 $X$ 的某种数字特征 ，如期望、方差等</p>
<p>参数估计有<strong>点估计</strong>和<strong>区间估计</strong>之分，要求给出参数 $\theta$ 的一个值或一个范围的估计。</p>
<hr>
<h3 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h3><p><strong>点估计方法定义</strong>：设$\theta$为待估计参数，构造统计量来估计$\theta$，当用统计量$g(X_1, X_2,…,X_n)$来估计$\theta$时，称此统计量为$\theta$的估计量，记为 $\hat\theta$，即$\hat\theta = g(X_1,X_2,…,X_n)$。对样本的一次观察值  $(x_1,x_2,…x_n)$，估计量的值$g(x_1, x_2,…,x_n)$称为$\theta$的估计值，仍记为$\hat\theta$。</p>
<p><strong>常用的点估计方法有两种</strong>：矩估计法与极大似然估计法。</p>
<hr>
<h4 id="矩估计法"><a href="#矩估计法" class="headerlink" title="矩估计法"></a>矩估计法</h4><p><strong>计算方法</strong>：用样本k阶原点矩代替总体k阶原点矩：令$EX^k=\frac1n\sum_{i=1}^nX_i^k,k=1,2,…,L$，$L$是未知参数个数，理论依据是大数定律。<br>（1）未知参数仅一个时，则可以解方程$EX=\bar X$得到矩估计<br>（2）未知参数仅两个时，可以解方程组</p>
<script type="math/tex; mode=display">\begin{cases}
EX = \bar X\\
DX = S^2
\end{cases}</script><p>&emsp;&emsp; 得到矩估计，其中 $S^2 = \frac1n\sum^n_{i=1}(X_i-\bar X)^2$。</p>
<p><strong>矩估计的优点</strong>：简单易算，n大时精确度高，不依赖于总体分布形式。<br><strong>矩估计的理论依据</strong>：大数定律</p>
<blockquote>
<p>伯努利大数定律：解释频率的稳定性<br>独立同分布大数定律：$X_1,X_2,…X_n$独立同分布，则$X_1^k,X_2^k,…,X_n^k$也独立同分布</p>
</blockquote>
<p><strong>参数函数的矩估计</strong>：设 $\hat\theta$ 是$\theta$的矩估计，$g(\theta)$为$\theta$的连续函数，则定义$g(\theta)$的矩估计为$g(\hat\theta)$。</p>
<hr>
<h4 id="极大似然估计法"><a href="#极大似然估计法" class="headerlink" title="极大似然估计法"></a>极大似然估计法</h4><p><strong>思想</strong>：选择使样本出现可能性最大的p作为p的估计</p>
<p><strong>定义1（似然函数）</strong>：设总体$X$的密度函数为$f(x;\theta_1,\theta_2,…,\theta_k)$，来自$X$的样本为 $X_1, X_2,…,X_n$，其观察值为$x_1, x_2,…,x_n$，称$L(\theta_1,\theta_2,…,\theta_k)=\prod_{i=1}^nf(x_1;\theta_1,\theta_2,…,\theta_k)$为似然函数（注：它就是样本的联合密度函数，离散型变量换成联合分布率）</p>
<p><strong>定义2（极大似然估计）</strong>：若$L(\hat\theta_1,\hat\theta_2,…\hat\theta_k) = max_{\theta_1,\theta_2,…,\theta_k\in \Theta}L(\theta_1,\theta_2,…,\theta_k)$，则称$\hat\theta_i$是$\theta_i$的极大似然估计，简记为MLE。</p>
<p><strong>定义3（对数似然函数）</strong>：$LnL(\theta_1,\theta_2,…,\theta_k)$</p>
<p>对似然方程组：</p>
<script type="math/tex; mode=display">\begin{cases}
\frac{\partial LnL}{\partial \theta_1} = 0\\
\frac{\partial LnL}{\partial \theta_2} = 0\\
...\\
\frac{\partial LnL}{\partial \theta_k} = 0
\end{cases}</script><p>解此方程组并验证可得参数的MLE</p>
<p><strong>例1</strong>：设$X_1,X_2,…,X_n$为来自总体$X(\mu,\sigma^2)$的样本，求$\mu, \sigma^2$的MLE。<br>解：设样本观察值为 $x_1,x_2,…,x_n$</p>
<script type="math/tex; mode=display">L(\mu,\sigma^2) = \prod_{i=1}^nf(x_i) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} = \frac{1}{\sqrt{2\pi}^n\sigma^n}e^{-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}}</script><p>则：</p>
<script type="math/tex; mode=display">LnL = ln\frac{1}{(\sqrt{2\pi})^n} + ln\frac{1}{\sigma^n} -\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2} = ln\frac{1}{(\sqrt{2\pi})^n} - \frac{n}{2}ln\sigma^2 -\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}</script><p>置偏导为0：</p>
<script type="math/tex; mode=display">\frac{\partial lnL}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu) = 0</script><script type="math/tex; mode=display">\frac{\partial lnL}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{\sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^4}</script><p>易解得：</p>
<script type="math/tex; mode=display">\mu = \bar x, \sigma^2 = s^2</script><p>（验证：若驻点唯一且不在边界上取到，则可以把驻点作为MLE）</p>
<p><strong>例2</strong>：设$X_1,X_2,…,X_n$为来自总体$B(1,p)$的样本，求$p$的MLE。<br>解：设样本观察值为$x_1,x_2,…,x_n$,</p>
<script type="math/tex; mode=display">L(p) = \prod_{i=1}^nP(X_i=x_i)</script><p>又 $P(X=x)=p^x(1-p)^{1-x}$，故</p>
<script type="math/tex; mode=display">L(p) = \prod_{i=1}^np^{x_i}(1-p)^{1-x_i} = p^{\sum_{i=1}^nx_i}(1-p)^{n-\sum_{i=1}^nx_i}</script><p>取对数似然函数：</p>
<script type="math/tex; mode=display">LnL(p) = \sum_{i=1}^nx_ilnp+(n-\sum_{i=1}^nx_i)ln(1-p)</script><p>对p求偏导：</p>
<script type="math/tex; mode=display">\frac{\partial lnL(p)}{\partial p} = \frac{\sum_{i=1}^nx_i}{p} - \frac{n-\sum_{i=1}^nx_i}{1-p}</script><p>解得：</p>
<script type="math/tex; mode=display">p = \bar x</script><p>求二阶导数，$\frac{\partial^2lnL(p)}{\partial^2p}$严格小于0，故$p=\bar x$是极大值点，故$p$的 MLE 为$\hat p=\bar x$.</p>
<p><strong>例3</strong>：设$X_1,X_2,…,X_n$为来自总体$U[0,\theta]$的样本，求$\theta$的MLE。<br>解：设样本观察值为$x_1,x_2,…,x_n$,<br>由密度函数：</p>
<script type="math/tex; mode=display">f(x) = \begin{cases}
\frac{1}{\theta},0\le x\le \theta\\
0，其他
\end{cases}</script><p>似然函数为：</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^nf(x_i) = \frac{1}{\theta^n}, 0\le x_i \le \theta,i=1,2,...,n</script><p>对$\theta$求导：</p>
<script type="math/tex; mode=display">\frac{\partial l(\theta)}{\partial \theta} = \frac{-n}{\theta^{n+1}} = 0</script><p>驻点不存在，只能通过定义求：</p>
<script type="math/tex; mode=display">\theta = \frac{1}{\theta^n}, 0\le x_i \le \theta,i=1,2,...,n = \frac{1}{\theta^n}, 0\le x_{(1)} \le x_{(n)} \le \theta</script><p>故 $\hat \theta = X_{(n)} = max{ {x_1, x_2,…,x_n} }$</p>
<p><strong>定理1</strong>：设$\hat\theta$是$\theta$的MLE，$g(\theta)$是$\theta$的严格单调函数，则$g(\theta)$的MLE为$\hat{g(\theta)}$ =  $g(\hat\theta)$.(与矩估计的相应结论类似)</p>
<p><strong>例4</strong>：从一批产品中抽取了n件产品检查质量，发现其中k件产品不合格，求这批产品中合格品数与不合格品数之比的MLE。<br>解：设这批产品的总数为N，其中不合格品数为K件，则这批产品中合格品数与不合格品数之比$=\frac{N-K}{K} = \frac{1-p}{p} = \frac{1}{p}-1$，其中p表示这批产品中的不合格率，关于 $p$ 严格单调，故所求的MLE$=\hat {\frac{1-p}{p}}= \frac{1-\hat p}{\hat p} = \frac{1-\bar x}{\bar x}$，其中$\bar x$是样本不合格率。</p>
<hr>
<h4 id="点估计的优良标准"><a href="#点估计的优良标准" class="headerlink" title="点估计的优良标准"></a>点估计的优良标准</h4><ol>
<li>无偏性：设 $\theta$ 是待估计参数，$\hat \theta$ 是 $\theta$ 的估计量，若 $E\hat \theta = \theta$，则称$\hat \theta$是$\theta$的无偏估计；（无偏估计有无穷多个）</li>
<li>有效性：设$T_1,T_2$均是$\theta$的无偏估计，若$DT_1\le DT_2$，则称$T_1$ 比$T_2$有效；（最佳点估计）</li>
<li>一致性：设$\hat \theta$是$\theta$的估计量，若任意的$\epsilon \lt 0$都有$\lim_{n\rightarrow \infty} P(|\hat \theta-\theta|\lt \epsilon) = 1$，则称 $\hat\theta$是$\theta$的一致估计。</li>
</ol>
<p><strong>定理2</strong>：样本均值是总体均值的无偏估计；样本方差是总体方差的无偏估计<br>证明：记总体$X$有$EX=\mu$，$DX = \delta^2$，来自$X$的样本为$x_1,x_2,…,x_n$，样本均值$\bar X = \frac1n\sum_{i=1}^nx_i$，方差$S^{*2} = \frac1{n-1}\sum_{i=1}^n(x_i-\bar x)^2$，有</p>
<script type="math/tex; mode=display">E\bar X = E\frac1n\sum_{i=1}^nEx_i = \frac1n\sum_{i=1}^n\mu = \mu</script><p>因此 $\bar X$ 是 $\mu$ 的无偏估计，又</p>
<script type="math/tex; mode=display">\begin{eqnarray}ES^{*2} &=& E\frac{n}{n-1}\sum_{i=1}^n\frac{(x_i-\bar x)^2}{n} \\
&=& \frac{n}{n-1}E(\frac1n\sum_{i=1}^nx_i^2 - \bar x^2) \\
&=& \frac{n}{n-1}(\frac1n\sum_{i=1}^nEx_i^2 - E\bar x^2)\\
&=& \frac{n}{n-1}(\frac1n\sum_{i=1}^n(\sigma^2+\mu^2) - (\frac{\sigma^2}{n} + \mu^2)\\
&=& \sigma^2
\end{eqnarray}</script><p>注意到 $ES^2 = \frac{n-1}{n}ES^{*2}$ 不是无偏估计，是有偏估计。</p>
<p><strong>定义4</strong>：若$E\hat \theta \neq \theta$，但$\lim\limits_{n \rightarrow +\infty} E\hat \theta = \theta$，则称$\hat \theta$是$\theta$的渐近无偏估计。<br>显见，$ES^2$是总体方差的渐近无偏估计。</p>
<p><strong>例5</strong>：设总体$X$ ~ $U[0,\theta]$，$\hat\theta$是$\theta$的MLE，找常数$a,b$使$a\hat \theta + b$是$\theta$的无偏估计。<br>解：由例3结果，$\hat\theta = X_{(n)}$，分布函数为：</p>
<script type="math/tex; mode=display">f_{X(n)} = n[F(x)]^{n-1}f(x)</script><p>且已知：</p>
<script type="math/tex; mode=display">f(x) = \begin{cases}
\frac{1}{\theta},0\le x\le \theta\\
0，其他
\end{cases}</script><p>以及：</p>
<script type="math/tex; mode=display">F(x) = \begin{cases}
0, x\lt 0\\
\frac{1}{\theta},0\le x\le \theta\\
1,x\geq\theta
\end{cases}</script><p>代入得：</p>
<script type="math/tex; mode=display">f_{X(n)}(x)=\begin{cases}
n(\frac{x}{\theta})^{n-1}\frac1\theta,0\le x\le \theta\\
0,其他
\end{cases}</script><p>故：</p>
<script type="math/tex; mode=display">\begin{eqnarray}
EX_{(n)} &=& \int_{-\infty}^{+\infty}xf_{x_{(n)}}(x){\rm d}x\\
&=& \int_0^\theta x·n\frac{x^{n-1}}{\theta^n}{\rm d}x\\
&=& \frac{n}{n+1}\theta
\end{eqnarray}</script><p>令</p>
<script type="math/tex; mode=display">E(aX_{(n)}+b) = aEX_{(n)} + b = a\frac{n}{n+1}\theta + b = \theta</script><p>有：</p>
<script type="math/tex; mode=display">a = \frac{n+1}{n}, b=0.</script><p>即 $\frac{n+1}{n}X_{(n)}$ 是$\theta$的无偏估计。</p>
<p><strong>定义5</strong>：$\theta$ 的无偏估计全体构成一个集合 —— $\theta$ 的无偏估计类，其中方差最小的估计称为最小元素无偏估计，记为MVUE。</p>
<p><strong>定义6</strong>：设$T$是 $g(\theta)$ 的任意一个无偏估计，在一定条件下，有下列 C-R 不等式：$DT\ge \frac{[g^`(\theta)]^2}{nI(\theta)}$，其中$I(\theta) = E[\frac{\partial (lnf(X;\theta))}{\partial \theta}]^2$，不等式右边实自被称为$g(\theta)$的无偏估计 C-R 下界。</p>
<p><strong>定义7</strong>：设T是$\theta$的无偏估计，称$e_T = \frac{\frac{1}{nI(\theta)}}{DT}$为$\theta$ 的有效率$(0\le e_T \le 1)$，有效率为1的估计称为优效估计。若$e_T\neq 1$，但$\lim_{n\rightarrow \infty}e_T = 1$，称为渐近优效估计。</p>
<h3 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h3><p><strong>定义1</strong>：设$T_1,T_2$是 $\theta$ 的估计量，若对于给定值 $\alpha(0\lt \alpha \lt 1)$有，$P(T_1\le \theta \le T_2)=1-\alpha$，则称$[T_1,T_2]$是$\theta$ 的置信水平为$1-\alpha$的区间估计或置信区间。</p>
<p><strong>区间估计的优良标准</strong>：</p>
<ol>
<li>可靠性：$1-\alpha$越大越可靠</li>
<li>精确性：$E(T_1-T_2)$越小越精确</li>
</ol>
<p>可靠性与精确性是一对矛盾，一般处理原则：在可靠性条件满足的下，找 $E[T_2-T_1]$ 最小的，这样的区间估计被认为是最优的。</p>
<h4 id="单个正态总体-N-mu-sigma-2-参数的区间估计"><a href="#单个正态总体-N-mu-sigma-2-参数的区间估计" class="headerlink" title="单个正态总体 $N(\mu,\sigma^2)$ 参数的区间估计"></a>单个正态总体 $N(\mu,\sigma^2)$ 参数的区间估计</h4><p><strong>$\mu$ 的区间估计</strong></p>
<ol>
<li><p>当$\sigma$已知时，$\mu$的置信水平为$1-\alpha$的区间估计是$[\bar x-z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}},\bar x + z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}})]$<br>证明：要找 $\mu \in [T_1, T_2]$，使得$P(T_1&lt;\mu &lt; T_2) = 1-\alpha$，取$a, b$，使得 $\mu \in [\bar x - a, \bar x + b]$，使得 $P(\bar x- a&lt;\mu &lt; \bar x + b) = 1-\alpha$，由于$\frac{\bar x - \mu}{\sigma}\sqrt n$~$N(0, 1)$，我们可以将原始化为</p>
<script type="math/tex; mode=display">P(-\frac{b\sqrt{n}}{\sigma} \le \frac{\bar x - \mu}{\sigma}\sqrt{n} \le \frac{a\sqrt n}{\sigma}) = 1-\alpha</script><p>我们只需找到$a^<em>$与$b^</em>$，使得</p>
<script type="math/tex; mode=display">P(b^* \le \frac{\bar x - \mu}{\sigma}\sqrt{n} \le a^*) = 1-\alpha</script><p>左式 = $\int_{b^<em>}^{a^</em>}\phi(x){\rm d}x$，设正态曲线分布图 $b^*$左边区域面积为 $\beta$，则有：</p>
<script type="math/tex; mode=display">P(z_{1-\alpha+\beta} \le \frac{\bar x - \mu}{\sigma}\sqrt{n} \le z_\beta) = 1-\alpha</script><p>即：</p>
<script type="math/tex; mode=display">P(\bar x-z_\beta \frac{\sigma}{\sqrt{n}} \le \mu \le \bar x - z_{1-\alpha+\beta}\frac{\sigma}{\sqrt{n}}) = 1-\alpha</script><p>(任意取定 $\alpha$，$\beta$有无穷多个取值，可以构造无穷多个满足条件的区间<br>找最优的区间估计：</p>
<script type="math/tex; mode=display">E[\bar x - z_{1-\alpha+\beta}\frac{\sigma}{\sqrt{n}} - \bar x+z_\beta \frac{\sigma}{\sqrt{n}}] = E(z_\beta - z_{1-\alpha+\beta})\frac{\sigma}{\sqrt{n}} = (z_\beta - z_{1-\alpha+\beta})\frac{\sigma}{\sqrt{n}}</script><p>令 $g^<code>(\beta) = z_\beta - z_{1-\alpha+\beta} = (z_\beta)^</code> - (z_{1-\alpha+\beta})^`$<br>设分布函数为 $\Phi(z_\beta) = \beta$，两边对$\beta$求导，得：</p>
<script type="math/tex; mode=display">\Phi(z_\beta)(z_{\beta})^`_\beta = 1</script><p>因此有 $(z_\beta)^`_\beta = \frac {1}{\Phi(z_\beta)}$，又有 $\Phi(z_{1-\alpha+\beta}) = 1-\alpha+\beta$，两边对 $\beta$ 求导，得：</p>
<script type="math/tex; mode=display">\Phi(z_{1-\alpha+\beta})(z_{1-\alpha+\beta})^`_\beta = 1</script><p>因此有 $(z_{1-\alpha+\beta})^<code>= \frac{1}{\Phi(z_{1-\alpha+\beta})}$
综上，要使 $g^</code>(\beta) =0$，即使 $\frac {1}{\Phi(z_\beta)} = \frac{1}{\Phi(z_{1-\alpha+\beta})}$，也就是$\Phi(z_\beta) = \Phi(z_{1-\alpha+\beta})$，因此两点关于 $y$ 轴对称，此时能保证平均长度最短</p>
</li>
<li><p>当 $\sigma$ 未知时， $\mu$ 的置信水平为 $1-\alpha$ 的区间估计是 $[\bar X - t_\frac{\alpha}{2}(n-1)\frac{S}{\sqrt{n}},\bar X + t_\frac{\alpha}{2}(n-1)\frac{S}{\sqrt{n}}]$<br>证明：$\sigma$ 未知，我们可以考虑它的估计量  $S^*$，由前面推导：</p>
<script type="math/tex; mode=display">\frac{\bar X-\mu}{S^*}\sqrt{n} - t(n-1)</script><p>因此有</p>
<script type="math/tex; mode=display">P(-a\le \frac{\bar X-\mu}{S^*}\sqrt{n}\le a) = \int_{-a}^a f_T(x){\rm d}x = 1-\alpha</script><p>容易确定 $a = t_{\frac{\alpha}{2}}(n-1)$，即得所证</p>
</li>
</ol>
<p><strong>$\sigma$ 的区间估计</strong></p>
<ol>
<li>$\sigma^2$的置信水平为$1-\alpha$的置信区间为 $[\frac{(n-1)S^{<em>2}}{\chi^2_{\frac{\alpha}{2}}(n-1)}, \frac{(n-1)S^{</em>2}}{\chi^2_{1-\frac{\alpha}{2}}(n-1)}]$.</li>
</ol>
<p>证明：由已知 </p>
<script type="math/tex; mode=display">\frac{(n-1)S^{*2}}{\sigma^2} - \chi^2(n-1)</script><p>设</p>
<script type="math/tex; mode=display">P(a\le \frac{(n-1)S^{*2}}{\sigma^2} \le b) = 1-\alpha</script><p>实际上，卡方密度函数并不对称，但我们可以近似认为其对称以求出简单化的结果：</p>
<script type="math/tex; mode=display">a = \chi^2_{1-\frac{\alpha}{2}}(n-1), b = \chi^2_{\frac{\alpha}{2}}(n-1)</script><p>取倒数，不等号反向即可</p>
<ol>
<li>$\sigma$ 的置信水平为 $1-\alpha$ 的置信区间为 $[\sqrt{\frac{(n-1)S^{<em>2}}{\chi^2_{\frac{\alpha}{2}}(n-1)}}, \sqrt{\frac{(n-1)S^{</em>2}}{\chi^2_{1-\frac{\alpha}{2}}(n-1)}}]$</li>
</ol>
<p>证明：事实上，由于 $\sigma&gt;0$，有：</p>
<script type="math/tex; mode=display">P(T_1\le \sigma^2 \le T_2) = 1-\alpha  P(\sqrt{T_1} \le \sigma \le \sqrt{T_2}) = 1-\alpha</script><p>故 $\sigma \in [\sqrt{T_1},\sqrt{T2}]$<br>(类似的有： $e^{\sigma} \in [e^{\sqrt{T_1}}, e^{\sqrt{T_2}}]$)</p>
<h4 id="分布自由时总体均值的区间估计"><a href="#分布自由时总体均值的区间估计" class="headerlink" title="分布自由时总体均值的区间估计"></a>分布自由时总体均值的区间估计</h4><p>$EX$的置信水平为$1-\alpha$的近似区间估计为$[\bar X-u_{1-\frac{\alpha}{2}}\frac{S^<em>}{\sqrt{n}}, \bar X+u_{1-\frac{\alpha}{2}}\frac{S^</em>}{\sqrt{n}}]$</p>
<blockquote>
<p>中心极限定理：设$x_1,x_2,…,x_n$独立同分布，且$Ex_1 = \mu, Dx_1=\sigma^2$存在，则 $lim_{n\rightarrow \infty}P(\frac{\sum_{i=1}^nx_i - n\mu}{\sqrt{n}\sigma} \le x) = \Phi(x), x \in R$，因此当$n$充分大时，$\frac{\sum_{i=1}^nx_i-n\mu}{\sqrt{n}\sigma}$ ~ $N(0,1)$，由于左式实际上为关于$\sum_{i=1}^nx_i$的线性变换，因此$\sum_{i=1}^nx_i$~$N(a, b)$，其中$a=n\mu, b=n\sigma^2$，同理$\bar X =\frac{\sum_{i=1}^nx_i}{n}$ ~ $N(\mu, \frac{\sigma^2}{n})$。</p>
</blockquote>
<p><strong>例</strong>：设样本总体 $X$ ~ $E(\lambda)$，来自$X$的样本为 $x_1,x_2,…,x_n$，求$\lambda$的置信水平为$1-\alpha$的区间估计，提示：若$x$~$E(\lambda)$，则$2x\lambda$ ~ $\chi^2(2)$</p>
<p>解：$P(a\le2\lambda n\bar x\le b) = 1-\alpha$，直接取左右两边各为$\frac{\alpha}{2}$即可满足，因此$a=\chi_{1-\frac{\alpha}{2}}^2(2n)$，$b=\chi_{\frac{\alpha}{2}}^2(2n)$，取倒数即可解出$\lambda$的区间估计。</p>
<h4 id="两个正态总体参数的区间估计"><a href="#两个正态总体参数的区间估计" class="headerlink" title="两个正态总体参数的区间估计"></a>两个正态总体参数的区间估计</h4><p><strong>均值差的区间估计</strong></p>
<ol>
<li><p>$\sigma_1^2, \sigma_2^2$已知，则$\mu_1-\mu_2$的置信水平为$1-\alpha$的区间估计为$[\bar X - \bar Y -Z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}, \bar X - \bar Y +Z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}]$<br>证明：$\bar X$ ~ $N(\mu_1, \frac{\sigma_1^2}{n})$，$\bar Y$ ~ $N(\mu_2, \frac{\sigma_2^2}{m})$，<br>$E(\bar X-\bar Y)=\mu_1-\mu_2$，$D(\bar X-\bar Y) = D\bar X+D(-\bar Y) = D\bar X +D\bar Y$，<br>因此$\frac{\bar X-\bar Y - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}}$~$N(0,1)$，由枢轴量法易解</p>
</li>
<li><p>$\sigma_1^2,\sigma_2^2$未知，但$\sigma_1^2=\sigma_2^2 = \sigma^2$.<br>证明：$\frac{\bar X-\bar Y - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma^2}{n}+\frac{\sigma^2}{m}}} = \frac{\bar X-\bar Y - (\mu_1-\mu_2)}{\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}}$~$N(0,1)$，由于$\sigma$未知，因此不可以直接作为枢轴量。<br>由已知<br>$\frac{(n-1)S_X^2}{\sigma^2}+\frac{(m-1)S_Y^2}{\sigma^2}$ ~ $\chi^2(n+m-2)$，易知该变量与标准正态独立，因此联立消去$\sigma$即可</p>
</li>
</ol>
<h4 id="求区间估计的常用方法——找枢轴量法"><a href="#求区间估计的常用方法——找枢轴量法" class="headerlink" title="求区间估计的常用方法——找枢轴量法"></a>求区间估计的常用方法——找枢轴量法</h4><p>设 $\theta$ 是待估计参数，样本为 $x_1, x_2, …, x_n$</p>
<ol>
<li>构造函数$g(x_1, x_2, …, x_n;\theta)$关于$\theta$单调；</li>
<li>求此函数得分布；</li>
<li>确定分位数；</li>
<li>解不等式可得$\theta$得区间估计.</li>
</ol>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
  </entry>
  <entry>
    <title>R语言实战（三）</title>
    <url>/posts/17ce1303.html</url>
    <content><![CDATA[<p>本次分享，我们将评述用于生成基本的描述性统计量和推断统计量的R函数。首先，我们将着眼于定量变量的位置和尺度的衡量方式。然后我们将学习生成类别型变量的频数表和列联表的方法（以及连带的卡方检验）。接下来，我们将考察连续型和有序型变量相关系数的多种形式。最后，我们将转而通过参数检验（t检验）和非参数检验（Mann-WhitneyU检验、Kruskal-Wallis检验）方法研究组间差异。</p>
<p>参考书籍：</p>
<blockquote>
<p>《R语言实战—第2版》————-Robert I.Kabacoff</p>
</blockquote>
<p><strong>本次内容：</strong></p>
<ol>
<li><strong>描述性统计分析</strong></li>
<li><strong>频数表和列联表</strong></li>
<li><strong>相关</strong></li>
<li><strong>t检验</strong></li>
<li><strong>组间差异的非参数检验</strong></li>
</ol>
<h1 id="1-描述性统计分析"><a href="#1-描述性统计分析" class="headerlink" title="1. 描述性统计分析"></a>1. 描述性统计分析</h1><p>我们将使用中Motor Trend杂志的车辆路试（mtcars）数据集。我们的关注焦点是每加仑汽油行驶英里数（mpg） 、马力（hp）和车重（wt）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(car)</span><br><span class="line">attach(mtcars)</span><br><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">head(mtcars[myvars])</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-fd475d968ea368e2d950241e22eda92e_720w.jpg" alt="img"></p>
<p>我们将首先查看所有32种车型的描述性统计量，然后按照变速箱类型（am）和汽缸数（cyl）考察描述性统计量。变速箱类型是一个以0表示自动挡、1表示手动挡来编码的二分变量，而汽缸数可为4、5或6。</p>
<h2 id="1-1-使用summary-函数来获取描述性统计量"><a href="#1-1-使用summary-函数来获取描述性统计量" class="headerlink" title="1.1 使用summary()函数来获取描述性统计量"></a>1.1 使用summary()函数来获取描述性统计量</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">summary(mtcars[myvars])</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-579306ded6133d2c94a145c57c98a0f5_720w.jpg" alt="img"></p>
<p><strong>summary()函数提供了最小值、最大值、四分位数和数值型变量的均值，以及因子向量和逻辑型向量的频数统计。</strong></p>
<h2 id="1-2-通过sapply-计算描述性统计量"><a href="#1-2-通过sapply-计算描述性统计量" class="headerlink" title="1.2 通过sapply()计算描述性统计量"></a>1.2 通过sapply()计算描述性统计量</h2><p>sapply()函数，其使用格式为：</p>
<p>sapply(x, FUN, options)</p>
<p>其中的x是你的数据框（或矩阵），FUN为一个任意的函数。如果指定了options，它们将被传递给FUN。你可以在这里插入的典型函数有mean()、sd()、var()、min()、max()、median()、length()、 range()和quantile()。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mystats &lt;- function(x,na.omit&#x3D;FALSE)&#123;</span><br><span class="line">  if(na.omit)&#123;x &lt;- x[!is.na(x)]&#125;</span><br><span class="line">  m &lt;- mean(x)</span><br><span class="line">  n &lt;- length(x)</span><br><span class="line">  s &lt;- sd(x)</span><br><span class="line">  skew &lt;- sum((x-m)^3&#x2F;s^3)&#x2F;n</span><br><span class="line">  kurt &lt;- sum((x-m)^4&#x2F;s^4)&#x2F;n-3</span><br><span class="line">  return(c(n&#x3D;n,mean&#x3D;m,stdev&#x3D;s,skew&#x3D;skew,kurtosis&#x3D;kurt))</span><br><span class="line">&#125;</span><br><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">sapply(mtcars[myvars], mystats)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-d020b9a1a2901cee219233013eaaeac1_720w.jpg" alt="img"></p>
<p>对于样本中的车型， 每加仑汽油行驶英里数的平均值为20.1，标准差为6.0。分布呈现右偏（偏度+0.61），并且较正态分布稍平（峰度–0.37）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(RColorBrewer) </span><br><span class="line">mycolors &lt;- brewer.pal(8, &quot;Set1&quot;) </span><br><span class="line">hist(mtcars$mpg,col &#x3D; mycolors[2],xlab &#x3D; &quot;mpg&quot;,breaks &#x3D; 15,freq &#x3D; FALSE)</span><br><span class="line">lines(density(mtcars$mpg), col&#x3D;mycolors[1], lwd&#x3D;2) </span><br><span class="line">abline(v &#x3D; mean(mtcars$mpg),lty&#x3D;2,col&#x3D;mycolors[3])</span><br><span class="line">text(mean(mtcars$mpg)-2.5,0.01,&quot;mean(mpg)&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-a5f7b6a6b1f751e142dc31d8eacf4028_720w.jpg" alt="img"></p>
<h2 id="1-3-pastecs包中的stat-desc-函数计算描述性统计量"><a href="#1-3-pastecs包中的stat-desc-函数计算描述性统计量" class="headerlink" title="1.3 pastecs包中的stat.desc()函数计算描述性统计量"></a>1.3 pastecs包中的stat.desc()函数计算描述性统计量</h2><p>pastecs包中有一个名为stat.desc()的函数， 它可以计算种类繁多的描述性统计量。 使用格式为：</p>
<p>stat.desc(x, basic=TRUE, desc=TRUE, norm=FALSE, p=0.95)</p>
<p>其中的x是一个数据框或时间序列。若basic=TRUE（默认值），则计算其中所有值、空值、缺失值的数量，以及最小值、最大值、值域，还有总和。若desc=TRUE（同样也是默认值），则计算中位数、平均数、平均数的标准误、平均数置信度为95%的置信区间、方差、标准差以及变异系数。最后，若norm=TRUE（不是默认的），则返回正态分布统计量，包括偏度和峰度（以及它们的统计显著程度）和Shapiro-Wilk正态检验结果。这里使用了p值来计算平均数的置信区间（默认置信度为0.95） 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(pastecs)</span><br><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">stat.desc(mtcars[myvars],norm&#x3D;TRUE)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-80fc3b04c7cf18fb3e76f457e5b4a625_720w.jpg" alt="img"></p>
<h2 id="1-4-使用aggregate-分组获取描述性统计量"><a href="#1-4-使用aggregate-分组获取描述性统计量" class="headerlink" title="1.4 使用aggregate()分组获取描述性统计量"></a>1.4 使用aggregate()分组获取描述性统计量</h2><p>在比较多组个体或观测时，关注的焦点经常是各组的描述性统计信息，而不是样本整体的描述性统计信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">aggregate(mtcars[myvars],by&#x3D;list(am&#x3D;mtcars$am),mean)</span><br><span class="line">aggregate(mtcars[myvars],by&#x3D;list(am&#x3D;mtcars$am),sd)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-87aa841532fd1c70e91b8f856a643afc_720w.jpg" alt="img"></p>
<p>注意list(am=mtcars$am)的使用。如果使用的是list(mtcars$am)，则am列将被标注为Group.1而不是am。你使用这个赋值指定了一个更有帮助的列标签。如果有多个分组变量，可以使用by=list(name1=groupvar1, name2=groupvar2, … , nameN=groupvarN)这样的语句。<strong>遗憾的是，aggregate()仅允许在每次调用中使用平均数、标准差这样的单返回值函数。它无法一次返回若干个统计量。</strong></p>
<h2 id="1-5-使用by-分组计算描述性统计量"><a href="#1-5-使用by-分组计算描述性统计量" class="headerlink" title="1.5 使用by()分组计算描述性统计量"></a>1.5 使用by()分组计算描述性统计量</h2><p>使用by()函数。格式为:by(data, INDICES, FUN)</p>
<p>其中data是一个数据框或矩阵，INDICES是一个因子或因子组成的列表，定义了分组，FUN是任意函数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dstats &lt;- function(x)sapply(x, mystats)</span><br><span class="line">myvars &lt;- c(&quot;mpg&quot;,&quot;hp&quot;,&quot;wt&quot;)</span><br><span class="line">by(mtcars[myvars],mtcars$am,dstats)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-4acbddeb930ae4a6aa2380440873cf05_720w.jpg" alt="img"></p>
<p>dstats()调用了mystats()函数，将其应用于数据框的每一栏中。再通过by()函数则可得到am中每一水平的概括统计量。</p>
<h1 id="2-频数表和列联表"><a href="#2-频数表和列联表" class="headerlink" title="2.频数表和列联表"></a>2.频数表和列联表</h1><p>本节中的数据来自vcd包中的Arthritis数据集。这份数据来自Kock &amp; Edward （1988） ，表示了一项风湿性关节炎新疗法的双盲临床实验的结果。前几个观测是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(vcd)</span><br><span class="line">head(Arthritis)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-972fc849bd835e7a447f3d9ae9860a6f_720w.jpg" alt="img"></p>
<p>治疗情况（安慰剂治疗、用药治疗）、性别（男性、女性）和改善情况（无改善、一定程度的改善、显著改善）均为类别型因子。</p>
<h2 id="2-1-生成频数表"><a href="#2-1-生成频数表" class="headerlink" title="2.1 生成频数表"></a>2.1 生成频数表</h2><p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-6f00e070172c5d851f8495984c366532_720w.jpg" alt="img"></p>
<h3 id="2-1-1-一维列联表"><a href="#2-1-1-一维列联表" class="headerlink" title="2.1.1 一维列联表"></a>2.1.1 一维列联表</h3><p>可以使用table()函数生成简单的频数统计表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mytable &lt;- with(Arthritis,table(Improved))</span><br><span class="line">mytable</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-eabe2646cde6814b487b071eebb2f281_720w.jpg" alt="img"></p>
<p>可以用prop.table()将这些频数转化为比例值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">prop.table(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-1f6818def8bbcfe0bd5c3031c902257d_720w.jpg" alt="img"></p>
<h3 id="2-1-2-二维列联表"><a href="#2-1-2-二维列联表" class="headerlink" title="2.1.2 二维列联表"></a>2.1.2 二维列联表</h3><p>对于二维列联表，table()函数的使用格式为：</p>
<p>mytable &lt;- table(A, B)</p>
<p>其中的A是行变量，B是列变量。除此之外，xtabs()函数还可使用公式风格的输入创建列联表，格式为：</p>
<p>mytable &lt;- xtabs(~ A + B, data=mydata)</p>
<p>其中的mydata是一个矩阵或数据框。总的来说，要进行交叉分类的变量应出现在公式的右侧 （即~符号的右方），以+作为分隔符。若某个变量写在公式的左侧，则其为一个频数向量（在数据已经被表格化时很有用） 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mytable &lt;- xtabs(~Treatment+Improved,data&#x3D;Arthritis)</span><br><span class="line">mytable</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-56bd9cc92a366a59f757ecf77cd84312_720w.jpg" alt="img"></p>
<p>可以使用margin.table()和prop.table()函数分别生成边际频数和比例。行和与行比例可以这样计算：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">margin.table(mytable,1)</span><br><span class="line">prop.table(mytable,1)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-9326520883031626c2a575be5a2e8090_720w.jpg" alt="img"></p>
<p>下标1指代table()语句中的第一个变量。观察表格可以发现，与接受安慰剂的个体中有显著改善的16%相比，接受治疗的个体中的51%的个体病情有了显著的改善。</p>
<p>列和与列比例可以这样计算：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">margin.table(mytable,2)</span><br><span class="line">prop.table(mytable,2)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-57aef697a03f4b4d62e0e187961ce011_720w.jpg" alt="img"></p>
<p>这里的下标2指代table()语句中的第二个变量。</p>
<p>各单元格所占比例可用如下语句获取：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">prop.table(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-68e40ba2b2c4c7c70091647b8a8517f1_720w.jpg" alt="img"></p>
<p>可以使用addmargins()函数为这些表格添加边际和。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">addmargins(mytable)</span><br><span class="line">addmargins(prop.table(mytable))</span><br><span class="line"># 仅添加了各行和各列的和</span><br><span class="line">addmargins(prop.table(mytable,1),2)</span><br><span class="line">addmargins(prop.table(mytable,2),1)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-f4f4ee9f11ce0614ee839d46e00f8e76_720w.jpg" alt="img"></p>
<p>使用gmodels包中的CrossTable()函数是创建二维列联表的第三种方法。CrossTable()函数仿照SAS中PROC FREQ或SPSS中CROSSTABS的形式生成二维列联表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(gmodels)</span><br><span class="line">CrossTable(Arthritis$Treatment,Arthritis$Improved)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-944c6dfd0b270d47d69c62eca2881e59_720w.jpg" alt="img"></p>
<h3 id="2-1-3-多维列联表"><a href="#2-1-3-多维列联表" class="headerlink" title="2.1.3 多维列联表"></a>2.1.3 多维列联表</h3><p>table() 和 xtabs() 都 可 以 基 于 三 个 或 更 多 的 类 别 型 变 量 生 成 多 维 列 联 表 。margin.table()、prop.table()和addmargins()函数可以自然地推广到高于二维的情况。另外，ftable()函数可以以一种紧凑而吸引人的方式输出多维列联表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 三维列联表</span><br><span class="line">mytable &lt;- xtabs(~Treatment+Sex+Improved,data&#x3D;Arthritis)</span><br><span class="line">mytable</span><br><span class="line">ftable(mytable)</span><br><span class="line"># 边际频数</span><br><span class="line">margin.table(mytable,1)</span><br><span class="line">margin.table(mytable,2)</span><br><span class="line">margin.table(mytable,3)</span><br><span class="line">margin.table(mytable,c(1,3))</span><br><span class="line">margin.table(mytable,c(1,2))</span><br><span class="line">ftable(prop.table(mytable, c(1, 2))) </span><br><span class="line">ftable(addmargins(prop.table(mytable, c(1, 2)), 3))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-f7fe47703252fd15da9760065638c9b1_720w.jpg" alt="img"></p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-be904c2c25c43a6815a79a16b9c931e5_720w.jpg" alt="img"></p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-8b0b97c90344759abdd96cc5194ec7ef_720w.jpg" alt="img"></p>
<h2 id="2-2-独立性检验"><a href="#2-2-独立性检验" class="headerlink" title="2.2 独立性检验"></a>2.2 独立性检验</h2><h3 id="2-2-1-卡方独立性检验"><a href="#2-2-1-卡方独立性检验" class="headerlink" title="2.2.1 卡方独立性检验"></a>2.2.1 卡方独立性检验</h3><p>可以使用chisq.test()函数对二维表的行变量和列变量进行卡方独立性检验。</p>
<p>这里的p值表示从总体中抽取的样本行变量与列变量是相互独立的概率。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(vcd)</span><br><span class="line">mytable &lt;- xtabs(~Treatment+Improved, data&#x3D;Arthritis)</span><br><span class="line">chisq.test(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-14c7354be5c3b24d058efce40a7bf264_720w.jpg" alt="img"></p>
<p>结果中，患者接受的治疗和改善的水平看上去存在着某种关系（p&lt;0.01）(不独立)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mytable &lt;- xtabs(~Improved+Sex,data&#x3D;Arthritis)</span><br><span class="line">chisq.test(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-d5617a30cc6df7df25742c48303f9f35_720w.jpg" alt="img"></p>
<p>患者性别和改善情况之间却不存在关系（p&gt;0.05）(独立)。</p>
<h3 id="2-2-2-Fisher精确检验"><a href="#2-2-2-Fisher精确检验" class="headerlink" title="2.2.2 Fisher精确检验"></a>2.2.2 Fisher精确检验</h3><p>可以使用fisher.test()函数进行Fisher精确检验。Fisher精确检验的原假设是：边界固定的列联表中行和列是相互独立的。其调用格式为fisher.test(mytable)，其中的mytable是一个二维列联表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mytable &lt;- xtabs(~Treatment+Improved,data&#x3D;Arthritis)</span><br><span class="line">fisher.test(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-7dcdadec1b25fbd0e07ea36c5cd2f0a1_720w.jpg" alt="img"></p>
<p>结果中，患者接受的治疗和改善的水平看上去存在着某种关系（p&lt;0.01）(不独立)。</p>
<h3 id="2-2-3-Cochran-Mantel-Haenszel检验"><a href="#2-2-3-Cochran-Mantel-Haenszel检验" class="headerlink" title="2.2.3 Cochran-Mantel-Haenszel检验"></a>2.2.3 Cochran-Mantel-Haenszel检验</h3><p>mantelhaen.test()函数可用来进行Cochran-Mantel-Haenszel卡方检验，其原假设是， 两个名义变量在第三个变量的每一层中都是条件独立的。下列代码可以检验治疗情况和改善情况在性别的每一水平下是否独立。此检验假设不存在三阶交互作用（治疗情况×改善情况×性别） 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mytable &lt;- xtabs(~Treatment+Improved+Sex,data&#x3D;Arthritis)</span><br><span class="line">mantelhaen.test(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-c6167b3cd925aad769c5ef748bd96587_720w.jpg" alt="img"></p>
<p>结果表明，患者接受的治疗与得到的改善在性别的每一水平下并不独立（分性别来看，用药治疗的患者较接受安慰剂的患者有了更多的改善） 。</p>
<h2 id="2-3-相关性度量"><a href="#2-3-相关性度量" class="headerlink" title="2.3 相关性度量"></a>2.3 相关性度量</h2><h3 id="2-3-1-相关性度量指标"><a href="#2-3-1-相关性度量指标" class="headerlink" title="2.3.1 相关性度量指标"></a>2.3.1 相关性度量指标</h3><p>显著性检验评估了是否存在充分的证据以拒绝变量间相互独立的原假设。如果可以拒绝原假设，那么你的兴趣就会自然而然地转向用以衡量相关性强弱的相关性度量。vcd包中的assocstats()函数可以用来计算二维列联表的phi系数、列联系数和Cramer’s V系数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(vcd)</span><br><span class="line">mytable &lt;- xtabs(~Treatment+Improved,data&#x3D;Arthritis)</span><br><span class="line">assocstats(mytable)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-da1d9da82243a58dd56873294b5bc31f_720w.jpg" alt="img"></p>
<p>总体来说，较大的值意味着较强的相关性。</p>
<h3 id="2-3-2-相关性度量的可视化-类别型变量-马赛克图"><a href="#2-3-2-相关性度量的可视化-类别型变量-马赛克图" class="headerlink" title="2.3.2 相关性度量的可视化(类别型变量):马赛克图"></a>2.3.2 相关性度量的可视化(类别型变量):马赛克图</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(vcd)</span><br><span class="line">mosaic(~Treatment+Improved+Sex,data&#x3D;Arthritis,shade&#x3D;TRUE,legend&#x3D;TRUE)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-bf383ec481c0583433fc514d80beba76_720w.jpg" alt="img"></p>
<h1 id="3-相关"><a href="#3-相关" class="headerlink" title="3.相关"></a>3.相关</h1><p>相关系数可以用来描述定量变量之间的关系。相关系数的符号（±）表明关系的方向（正相关或负相关） ，其值的大小表示关系的强弱程度（完全不相关时为0，完全相关时为1） 。 本节中，我们将关注多种相关系数和相关性的显著性检验。我们将使用R基础安装中的state.x77数据集，它提供了美国50个州在1977年的人口、收入、文盲率、预期寿命、谋杀率和高中毕业率数据。数据集中还收录了气温和土地面积数据，但为了节约空间，这里将其丢弃。你可以使用help(state.x77)了解数据集的更多信息。除了基础安装以外，我们还将使用psych和ggm包。</p>
<h2 id="3-1-相关的类型"><a href="#3-1-相关的类型" class="headerlink" title="3.1 相关的类型"></a>3.1 相关的类型</h2><p>R可以计算多种相关系数，包括Pearson相关系数、Spearman相关系数、Kendall相关系数、偏相关系数、多分格（polychoric）相关系数和多系列（polyserial）相关系数。</p>
<h3 id="3-1-1-Pearson、Spearman和Kendall相关"><a href="#3-1-1-Pearson、Spearman和Kendall相关" class="headerlink" title="3.1.1 Pearson、Spearman和Kendall相关"></a>3.1.1 Pearson、Spearman和Kendall相关</h3><p>Pearson积差相关系数衡量了两个定量变量之间的线性相关程度。Spearman等级相关系数则衡量分级定序变量之间的相关程度。Kendall’sTau相关系数也是一种非参数的等级相关度量。cor()函数可以计算这三种相关系数，而cov()函数可用来计算协方差。两个函数的参数有很多，其中与相关系数的计算有关的参数可以简化为：</p>
<p>cor(x, use= , method= )</p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-ed29b85654228302f4b8881f00dde68f_720w.jpg" alt="img"></p>
<p>默认参数为use=”everything”和method=”pearson”。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">states &lt;- state.x77[,1:6]</span><br><span class="line">cov(states)</span><br><span class="line">cor(states)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-e4ed2232587b65753a8e369b7ef88317_720w.jpg" alt="img"></p>
<p>我们可以看到收入和高中毕业率之间存在很强的正相关，而文盲率和预期寿命之间存在很强的负相关。</p>
<p>在默认情况下得到的结果是一个方阵（所有变量之间两两计算相关） 。当你对某一组变量与另外一组变量之间的关系感兴趣时,你同样可以计算非方形的相关矩阵。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- states[,c(&quot;Population&quot;, &quot;Income&quot;, &quot;Illiteracy&quot;, &quot;HS Grad&quot;)] </span><br><span class="line">y &lt;- states[,c(&quot;Life Exp&quot;, &quot;Murder&quot;)]  </span><br><span class="line">cor(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-020367797dcb742a83ce3e24b2ca0f35_720w.jpg" alt="img"></p>
<p><strong>注意，上述结果并未指明相关系数是否显著不为0（即，根据样本数据是否有足够的证据得出总体相关系数不为0的结论）。由于这个原因，你需要对相关系数进行显著性检验。</strong></p>
<h3 id="3-1-2-偏相关"><a href="#3-1-2-偏相关" class="headerlink" title="3.1.2 偏相关"></a>3.1.2 偏相关</h3><p>偏相关是指在控制一个或多个定量变量时，另外两个定量变量之间的相互关系。你可以使用ggm包中的pcor()函数计算偏相关系数。函数调用格式为：</p>
<p>pcor(u, S)</p>
<p>其中的u是一个数值向量，前两个数值表示要计算相关系数的变量下标，其余的数值为条件变量（即要排除影响的变量）的下标。S为变量的协方差阵。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(ggm)</span><br><span class="line">colnames(states)</span><br><span class="line">pcor(c(1,5,2,3,6),cov(states))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-62da63dd940bc0f83a6f33a296481ecf_720w.jpg" alt="img"></p>
<p>在控制了收入、文盲率和高中毕业率的影响时，人口和谋杀率之间的相关系数为0.346。偏相关系数常用于社会科学的研究中。</p>
<h2 id="3-2-相关性显著性检验"><a href="#3-2-相关性显著性检验" class="headerlink" title="3.2 相关性显著性检验"></a>3.2 相关性显著性检验</h2><p>在计算好相关系数以后， 如何对它们进行统计显著性检验呢？常用的原假设为变量间不相关（即总体的相关系数为0）。</p>
<p>(1) 可以使用cor.test()函数对单个的Pearson、Spearman和Kendall相关系数进行检验。</p>
<p>简化后的使用格式为：</p>
<p>cor.test(x, y, alternative = , method = )</p>
<p>其中的x和y为要检验相关性的变量，alternative则用来指定进行双侧检验或单侧检验（取值为”two.side”、”less”或”greater”），而method用以指定要计算的相关类型（”pearson”、”kendall” 或 “spearman” ） 。 当 研 究 的 假 设 为 总 体 的 相 关 系 数 小 于 0 时 ， 请 使 用alternative=”less” 。 在 研 究 的 假 设 为 总 体 的相关系 数 大 于 0 时 ， 应 使 用alternative=”greater”。在默认情况下，假设为alternative=”two.side”（总体相关系数不等于0） 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cor.test(states[,3],states[,5])</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-3da029401fc1c12fc43a78d14f466543_720w.jpg" alt="img"></p>
<p>这段代码检验了预期寿命和谋杀率的Pearson相关系数为0的原假设。假设总体的相关度为0，则预计在一千万次中只会有少于一次的机会见到0.703这样大的样本相关度（即p=1.258e–08） 。由于这种情况几乎不可能发生，所以你可以拒绝原假设，从而支持了要研究的猜想，即预期寿命和谋杀率之间的总体相关度不为0。</p>
<p>(2) 通过corr.test计算相关矩阵并进行显著性检验:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(psych)</span><br><span class="line">corr.test(states,use&#x3D;&quot;complete&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-8092025a9c4f220a10b29b8553e4e025_720w.jpg" alt="img"></p>
<p>参数use=的取值可为”pairwise”或”complete”（分别表示对缺失值执行成对删除或行删除）。参数method=的取值可为”pearson”（默认值）、”spearman”或”kendall”。这里可以看到，人口数量和高中毕业率的相关系数（–0.10）并不显著地不为0（p=0.5） 。</p>
<h2 id="3-3-相关系数的可视化"><a href="#3-3-相关系数的可视化" class="headerlink" title="3.3 相关系数的可视化"></a>3.3 相关系数的可视化</h2><h3 id="3-3-1-散点图"><a href="#3-3-1-散点图" class="headerlink" title="3.3.1 散点图"></a>3.3.1 散点图</h3><p>(1) 添加了最佳拟合曲线的散点图</p>
<p>abline()函数用来添加最佳拟合的线性直线， 而lowess()函数则用来添加一条平滑曲线。 该平滑曲线拟合是一种基于局部加权多项式回归的非参数方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">attach(mtcars)  </span><br><span class="line">plot(wt, mpg, </span><br><span class="line">     main&#x3D;&quot;Basic Scatter plot of MPG vs. Weight&quot;, </span><br><span class="line">     xlab&#x3D;&quot;Car Weight (lbs&#x2F;1000)&quot;, </span><br><span class="line">     ylab&#x3D;&quot;Miles Per Gallon &quot;, pch&#x3D;19) </span><br><span class="line">abline(lm(mpg~wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) </span><br><span class="line">lines(lowess(wt,mpg), col&#x3D;&quot;blue&quot;, lwd&#x3D;2, lty&#x3D;2)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-785d6b790ae8c195748540284bcc6e98_720w.jpg" alt="img"></p>
<p>(2) car包中的scatterplot()函数增强了散点图的许多功能</p>
<p>此处，scatterplot()函数用来绘制四缸、六缸和八缸汽车每加仑英里数对车重的图形。表达式mpg ~ wt | cyl表示按条件绘图（即按cyl的水平分别绘制mpg和wt的关系图）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(car)  </span><br><span class="line">scatterplot(mpg ~ wt | cyl, data&#x3D;mtcars, lwd&#x3D;2, span&#x3D;0.75, </span><br><span class="line">            main&#x3D;&quot;Scatter Plot of MPG vs. Weight by # Cylinders&quot;, </span><br><span class="line">            xlab&#x3D;&quot;Weight of Car (lbs&#x2F;1000)&quot;, </span><br><span class="line">            ylab&#x3D;&quot;Miles Per Gallon&quot;, </span><br><span class="line">            legend.plot&#x3D;TRUE, </span><br><span class="line">            id.method&#x3D;&quot;identify&quot;, </span><br><span class="line">            #labels&#x3D;row.names(mtcars), </span><br><span class="line">            boxplots&#x3D;&quot;xy&quot; </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-eb708d7f4869fb0c84b1d2b201f4fc6a_720w.jpg" alt="img"></p>
<p>(3) 散点图矩阵</p>
<p>pairs()函数可以创建基础的散点图矩阵。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pairs(~mpg+disp+drat+wt, data&#x3D;mtcars, </span><br><span class="line">      main&#x3D;&quot;Basic Scatter Plot Matrix&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-9548b951880161b515b96076c87cfe63_720w.jpg" alt="img"></p>
<p>car包中的scatterplotMatrix()函数也可以生成散点图矩阵:</p>
<ul>
<li>以某个因子为条件绘制散点图矩阵；</li>
<li>包含线性和平滑拟合曲线；</li>
<li>在主对角线放置箱线图、密度图或者直方图；</li>
<li>在各单元格的边界添加轴须图。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(car)  </span><br><span class="line">scatterplotMatrix(~ mpg + disp + drat + wt, data&#x3D;mtcars, </span><br><span class="line">                  spread&#x3D;FALSE, smoother.args&#x3D;list(lty&#x3D;2), </span><br><span class="line">                  main&#x3D;&quot;Scatter Plot Matrix via car Package&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-27d029d69a1fe74927775a140eed4cc9_720w.jpg" alt="img"></p>
<p>(4) 高密度散点图</p>
<p>当数据点重叠很严重时，用散点图来观察变量关系就显得“力不从心”了。</p>
<p>smoothScatter()函数可利用核密度估计生成用颜色密度来表示点分布的散点图。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set.seed(1234) </span><br><span class="line">n &lt;- 10000 </span><br><span class="line">c1 &lt;- matrix(rnorm(n, mean&#x3D;0, sd&#x3D;.5), ncol&#x3D;2) </span><br><span class="line">c2 &lt;- matrix(rnorm(n, mean&#x3D;3, sd&#x3D;2), ncol&#x3D;2) </span><br><span class="line">mydata &lt;- rbind(c1, c2) </span><br><span class="line">mydata &lt;- as.data.frame(mydata) </span><br><span class="line">names(mydata) &lt;- c(&quot;x&quot;, &quot;y&quot;) </span><br><span class="line">with(mydata,smoothScatter(x,y,main&#x3D;&quot;Scatter Plot Colored by Smoothed Densities&quot;))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-7d4a0d5aaed85d28b8b5334118266439_720w.jpg" alt="img"></p>
<p>hexbin包中的hexbin()函数将二元变量的封箱放到六边形单元格中（图形比名称更直观） 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(hexbin)</span><br><span class="line">with(mydata,&#123;bin &lt;- hexbin(x,y,xbins&#x3D;50)</span><br><span class="line">plot(bin,main &#x3D; &quot;Hexagonal Binning with 10,000 Observations&quot;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-7c26291c28d5db5d9c51c43ed8b2cf7c_720w.jpg" alt="img"></p>
<p>(5) 三维散点图</p>
<p>散点图和散点图矩阵展示的都是二元变量关系。 倘若你想一次对三个定量变量的交互关系进行可视化呢？</p>
<p>假使你对汽车英里数、车重和排量间的关系感兴趣，可用scatterplot3d包中的scatterplot3d()函数来绘制它们的关系。格式如下：</p>
<p>scatterplot3d(x, y, z)</p>
<p>x被绘制在水平轴上，y被绘制在竖直轴上，z被绘制在透视轴上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(scatterplot3d)  </span><br><span class="line">attach(mtcars) </span><br><span class="line">scatterplot3d(wt, disp, mpg, </span><br><span class="line">    main&#x3D;&quot;Basic 3D Scatter Plot&quot;)</span><br><span class="line">&#96;&#96;&#96;</span><br><span class="line">satterplot3d()函数提供了许多选项，包括设置图形符号、轴、颜色、线条、网格线、突出显示和角度等功能。         </span><br><span class="line">&#96;&#96;&#96;&#123;r&#125;</span><br><span class="line">library(scatterplot3d) </span><br><span class="line">attach(mtcars)  </span><br><span class="line">scatterplot3d(wt, disp, mpg, </span><br><span class="line">              pch&#x3D;16, </span><br><span class="line">              highlight.3d&#x3D;TRUE, </span><br><span class="line">              type&#x3D;&quot;h&quot;, </span><br><span class="line">              main&#x3D;&quot;3D Scatter Plot with Vertical Lines&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-cd861dfd961bb7e1d8bdc736bb6cf638_720w.jpg" alt="img"></p>
<p>我们在刚才那幅图上添加一个回归面。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(scatterplot3d)  </span><br><span class="line">attach(mtcars) </span><br><span class="line">s3d &lt;-scatterplot3d(wt, disp, mpg, </span><br><span class="line">      pch&#x3D;16, </span><br><span class="line">      highlight.3d&#x3D;TRUE, </span><br><span class="line">      type&#x3D;&quot;h&quot;, </span><br><span class="line">      main&#x3D;&quot;3D Scatter Plot with Vertical Lines and Regression Plane&quot;) </span><br><span class="line">fit &lt;- lm(mpg ~ wt+disp) </span><br><span class="line">s3d$plane3d(fit)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-f97a3592e3295ced265e053acb5155e3_720w.jpg" alt="img"></p>
<p>(6) 气泡图</p>
<p>先创建一个二维散点图， 然后用点的大小来代表第三个变量的值。这便是气泡图（bubbleplot）。你可用symbols()函数来创建气泡图。该函数可以在指定的(x,y)坐标上绘制圆圈图、方形图、星形图、温度计图和箱线图。以绘制圆圈图为例： symbols(x, y, circle=radius) 其中x、y和radius是需要设定的向量，分别表示x、y坐标和圆圈半径。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">attach(mtcars)  </span><br><span class="line">r &lt;- sqrt(disp&#x2F;pi) </span><br><span class="line">symbols(wt, mpg, circle&#x3D;r, inches&#x3D;0.30, </span><br><span class="line">        fg&#x3D;&quot;white&quot;, bg&#x3D;&quot;lightblue&quot;, </span><br><span class="line">        main&#x3D;&quot;Bubble Plot with point size proportional to displacement&quot;, </span><br><span class="line">        ylab&#x3D;&quot;Miles Per Gallon&quot;, </span><br><span class="line">        xlab&#x3D;&quot;Weight of Car (lbs&#x2F;1000)&quot;) </span><br><span class="line"># text(wt, mpg, rownames(mtcars), cex&#x3D;0.6) </span><br><span class="line">detach(mtcars)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-d9d1490084558962c6499afb25bdc0c3_720w.jpg" alt="img"></p>
<p>一般来说，统计人员使用R时都倾向于避免用气泡图，原因和避免使用饼图一样：相比对长度的判断，人们对体积/面积的判断通常更困难。但是气泡图在商业应用中非常受欢迎。</p>
<h3 id="3-3-2-折线图"><a href="#3-3-2-折线图" class="headerlink" title="3.3.2 折线图"></a>3.3.2 折线图</h3><p>如果将散点图上的点从左往右连接起来，就会得到一个折线图。</p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-04bce0238cce2474611f2f4f7e0e25e6_720w.jpg" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">opar &lt;- par(no.readonly&#x3D;TRUE) </span><br><span class="line">par(mfrow&#x3D;c(1,2))  </span><br><span class="line">t1 &lt;- subset(Orange, Tree&#x3D;&#x3D;1) </span><br><span class="line">plot(t1$age, t1$circumference, </span><br><span class="line">     xlab&#x3D;&quot;Age (days)&quot;, </span><br><span class="line">     ylab&#x3D;&quot;Circumference (mm)&quot;, </span><br><span class="line">     main&#x3D;&quot;Orange Tree 1 Growth&quot;) </span><br><span class="line">plot(t1$age, t1$circumference, </span><br><span class="line">     xlab&#x3D;&quot;Age (days)&quot;, </span><br><span class="line">     ylab&#x3D;&quot;Circumference (mm)&quot;, </span><br><span class="line">     main&#x3D;&quot;Orange Tree 1 Growth&quot;, </span><br><span class="line">     type&#x3D;&quot;b&quot;) </span><br><span class="line">par(opar)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-e0c28fdb60f1a6f2c7e609f0fa479c4c_720w.jpg" alt="img"></p>
<h3 id="3-3-3-相关图"><a href="#3-3-3-相关图" class="headerlink" title="3.3.3 相关图"></a>3.3.3 相关图</h3><p>利用corrgram包中的corrgram()函数，你可以用图形的方式展示该相关系数矩阵。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(corrgram)  </span><br><span class="line">corrgram(mtcars, order&#x3D;TRUE, lower.panel&#x3D;panel.shade, </span><br><span class="line">         upper.panel&#x3D;panel.pie, text.panel&#x3D;panel.txt, </span><br><span class="line">         main&#x3D;&quot;Corrgram of mtcars intercorrelations&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-322d850317327893382beb102ef4feba_720w.jpg" alt="img"></p>
<p>我们先从下三角单元格（在主对角线下方的单元格）开始解释这幅图形。默认地，蓝色和从左下指向右上的斜杠表示单元格中的两个变量呈正相关。反过来，红色和从左上指向右下的斜杠表示变量呈负相关。色彩越深，饱和度越高，说明变量相关性越大。相关性接近于0的单元格基本无色。上三角单元格用饼图展示了相同的信息。颜色的功能同上，但相关性大小由被填充的饼图块的大小来展示。 正相关性将从12点钟处开始顺时针填充饼图， 而负相关性则逆时针方向填充饼图。</p>
<p>corrgram()函数的格式如下：</p>
<p>corrgram(x, order=, panel=, text.panel=, diag.panel=)</p>
<p>其中，x是一行一个观测的数据框。当order=TRUE时，相关矩阵将使用主成分分析法对变量重排序，这将使得二元变量的关系模式更为明显。选项panel设定非对角线面板使用的元素类型。你可以通过选项lower.panel和upper.panel来分别设置主对角线下方和上方的元素类型。而text.panel和diag.panel选项控制着主对角线元素类型。</p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-62268d5df6923cd36f5fd75a44c4f801_720w.jpg" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(corrgram)  </span><br><span class="line">corrgram(mtcars, order&#x3D;TRUE, lower.panel&#x3D;panel.ellipse, </span><br><span class="line">         upper.panel&#x3D;panel.pts, text.panel&#x3D;panel.txt, </span><br><span class="line">         diag.panel&#x3D;panel.minmax, </span><br><span class="line">         main&#x3D;&quot;Corrgram of mtcars data using scatter plots </span><br><span class="line">               and ellipses&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-87b3236d14ece13573241bf810ee619b_720w.jpg" alt="img"></p>
<p>最后一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(corrgram)  </span><br><span class="line">corrgram(mtcars, lower.panel&#x3D;panel.shade, </span><br><span class="line">         upper.panel&#x3D;NULL, text.panel&#x3D;panel.txt, </span><br><span class="line">         main&#x3D;&quot;Car Mileage Data (unsorted)&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-8f348bc44894ede15fcd912809f0d70d_720w.jpg" alt="img"></p>
<h3 id="3-3-4-马赛克图"><a href="#3-3-4-马赛克图" class="headerlink" title="3.3.4 马赛克图"></a>3.3.4 马赛克图</h3><p>vcd包中的mosaic()函数可以绘制马赛克图。 （R基础安装中的mosaicplot()也可绘制马赛克图，但我还是推荐vcd包，因为它具有更多扩展功能。）以基础安装中的Titanic数据集为例，它包含存活或者死亡的乘客数、乘客的船舱等级（一等、二等、三等和船员） 、性别（男性、女性） ，以及年龄层（儿童、成人）。这是一个被充分研究过的数据集。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(vcd) </span><br><span class="line">mosaic(~Class+Sex+Age+Survived, data&#x3D;Titanic, shade&#x3D;TRUE, legend&#x3D;TRUE)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-9aa752871ba075ea62b9f3409b5af814_720w.jpg" alt="img"></p>
<p>在本例中，蓝色阴影表明，在假定生存率与船舱等级、性别和年龄层无关的条件下，该类别下的生存率通常超过预期值。红色阴影则含义相反。</p>
<p>赛克图隐含着大量的数据信息。例如：(1) 从船员到头等舱，存活率陡然提高；(2)大部分孩子都处在三等舱和二等舱中；(3)在头等舱中的大部分女性都存活了下来，而三等舱仅有一半女性存活；(4) 船员中女性很少，导致该组的Survived标签重叠（图底部的No和Yes） 。</p>
<h1 id="4-t检验"><a href="#4-t检验" class="headerlink" title="4.t检验"></a>4.t检验</h1><h2 id="4-1-独立样本的-t-检验"><a href="#4-1-独立样本的-t-检验" class="headerlink" title="4.1 独立样本的 t 检验"></a>4.1 独立样本的 t 检验</h2><p>如果你在美国的南方犯罪，是否更有可能被判监禁？我们比较的对象是南方和非南方各州，因变量为监禁的概率。一个针对两组的独立样本t检验<strong>可以用于检验两个总体的均值相等的假设</strong>。这里假设两组数据是独立的，并且是从正态总体中抽得。检验的调用格式为：</p>
<p>t.test(y ~ x, data)</p>
<p>其中的y是一个数值型变量，x是一个二分变量。调用格式或为：</p>
<p>t.test(y1, y2)</p>
<p>其中的y1和y2为数值型向量（即各组的结果变量） 。可选参数data的取值为一个包含了这些变量的矩阵或数据框。与其他多数统计软件不同的是，这里的t检验默认假定方差不相等，并使用Welsh的修正自由度。你可以添加一个参数var.equal=TRUE以假定方差相等，并使用合并方差估计。默认的备择假设是双侧的（即均值不相等，但大小的方向不确定）。你可以添加一个参数alternative=”less”或alternative=”greater”来进行有方向的检验。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">head(UScrime)</span><br><span class="line">t.test(Prob~So,data&#x3D;UScrime)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-fb6bd99acfdc234f1389ba509da7de81_720w.jpg" alt="img"></p>
<p>你可以拒绝南方各州和非南方各州拥有相同监禁概率的假设（p&lt;0.001） 。</p>
<h2 id="4-2-非独立样本t检验"><a href="#4-2-非独立样本t检验" class="headerlink" title="4.2 非独立样本t检验"></a>4.2 非独立样本t检验</h2><p>再举个例子，你可能会问：较年轻（14~24岁）男性的失业率是否比年长（35~39岁）男性的失业率更高？在这种情况下，这两组数据并不独立。你不能说亚拉巴马州的年轻男性和年长男性的失业率之间没有关系。</p>
<p><strong>非独立样本的t检验假定组间的差异呈正态分布</strong>。对于本例，检验的调用格式为：</p>
<p>t.test(y1, y2, paired=TRUE)</p>
<p>其中的y1和y2为两个非独立组的数值向量。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">sapply(UScrime[c(&quot;U1&quot;,&quot;U2&quot;)], function(x)(c(mean(x),sd(x))))</span><br><span class="line">with(UScrime,t.test(U1,U2,paired&#x3D;TRUE))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-047e0d929a945486decafe20fb4e0455_720w.jpg" alt="img"></p>
<p>差异的均值（61.5）足够大，可以保证拒绝年长和年轻男性的平均失业率相同的假设。年轻男性的失业率更高。事实上，若总体均值相等，获取一个差异如此大的样本的概率小于0.000 000 000 000 000 22（即2.2e–16） 。</p>
<h1 id="5-组间差异的非参数检验"><a href="#5-组间差异的非参数检验" class="headerlink" title="5.组间差异的非参数检验"></a>5.组间差异的非参数检验</h1><p>如果数据无法满足t检验或ANOVA的参数假设，若结果变量在本质上就严重偏倚或呈现有序关系，可以转而使用非参数方法。</p>
<h2 id="5-1-两组的比较"><a href="#5-1-两组的比较" class="headerlink" title="5.1 两组的比较"></a>5.1 两组的比较</h2><p>若两组数据独立，可以使用Wilcoxon秩和检验（更广为人知的名字是Mann-WhitneyU检验）来评估观测是否是从相同的概率分布中抽得的（即，在一个总体中获得更高得分的概率是否比另一个总体要大） 。调用格式为：</p>
<p>wilcox.test(y ~ x, data)</p>
<p>其中的y是数值型变量，而x是一个二分变量。调用格式或为：</p>
<p>wilcox.test(y1, y2)</p>
<p>其中的y1和y2为各组的结果变量。可选参数data的取值为一个包含了这些变量的矩阵或数据框。默认进行一个双侧检验。你可以添加参数exact来进行精确检验，指定alternative=”less”或alternative=”greater”进行有方向的检验。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with(UScrime, by(Prob, So, median)) </span><br><span class="line">with(UScrime,wilcox.test(Prob~So,data&#x3D;UScrime))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-613a74ae52943f25e2f1686a1eacfe7b_720w.jpg" alt="img"></p>
<p>你可以再次拒绝南方各州和非南方各州监禁率相同的假设（p&lt;0.001） 。</p>
<p>Wilcoxon符号秩检验是非独立样本t检验的一种非参数替代方法。它适用于两组成对数据和无法保证正态性假设的情境。调用格式与Mann-WhitneyU检验完全相同，不过还可以添加参数paired=TRUE。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sapply(UScrime[c(&quot;U1&quot;,&quot;U2&quot;)], median)</span><br><span class="line">with(UScrime,wilcox.test(U1,U2,paired &#x3D; TRUE))</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-93154106af0a60e4a3ec71d4a8785dca_720w.jpg" alt="img"></p>
<p>你再次得到了与配对t检验相同的结论。</p>
<p>在本例中，含参的t检验和与其作用相同的非参数检验得到了相同的结论。当t检验的假设合理时，参数检验的功效更强（更容易发现存在的差异）。而非参数检验在假设非常不合理时（如<strong>对于等级有序数据</strong>）更适用。</p>
<h2 id="5-2-多于两组的比较"><a href="#5-2-多于两组的比较" class="headerlink" title="5.2 多于两组的比较"></a>5.2 多于两组的比较</h2><p>考虑state.x77数据集。它包含了美国各州的人口、收入、文盲率、预期寿命、谋杀率和高中毕业率数据。如果你想比较美国四个地区（东北部、南部、中北部和西部）的文盲率， 应该怎么做呢？这称为单向设计 （one-waydesign），我们可以使用参数或非参数的方法来解决这个问题。如果无法满足ANOVA设计的假设，那么可以使用非参数方法来评估组间的差异。如果各组独立，则Kruskal-Wallis检验将是一种实用的方法。如果各组不独立（如重复测量设计或随机区组设计），那么Friedman检验会更合适。</p>
<p>Kruskal-Wallis检验的调用格式为：</p>
<p>kruskal.test(y ~ A, data)</p>
<p>其中的y是一个数值型结果变量，A是一个拥有两个或更多水平的分组变量（grouping variable）。（若有两个水平，则它与Mann-Whitney U检验等价。 ）</p>
<p>而Friedman检验的调用格式为：</p>
<p>friedman.test(y ~ A | B, data)</p>
<p>其中的y是数值型结果变量，A是一个分组变量，而B是一个用以认定匹配观测的区组变量（blockingvariable）。在以上两例中，data皆为可选参数，它指定了包含这些变量的矩阵或数据框。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">states &lt;- data.frame(state.region, state.x77) </span><br><span class="line">kruskal.test(Illiteracy ~ state.region, data&#x3D;states)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-d4e51ee79856f83cc3814cd589f68244_720w.jpg" alt="img"></p>
<p>显著性检验的结果意味着美国四个地区的文盲率各不相同（p&lt;0.001） 。</p>
<p>虽然你可以拒绝不存在差异的原假设， 但这个检验并没有告诉你哪些地区显著地与其他地区不同。要回答这个问题，你可以使用Wilcoxon检验每次比较两组数据。一种更为优雅的方法是在控制犯第一类错误的概率（发现一个事实上并不存在的差异的概率）的前提下，执行可以同步进行的多组比较， 这样可以直接完成所有组之间的成对比较。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Nonparametric pairwise multiple comparisons using the Wilcoxon Signed Rank Test</span><br><span class="line"># Probability values are adjusted using the p.adjust function</span><br><span class="line">wmc &lt;- function(formula, data, exact&#x3D;FALSE, sort&#x3D;TRUE, method&#x3D;&quot;holm&quot;)&#123;</span><br><span class="line"></span><br><span class="line">  # setup</span><br><span class="line">  df &lt;- model.frame(formula, data)</span><br><span class="line">  y &lt;- df[[1]]</span><br><span class="line">  x &lt;- as.factor(df[[2]])</span><br><span class="line"> </span><br><span class="line">  </span><br><span class="line">  # reorder levels of x by median y</span><br><span class="line">  if(sort)&#123;</span><br><span class="line">    medians &lt;- aggregate(y, by&#x3D;list(x), FUN&#x3D;median)[2]</span><br><span class="line">    index &lt;- order(medians)</span><br><span class="line">    x &lt;- factor(x, levels(x)[index])</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  groups &lt;- levels(x)</span><br><span class="line">  k &lt;- length(groups)</span><br><span class="line">  </span><br><span class="line">  # summary statistics</span><br><span class="line">  stats &lt;- function(z)(c(N &#x3D; length(z), Median &#x3D; median(z), MAD &#x3D; mad(z)))</span><br><span class="line">  sumstats &lt;- t(aggregate(y, by&#x3D;list(x), FUN&#x3D;stats)[2])</span><br><span class="line">  rownames(sumstats) &lt;- c(&quot;n&quot;, &quot;median&quot;, &quot;mad&quot;)</span><br><span class="line">  colnames(sumstats) &lt;- groups</span><br><span class="line">  cat(&quot;Descriptive Statistics\n\n&quot;)</span><br><span class="line">  print(sumstats)</span><br><span class="line">  </span><br><span class="line">  # multiple comparisons</span><br><span class="line">  mc &lt;- data.frame(Group.1&#x3D;character(0), </span><br><span class="line">                   Group.2&#x3D;character(0), </span><br><span class="line">                   W&#x3D;numeric(0),</span><br><span class="line">                   p.unadj&#x3D;numeric(0), </span><br><span class="line">                   p&#x3D;numeric(0),</span><br><span class="line">                   stars&#x3D;character(0),</span><br><span class="line">                   stringsAsFactors&#x3D;FALSE)</span><br><span class="line">  </span><br><span class="line">  # perform Wilcoxon test</span><br><span class="line">  row &lt;- 0</span><br><span class="line">  for(i in 1:k)&#123;</span><br><span class="line">    for(j in 1:k)&#123;</span><br><span class="line">      if (j &gt; i)&#123;</span><br><span class="line">        row &lt;- row + 1</span><br><span class="line">        y1 &lt;- y[x&#x3D;&#x3D;groups[i]]</span><br><span class="line">        y2 &lt;- y[x&#x3D;&#x3D;groups[j]] </span><br><span class="line">        test &lt;- wilcox.test(y1, y2, exact&#x3D;exact)</span><br><span class="line">        mc[row,1] &lt;- groups[i]</span><br><span class="line">        mc[row,2] &lt;- groups[j]</span><br><span class="line">        mc[row,3] &lt;- test$statistic</span><br><span class="line">        mc[row,4] &lt;- test$p.value</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  mc$p &lt;- p.adjust(mc$p.unadj, method&#x3D;method)</span><br><span class="line">  </span><br><span class="line">  # add stars</span><br><span class="line">  mc$stars &lt;- &quot; &quot;</span><br><span class="line">  mc$stars[mc$p &lt;   .1] &lt;- &quot;.&quot;</span><br><span class="line">  mc$stars[mc$p &lt;  .05] &lt;- &quot;*&quot;</span><br><span class="line">  mc$stars[mc$p &lt;  .01] &lt;- &quot;**&quot;</span><br><span class="line">  mc$stars[mc$p &lt; .001] &lt;- &quot;***&quot;</span><br><span class="line">  names(mc)[6] &lt;- &quot; &quot;</span><br><span class="line">  </span><br><span class="line">  cat(&quot;\nMultiple Comparisons (Wilcoxon Rank Sum Tests)\n&quot;)</span><br><span class="line">  cat(paste(&quot;Probability Adjustment &#x3D; &quot;, method, &quot;\n\n&quot;, sep&#x3D;&quot;&quot;))</span><br><span class="line">  print(mc[-4], right&#x3D;TRUE)</span><br><span class="line">  cat(&quot;---\nSignif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1\n&quot;)</span><br><span class="line">  return(invisible(NULL))</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wmc(Illiteracy ~ state.region, data&#x3D;states, method&#x3D;&quot;holm&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/v2-988542c2ecda1aab54a4cba442d63b7e_720w.jpg" alt="img"></p>
<p>wmc()函数首先给出了样本量、样本中位数、每组的绝对中位差。其中，西部地区（West）的文盲率最低，南部地区（South）文盲率最高。然后，函数生成了六组统计比较（南部与中北部（North Central） 、西部与东北部（Northeast）、西部与南部、中北部与东北部、中北部与南部、东北部与南部）。可以从双侧p值（p）看到，南部与其他三个区域有明显差别，但当显著性水平p＜0.05时，其他三个区域间并没有统计显著的差别。</p>
<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>在本次分享中，我们评述了R中用于生成统计概要和进行假设检验的函数。我们关注了样本统计量和频数表、独立性检验和类别型变量的相关性度量、定量变量的相关系数（和连带的显著性检验）以及两组或更多组定量结果变量的比较。下次分享，我们将探索一元回归和多元回归，讨论的焦点在于如何理解一个预测变量（一元回归）或多个预测变量（多元回归）与某个被预测变量或效标变量（criterion variable）之间的关系。图形将有助于诊断潜在的问题、评估和提高模型的拟合精度，并发现数据中意料之外的信息瑰宝。</p>
]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title>R语言实战（一）</title>
    <url>/posts/6ac65c89.html</url>
    <content><![CDATA[<p>R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。在学习R数据科学之前，我们首先要对R语言的基础语法有一个良好的了解，方便我们理解以后的数据科学算法。<strong>本次语法精讲分三次讲完，每次负责讲解其中一部分！本次的R语言语法精讲（一）主要介绍了 R语言的数据结构，R语言的运算以及R语言的编程结构。学完本文后，您将可以具备初步的R语言编程技巧，并能编写大部分程序以及算法。</strong></p>
<p><strong>本文引用：</strong></p>
<blockquote>
<p><strong>《R语言实战—第2版》————-Robert I.Kabacoff</strong><br><strong>《统计计算与模拟》课程课件———-深圳大学林炳清老师</strong></p>
</blockquote>
<p>本文内容：</p>
<ul>
<li>R常用数据结构（本文）</li>
<li>R的运算以及常用函数（本文）</li>
<li>R语言编程结构（本文）<br>判断if-else（本文）<br>循环for、while、repeat（本文）<br>自定义函数（本文）</li>
<li>R的输入与输出</li>
<li>R基础绘图</li>
<li>基本数据管理</li>
<li>高级数据管理</li>
</ul>
<h2 id="1-R常用数据结构"><a href="#1-R常用数据结构" class="headerlink" title="1. R常用数据结构"></a>1. R常用数据结构</h2><p>R拥有许多用于存储数据的对象类型，包括标量、向量、矩阵、数组、数据框和列表。它们在存储数据的类型、创建方式、结构复杂度，以及用于定位和访问其中个别元素的标记等方面均有所不同。</p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-b5d1b28ef8d8b558f8902dcb90450a08_720w.jpg" alt="img"></p>
<h2 id="1-1-向量-Vector"><a href="#1-1-向量-Vector" class="headerlink" title="1.1 向量(Vector)"></a>1.1 向量(Vector)</h2><p>我们可以把vector想象:vector是一串糖葫芦，把山楂都串在一起。vector包含的是一串数据，要求这一串的数据类型是一样的。在R中，常见的数据类型有3种:</p>
<ul>
<li>numeric (数值型, Ex, 1, 3.14, -2。在R中，我们可以不区分整数型，浮点型，双精度型等)</li>
<li>character (字符型, Ex, “a”, “hello world”)</li>
<li>logical (布尔型, Ex, “TRUE” and “FALSE”)</li>
</ul>
<p><strong>(1) 创建向量：</strong></p>
<p>我们可以使用 c() 创造一个vector, c是英文单词cancatenate的缩写，意思是连结，连锁。所以c()可以把括号里的数字或者其他的数据类型的元素串成一个vector. 例如:(&lt;-代表R语言中的赋值符号，绝大多数也可以用=代替；相当于python的“=”)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &lt;- c(1, 2, 5, 3, 6, -2, 4) </span><br><span class="line">a</span><br><span class="line">b &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;) </span><br><span class="line">b</span><br><span class="line">c &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)</span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-6b9204ed8835961830ea09a23fac8335_720w.jpg" alt="img"></p>
<p>这里，a是数值型向量，b是字符型向量，而c是逻辑型向量。 注意，单个向量中的数据必须拥有相同的类型或模式（数值型、字符型或逻辑型） 。同一向量中无法混杂不同模式的数据。</p>
<p><strong>(2) 向量vector的索引</strong></p>
<p>vector索引指的是R会给vector的每个元素一个位置坐标。R的位置坐标从1开始，从左到右依次给予vector的每个元素。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &lt;- c(&quot;k&quot;, &quot;j&quot;, &quot;h&quot;, &quot;a&quot;, &quot;c&quot;, &quot;m&quot;) </span><br><span class="line">a[3]</span><br><span class="line">a[c(1, 3, 5)] </span><br><span class="line">a[2:6]</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-f00d843889afd15b73b30096302b8d49_720w.jpg" alt="img"></p>
<p>此外，我们还可以很方便的输出vector除去某些位置的对应元素后的vector, 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x  &lt;- c(1, 3, 6, 4)</span><br><span class="line">x[-c(3, 4)]</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-24b9ed7c9bb4b00b233ef51a02f86b28_720w.jpg" alt="img"></p>
<p>我们还可以用names()函数给vector的每一个元素赋予一个名字，例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">score &lt;- c(98, 100, 60, 20)                      # score 表示一次考试的成绩</span><br><span class="line">names(score) &lt;- c(&quot;Lin&quot;, &quot;Wang&quot;, &quot;Chen&quot;, &quot;Sun&quot;)  # score 每一个成绩对应的人名</span><br><span class="line">score</span><br><span class="line">score[2]</span><br><span class="line">score[&quot;Wang&quot;]   # 可以通过vector元素的名字得到第二个同学的成绩</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-6e0841e936aabd78245c760ea867afcd_720w.jpg" alt="img"></p>
<p><strong>(3) 产生常用vector的方法：:, seq(), rep()</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1:50</span><br><span class="line">6:2</span><br><span class="line">-5:-10</span><br><span class="line">seq(from &#x3D; 3, to &#x3D; 10)</span><br><span class="line">seq(from &#x3D; 1, to &#x3D; 10, by &#x3D; 2)   # seq()可以自定义递增或者递减的步长</span><br><span class="line">rep(10, 5)    # 产生一个vector, 该vector包含5个10</span><br><span class="line">rep(c(2:4), 3) # 产生一个vector, 该vecotr包含3个 &#96;c(2:4)&#96;</span><br><span class="line">rep(&quot;I Love R!&quot;, 3)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-22ced1d4aa66c80b6d3c4047e6f53d69_720w.jpg" alt="img"></p>
<h2 id="1-2-矩阵matrix"><a href="#1-2-矩阵matrix" class="headerlink" title="1.2 矩阵matrix"></a>1.2 矩阵matrix</h2><p>矩阵是一个二维数组，只是每个元素都拥有相同的模式（数值型、字符型或逻辑型）。可通过函数matrix()创建矩阵。一般使用格式为：</p>
<p>myymatrix &lt;- matrix(vector, nrow=number_of_rows, ncol=number_of_columns,byrow=logical_value, dimnames=list(char_vector_rownames, char_vector_colnames))</p>
<p>其中vector包含了矩阵的元素，nrow和ncol用以指定行和列的维数，dimnames包含了可选的、以字符型向量表示的行名和列名。选项byrow则表明矩阵应当按行填充（byrow=TRUE）还是按列填充（byrow=FALSE） ，默认情况下按列填充。代码清单2-1中的代码演示了matrix函数的用法。</p>
<p><strong>(1) 创建矩阵</strong></p>
<p>我们首先创建了一个5×4的矩阵, 接着创建了一个2×2的含列名标签的矩阵， 并按行进行填 充,最后创建了一个2×2的矩阵并按列进行了填充。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &lt;- matrix(1:20, nrow&#x3D;5, ncol&#x3D;4)  # 创建一个5×4的矩阵</span><br><span class="line">y</span><br><span class="line">cells    &lt;- c(1,26,24,68)</span><br><span class="line">rnames   &lt;- c(&quot;R1&quot;, &quot;R2&quot;)</span><br><span class="line">cnames   &lt;- c(&quot;C1&quot;, &quot;C2&quot;)</span><br><span class="line">mymatrix &lt;- matrix(cells, nrow&#x3D;2, ncol&#x3D;2, byrow&#x3D;TRUE,    # 按行填充的2×2矩阵 </span><br><span class="line">                     dimnames&#x3D;list(rnames, cnames)) </span><br><span class="line">mymatrix</span><br><span class="line">mymatrix &lt;- matrix(cells, nrow&#x3D;2, ncol&#x3D;2, byrow&#x3D;FALSE,  # 按列填充的2×2矩阵 </span><br><span class="line">                     dimnames&#x3D;list(rnames, cnames))</span><br><span class="line">mymatrix</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-6122695124438c5200681555cf2e9ab4_720w.jpg" alt="img"></p>
<p><strong>(2) 矩阵的索引</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:10, nrow&#x3D;2)</span><br><span class="line">x</span><br><span class="line">x[2,]</span><br><span class="line">x[,2]</span><br><span class="line">x[1,4]</span><br><span class="line">x[1, c(4,5)]</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-1b95d74739867a21d1add7623521d206_720w.jpg" alt="img"></p>
<p><strong>(3) 删除矩阵的某些行，列</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- c(3, 1, 5, 2, 3, 8, 5, 8, 9, 4, 2, 3)</span><br><span class="line">x &lt;- matrix(x, nrow &#x3D; 3, ncol &#x3D; 4, byrow &#x3D; T)</span><br><span class="line">x</span><br><span class="line">y &lt;- x[-2, ]</span><br><span class="line">y</span><br><span class="line">z &lt;- x[, -c(2:3)]</span><br><span class="line">z</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-ea29a588644bec8058226764adfd87f8_720w.jpg" alt="img"></p>
<p><strong>(4) 添加一行或一列：cbind或者rbind合并两个矩阵</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &lt;- cbind(x, c(3, 4, 5))    # 添加一列c(3,4,5)到x矩阵最后</span><br><span class="line">y</span><br><span class="line">z &lt;- rbind(c(7, 8, 9, 10), x)  # 添加X到c(7,8,9,10)的行上</span><br><span class="line">z</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-1c848c74ec42d1282a30dd57449b8b8a_720w.jpg" alt="img"></p>
<p><strong>(5) 给matrix的行和列加上名字</strong></p>
<p>rownames()和colnames()给matrix的行和列添加或者修改名字:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- c(3, 1, 5, 2, 3, 8, 5, 8, 9, 4, 2, 3)</span><br><span class="line">x &lt;- matrix(x, nrow &#x3D; 3, ncol &#x3D; 4, byrow &#x3D; T)</span><br><span class="line">x</span><br><span class="line">rownames(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)   # 添加行名</span><br><span class="line">colnames(x) &lt;- c(&quot;w&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;)  # 添加列名</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-34f79eaa1b005662c967756b6fc8f1b9_720w.jpg" alt="img"></p>
<h2 id="1-3-数组"><a href="#1-3-数组" class="headerlink" title="1.3 数组"></a>1.3 数组</h2><p>数组（array）与矩阵类似，但是维度可以大于2。数组可通过array函数创建，形式如下：</p>
<p>myarray &lt;- array(vector, dimensions, dimnames)</p>
<p>其中vector包含了数组中的数据， dimensions是一个数值型向量， 给出了各个维度下标的最大值，而dimnames是可选的、各维度名称标签的列表。</p>
<p><strong>(1) 数组的创建</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dim1 &lt;- c(&quot;A1&quot;, &quot;A2&quot;)</span><br><span class="line">dim2 &lt;- c(&quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;) </span><br><span class="line">dim3 &lt;- c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;, &quot;C4&quot;) </span><br><span class="line">z &lt;- array(1:24, c(2, 3, 4), dimnames&#x3D;list(dim1, dim2, dim3))</span><br><span class="line">z</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-69fbf822be8bf6bc83805650b80b6651_720w.jpg" alt="img"></p>
<p>如你所见，数组是矩阵的一个自然推广。它们在编写新的统计方法时可能很有用。像矩阵一样，数组中的数据也只能拥有一种模式。</p>
<p><strong>(2) 数组的索引</strong></p>
<p>从数组中选取元素的方式与矩阵相同。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">z[1,2,3]</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-802bdd6e4bd921e2b3e2a9ba9e91593b_720w.jpg" alt="img"></p>
<h2 id="1-4-数据框-dataframe"><a href="#1-4-数据框-dataframe" class="headerlink" title="1.4 数据框 dataframe"></a>1.4 数据框 dataframe</h2><p>data frame可以看成是一个excel表格。dataframe是数据分析中非常常用的一种储存数据的方式。dataframe也是一个2维的表格，和matrix一样不一样的地方是data frame的每一列的数据类型可以不一样，但是要求每一列内部数据类型是一样的。数据框可通过函数data.frame()创建：</p>
<p>mydata &lt;- data.frame(col1, col2, col3,…)</p>
<p>其中的列向量col1、col2、col3等可为任何类型（如字符型、数值型或逻辑型） 。每一列的名称可由函数names指定。</p>
<p><strong>(1) 数据框的创建</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">patientID &lt;- c(1, 2, 3, 4)</span><br><span class="line">age &lt;- c(25, 34, 28, 52)</span><br><span class="line">diabetes &lt;- c(&quot;Type1&quot;, &quot;Type2&quot;, &quot;Type1&quot;, &quot;Type1&quot;) </span><br><span class="line">status &lt;- c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;, &quot;Poor&quot;) </span><br><span class="line">patientdata &lt;- data.frame(patientID, age, diabetes, status) </span><br><span class="line">patientdata</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-467781882029c8229e48da9da98044f8_720w.jpg" alt="img"></p>
<p>每一列数据的模式必须唯一，不过你却可以将多个模式的不同列放到一起组成数据框。由于数据框与分析人员通常设想的数据集的形态较为接近，我们在讨论数据框时将交替使用术语 <strong>列</strong>和 <strong>变量</strong>。</p>
<p>如果想指定index，那么：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">patientdata &lt;- data.frame(patientID, age, diabetes, </span><br><span class="line">                          status, row.names&#x3D;patientID) </span><br><span class="line">patientdata</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-cd57fdd4f0f5ce97ed22453f09fc22c2_720w.jpg" alt="img"></p>
<p><strong>(2) 数据框的索引</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">patientdata &lt;- data.frame(patientID, age, diabetes, status) </span><br><span class="line">patientdata  </span><br><span class="line">patientdata[1:2]</span><br><span class="line">patientdata[c(&quot;diabetes&quot;, &quot;status&quot;)] </span><br><span class="line">patientdata$age   # 表示patientdata数据框中的变量age</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-c73d3d4612ecfc78e802b0cd90e31ec0_720w.jpg" alt="img"></p>
<p><strong>(3) 粘结两个dataframe:用cbind和’rbind’结合两个data frame.</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- data.frame(Name &#x3D; c(&quot;Jone&quot;,&quot;Lily&quot;), Grade &#x3D; c(80,90))</span><br><span class="line">x</span><br><span class="line">x &lt;- cbind(x, data.frame(Asia &#x3D; c(F, F)) )</span><br><span class="line">x</span><br><span class="line">x &lt;- rbind(x, data.frame(Name &#x3D; &quot;Wang&quot;, Grade &#x3D; 100, Asia &#x3D; T))</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-00ee92fb4f896eda540ac617e49abe3f_720w.jpg" alt="img"></p>
<h2 id="1-5-因子factor"><a href="#1-5-因子factor" class="headerlink" title="1.5 因子factor"></a>1.5 因子factor</h2><p>如你所见，变量可归结为名义型、有序型或连续型变量。名义型变量是没有顺序之分的类别变量。糖尿病类型Diabetes（Type1、Type2）是名义型变量的一例。即使在数据中Type1编码为1而Type2编码为2，这也并不意味着二者是有序的。有序型变量表示一种顺序关系，而非数量关系。病情Status（poor、improved、excellent）是顺序型变量的一个上佳示例。我们明白，病情为poor（较差）病人的状态不如improved（病情好转）的病人，但并不知道相差多少。连续型变量可以呈现为某个范围内的任意值，并同时表示了顺序和数量。年龄Age就是一个连续型变量，它能够表示像14.5或22.8这样的值以及其间的其他任意值。很清楚，15岁的人比14岁的人年长一岁。</p>
<p>类别（名义型）变量和有序类别（有序型）变量在R中称为因子（factor） 。因子在R中非常重要，因为它决定了数据的分析方式以及如何进行视觉呈现。</p>
<p>函数factor()以一个整数向量的形式存储类别值，整数的取值范围是[1…k]（其中k是名义型变量中唯一值的个数），同时一个由字符串（原始值）组成的内部向量将映射到这些整数上。要表示有序型变量，需要为函数factor()指定参数ordered=TRUE。通过指定levels选项来覆盖默认排序。</p>
<p><strong>(1) 因子类型的使用</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">patientID &lt;- c(1, 2, 3, 4)</span><br><span class="line">age &lt;- c(25, 34, 28, 52)</span><br><span class="line">diabetes &lt;- c(&quot;Type1&quot;, &quot;Type2&quot;, &quot;Type1&quot;, &quot;Type1&quot;) </span><br><span class="line">status &lt;- c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;, &quot;Poor&quot;)</span><br><span class="line">diabetes &lt;- factor(diabetes)</span><br><span class="line">status &lt;- factor(status, order&#x3D;TRUE)    # 有序型变量</span><br><span class="line">patientdata &lt;- data.frame(patientID, age, diabetes, status) </span><br><span class="line">str(patientdata)</span><br><span class="line">summary(patientdata)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-a5d67716b7bab9244f0b96a97900a70e_720w.jpg" alt="img"></p>
<h2 id="1-6-列表"><a href="#1-6-列表" class="headerlink" title="1.6 列表"></a>1.6 列表</h2><p>列表（list）是R的数据类型中最为复杂的一种。一般来说，列表就是一些对象（或成分，component）的有序集合。列表允许你整合若干（可能无关的）对象到单个对象名下。例如，某个列表中可能是若干向量、矩阵、数据框，甚至其他列表的组合。</p>
<p>可以使用函数list()创建列表：</p>
<p>mylist &lt;- list(object1, object2, …)</p>
<p>其中的对象可以是目前为止讲到的任何结构。</p>
<p>你还可以为列表中的对象命名：</p>
<p>mylist &lt;- list(name1=object1, name2=object2, …)</p>
<p><strong>(1) 列表的创建</strong></p>
<p>本例创建了一个列表，其中有四个成分：一个字符串、一个数值型向量、一个矩阵及一个字符型向量。可以组合任意多的对象，并将它们保存为一个列表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">g &lt;- &quot;My First List&quot; </span><br><span class="line">h &lt;- c(25, 26, 18, 39) </span><br><span class="line">j &lt;- matrix(1:10, nrow&#x3D;5)</span><br><span class="line">k &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)</span><br><span class="line">mylist &lt;- list(title&#x3D;g, ages&#x3D;h, j, k)</span><br><span class="line">mylist</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-9513912f779da1b962d76039fce6cec2_720w.jpg" alt="img"></p>
<p>(2) 列表的索引</p>
<p>可以通过在<strong>双重方括号</strong>中指明代表某个成分的数字或名称来访问列表中的元素。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mylist[[2]] </span><br><span class="line">mylist[[&quot;ages&quot;]]</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-bd358c6480c81a64f99d0ad9c7024363_720w.jpg" alt="img"></p>
<h2 id="2-R的运算以及常用函数"><a href="#2-R的运算以及常用函数" class="headerlink" title="2.R的运算以及常用函数"></a>2.R的运算以及常用函数</h2><h2 id="2-1-R的四则运算法则"><a href="#2-1-R的四则运算法则" class="headerlink" title="2.1 R的四则运算法则"></a>2.1 R的四则运算法则</h2><p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-40ecb2fcb241129858e90118614faf0d_720w.jpg" alt="img"></p>
<p>R数字的四则运算+,-,*,/和算数中的四则运算时一致的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2 + 3</span><br><span class="line">2 - 3</span><br><span class="line">2 * 3</span><br><span class="line">2 &#x2F; 3</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-0bbc581c484b9173aebb09cff68174cc_720w.jpg" alt="img"></p>
<p>我们需要注意R的vector和matrix的运算。<strong>在R中，vector和matrix的+,-,*,/ 指的是相同位置元素之间的 +,-,*,/。</strong></p>
<p><strong>(1) 向量的四则运算法则：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- c(1, 2, 3)</span><br><span class="line">y &lt;- c(4, 5, 6)</span><br><span class="line">x + y</span><br><span class="line">x - y </span><br><span class="line">x * y</span><br><span class="line">x &#x2F; y</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-d9ed0976a02a9af0ca8f2400b0d9714a_720w.jpg" alt="img"></p>
<p><strong>(2) 矩阵的四则运算法则：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, nrow &#x3D; 3, ncol &#x3D; 2)</span><br><span class="line">y &lt;- matrix(3:8, nrow &#x3D; 3, ncol &#x3D; 2)</span><br><span class="line">x+y</span><br><span class="line">x-y</span><br><span class="line">x*y</span><br><span class="line">x&#x2F;y</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-2fc21f9fde002bf05280a79f5bcc70ae_720w.jpg" alt="img"></p>
<p><strong>(3) 矩阵与矩阵的乘法：</strong></p>
<p>矩阵相乘的函数是%<em>%,同样的，我们要求<em>*第1个矩阵的列数（column）和第2个矩阵的行数（row）相同</em></em> 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, nrow &#x3D; 2, byrow &#x3D; T)</span><br><span class="line">y &lt;- matrix(7:12, nrow &#x3D; 3, byrow &#x3D; T)</span><br><span class="line">x %*% y</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-4300e0a0fb66d11f40cacc2da9e04efb_720w.jpg" alt="img"></p>
<p>矩阵乘法中，需要注意，当矩阵和向量相乘时，把向量当成一个列数为1的矩阵即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, nrow &#x3D; 2, byrow &#x3D; T)</span><br><span class="line">x %*% c(1, 2, 3)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-8d6da7080bcdf8d0dd40bc8312c6eb77_720w.jpg" alt="img"></p>
<p><strong>(4) 矩阵的转置：t()</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, nrow &#x3D; 2)</span><br><span class="line">t(x)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-e1dbe144176ff828983b5d425cf8613b_720w.jpg" alt="img"></p>
<h2 id="2-2-R的逻辑运算"><a href="#2-2-R的逻辑运算" class="headerlink" title="2.2 R的逻辑运算"></a>2.2 R的逻辑运算</h2><p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-bac902d5c38a5f797940213a3dc61a6b_720w.jpg" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">manager &lt;- c(1, 2, 3, 4, 5) </span><br><span class="line">date &lt;- c(&quot;10&#x2F;24&#x2F;08&quot;, &quot;10&#x2F;28&#x2F;08&quot;, &quot;10&#x2F;1&#x2F;08&quot;, &quot;10&#x2F;12&#x2F;08&quot;, &quot;5&#x2F;1&#x2F;09&quot;) </span><br><span class="line">country &lt;- c(&quot;US&quot;, &quot;US&quot;, &quot;UK&quot;, &quot;UK&quot;, &quot;UK&quot;) </span><br><span class="line">gender &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;) </span><br><span class="line">age &lt;- c(32, 45, 25, 39, 99) </span><br><span class="line">q1 &lt;- c(5, 3, 3, 3, 2) </span><br><span class="line">q2 &lt;- c(4, 5, 5, 3, 2) </span><br><span class="line">q3 &lt;- c(5, 2, 5, 4, 1) </span><br><span class="line"></span><br><span class="line">leadership &lt;- data.frame(manager, date, country, gender, age, </span><br><span class="line">                         q1, q2, q3, stringsAsFactors&#x3D;FALSE)</span><br><span class="line">leadership</span><br><span class="line">leadership$agecat[leadership$age  &gt; 75]  &lt;- &quot;Elder&quot; </span><br><span class="line">leadership$agecat[leadership$age &gt;&#x3D; 55 &amp;  </span><br><span class="line">                  leadership$age &lt;&#x3D; 75]  &lt;- &quot;Middle Aged&quot; </span><br><span class="line">leadership$agecat[leadership$age  &lt; 55]  &lt;- &quot;Young&quot;</span><br><span class="line">leadership</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-f52eb4f60eb78e7a506ad71f7d56c076_720w.jpg" alt="img"></p>
<h2 id="2-3-常用函数"><a href="#2-3-常用函数" class="headerlink" title="2.3 常用函数"></a>2.3 常用函数</h2><p>在探索数据的阶段，常常会先探索数据分布的一些统计量，如求和，均值，标准差，方差，中值，分位数等。对vector,R有函数可以直接得到这些统计量，sum()(求和), mean()(均值), sd()(标准差), var()(方差), median()(中值), quantile()(分位数)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- 1:10</span><br><span class="line">sum(x)</span><br><span class="line">mean(x)</span><br><span class="line">sd(x)</span><br><span class="line">var(x)</span><br><span class="line">median(x)</span><br><span class="line">quantile(x)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-a57737f669a0cc6c19fa42c7e0249685_720w.jpg" alt="img"></p>
<p>对matrix, R有函数rowMeans(), colMeans()可以得到每一行或者每一列的平均值(注意：M要大写哦)，而rowSums,colSums可以得到每一行或者每一列的和(注意：S要大写哦)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, 3, 2)</span><br><span class="line">rowMeans(x)</span><br><span class="line">colMeans(x)</span><br><span class="line">rowSums(x)</span><br><span class="line">colSums(x)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-32dff95c5b3e1bd30bf431fb224038ec_720w.jpg" alt="img"></p>
<p>R的基本函数库里没有函数可以直接计算，方差，标准差等其他信息，我们可以用apply()函数。apply(X,MARGIN,FUN)的参数主要有3个，X通常是一个matrix， MARGIN通常的取值有两个，1或者2，1表示按照行计算，2表示按照列计算，FUN指的是一个函数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(1:6, 3, 2)</span><br><span class="line">apply(x, 1, sd)   # 计算x每一行的标准差</span><br><span class="line">apply(x, 2, quantile)   # 计算x每一列的分位数</span><br><span class="line">apply(x, 1, mean)     # 计算x每一行的均值</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-702ab3afa11576a763d0b12b1a5ba21c_720w.jpg" alt="img"></p>
<p>函数length()可以输出vector的长度，dim()可以输出matrix的2个维度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- rep(2, 8)</span><br><span class="line">length(x)</span><br><span class="line">y &lt;- matrix(1:10, 5, 2)</span><br><span class="line">dim(y)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-1186ae6cebec35c438a24bac3dbe7f04_720w.jpg" alt="img"></p>
<p>函数rnorm()可以产生服从正态分布的随机数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- rnorm(50)    # 产生50个服从标准正态分布的随机数</span><br><span class="line">x</span><br><span class="line">hist(x, breaks &#x3D; 10, cex.lab &#x3D; 1.5, cex.axis &#x3D; 1.5)</span><br><span class="line">abline(v &#x3D; 0, col &#x3D; &quot;red&quot;, lty &#x3D; 2, lwd &#x3D; 2)</span><br><span class="line">y &lt;- rnorm(50, mean &#x3D; 50, sd &#x3D; 0.2)   # 产生50个服从均值是50，标准差是0.2的正态分布随机数</span><br><span class="line">y</span><br><span class="line">hist(y, cex.lab &#x3D; 1.5, cex.axis &#x3D; 1.5)</span><br><span class="line">abline(v &#x3D; 50, col &#x3D; &quot;red&quot;, lty &#x3D; 2, lwd &#x3D; 2)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-ad0afd65df89391a33613dbe7eafb6bf_720w.jpg" alt="img"></p>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-ce08f486ed3827df2976fd6d9f68f8a1_720w.jpg" alt="img"></p>
<p>有时候我们需要重复我们的计算或者实验，这时我们需要用到set.seed()函数固定一个产生随机数的种子。下面的形式可以保证我们每次运行产生一样的随机数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set.seed(123)</span><br><span class="line">rnorm(5)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-dc9425b67bdd6a18b1f6aedc8faef318_720w.jpg" alt="img"></p>
<p>函数cor()可以计算两个vector的相关关系</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- rnorm(100)</span><br><span class="line">y &lt;- x + rnorm(100, 0, 0.2)</span><br><span class="line">cor(x, y)</span><br><span class="line">plot(x, y, col &#x3D; &quot;blue&quot;, pch &#x3D; 20, cex.lab &#x3D; 1.5, cex.axis &#x3D; 1.5)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-283e72023b5081b8e5c22b69141d40e2_720w.jpg" alt="img"></p>
<p>函数summary()可以得到matrix或者data frame的每一列的基本信息，包括最大值，最小值，中间值，25%和75%分位数，均值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- matrix(rnorm(100*3), 100, 3)</span><br><span class="line">summary(x)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-f81c30bc5f05b8a53548ebb143e4e164_720w.jpg" alt="img"></p>
<h2 id="3-R语言编程结构"><a href="#3-R语言编程结构" class="headerlink" title="3.R语言编程结构"></a>3.R语言编程结构</h2><h2 id="3-1-if-else语句"><a href="#3-1-if-else语句" class="headerlink" title="3.1 if-else语句"></a>3.1 if-else语句</h2><p>(1) 在R中，if-else语句的形式通常如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &lt;- 3</span><br><span class="line">if (a &#x3D;&#x3D; 4) &#123;</span><br><span class="line">  x &lt;- 2</span><br><span class="line">  y &lt;- 3</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  x &lt;- 3</span><br><span class="line">  y &lt;- 4</span><br><span class="line">&#125;</span><br><span class="line">x</span><br><span class="line">y</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-84e7f286a35c45a6760914db5e7f8326_720w.jpg" alt="img"></p>
<p>需要根据不同条件执行不同代码时，使用函数if(), 在括号里写入判断的语句，在上面的例子中，我们根据a == 4是TRUE, 还是FALSE执行不同的语句。</p>
<p>如果a=4, 那么a == 4是TRUE, 执行if()后面大括号{ }内的代码；</p>
<p>如果a!=4, 那么a == 4是FALSE, 执行else后面大括号{ }内的代码。</p>
<p>(2) if()也可以单独使用，例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (a &#x3D;&#x3D; 4) &#123;</span><br><span class="line">  x &lt;- 2</span><br><span class="line">  y &lt;- 3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-d9d319628748ddb0117a04716d1601f6_720w.jpg" alt="img"></p>
<p>(3) 多个if-else可以一起使用，例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (a &#x3D;&#x3D; 4) &#123;</span><br><span class="line">  x &lt;- 2</span><br><span class="line">  y &lt;- 3</span><br><span class="line">&#125; else if (a &#x3D;&#x3D; 5)&#123;</span><br><span class="line">  x &lt;- 3</span><br><span class="line">  y &lt;- 4</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  x &lt;- 5</span><br><span class="line">  y &lt;- 6</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(4) if-else还可以有如下的使用方式:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># v &#x3D; if(cond) expression1 else expression2 # v可能取expression1或者expression2的结果，这取决于cond是否为真。</span><br><span class="line">x &#x3D; 2</span><br><span class="line">y &#x3D; if(x&#x3D;&#x3D;2) x else x+1</span><br><span class="line">y</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-fe6a07d6a553659460e31a3035669e81_720w.jpg" alt="img"></p>
<h2 id="3-2-循环"><a href="#3-2-循环" class="headerlink" title="3.2 循环"></a>3.2 循环</h2><p><strong>(1) For循环</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我们如果要计算1到10的平均值</span><br><span class="line">s &lt;- 0</span><br><span class="line">for (i in 1:10) &#123;</span><br><span class="line">  s &lt;- s + i</span><br><span class="line">&#125;</span><br><span class="line">s &#x2F; 10</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-e374f1aa259824fc0d65487b95b9bc43_720w.jpg" alt="img"></p>
<p><strong>(2) while循环(一)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &lt;- 0</span><br><span class="line">i &lt;- 1</span><br><span class="line">while(i&lt;&#x3D;10) &#123;</span><br><span class="line">  s &lt;- s + i</span><br><span class="line">  i &lt;- i + 1</span><br><span class="line">&#125;</span><br><span class="line">s &#x2F; 10</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-56499369f4dd0d99fa30aba9ab11383f_720w.jpg" alt="img"></p>
<p><strong>(3) while循环(二)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &lt;- 0</span><br><span class="line">i &lt;- 1</span><br><span class="line">while(TRUE) &#123;</span><br><span class="line">  s &lt;- s + i</span><br><span class="line">  i &lt;- i + 1</span><br><span class="line">  if(i &gt; 10) break</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line">s &#x2F; 10</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-64d51fb9b5842b92e39e18e1ad7f0c88_720w.jpg" alt="img"></p>
<p><strong>(4) repeat循环</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &lt;- 0</span><br><span class="line">i &lt;- 1</span><br><span class="line">repeat &#123;</span><br><span class="line">  s &lt;- s + i</span><br><span class="line">  i &lt;- i + 1</span><br><span class="line">  if(i &gt; 10) break</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line">s &#x2F; 10</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-7b40d56a9a335187b2c2a55fb07ad2a1_720w.jpg" alt="img"></p>
<h2 id="3-3-自定义R函数"><a href="#3-3-自定义R函数" class="headerlink" title="3.3 自定义R函数"></a>3.3 自定义R函数</h2><p>在R中，函数可以通过如下形式定义：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 自定义函数求一个向量中所有偶数的和</span><br><span class="line">sumEvenNum &#x3D; function(v)&#123;</span><br><span class="line">  even_num &#x3D; v[(v %% 2)&#x3D;&#x3D;0]</span><br><span class="line">  sum_even_num  &#x3D; sum(even_num)</span><br><span class="line">  return(sum_even_num)</span><br><span class="line">&#125;</span><br><span class="line">x &#x3D; 1:10</span><br><span class="line">sumEvenNum(x)</span><br></pre></td></tr></table></figure>
<p><img src="/Pic/R%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/v2-562fad8f8149b8c5c4935e6fdbcabd44_720w.jpg" alt="img"></p>
<p>赋值号(=或者&lt;-)左边是自定义函数的函数名</p>
<p>赋值号右边是定义函数的函数function()</p>
<p>函数function()括号内是我们要传到自定义函数的参数</p>
<p>大括号内写函数的代码</p>
<p>最后使用函数return()返回结果</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本次的R语言语法精讲（一）主要介绍了 R语言的数据结构，R语言的运算以及R语言的编程结构。学完本文后，您将可以具备初步的R语言编程技巧，并能编写大部分程序以及算法。</p>
<blockquote>
<p>本文转载自知乎：<a href="https://zhuanlan.zhihu.com/p/180119895?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=960125220078702592&amp;utm_content=first" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/180119895?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=960125220078702592&amp;utm_content=first</a></p>
</blockquote>
]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十五）—— Scrapy爬虫实践（更新中）</title>
    <url>/posts/b0f5da86.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十四）—— Scrapy爬虫基础</title>
    <url>/posts/cd76fad2.html</url>
    <content><![CDATA[<p>Scrapy库不是一个简单的函数，而是一个爬虫框架。爬虫框架就是实现爬虫功能的一个软件结构和功能组件集合爬虫框架就是一个半成品，能够帮助用户实现专业网络爬虫。</p>
<h1 id="Scrapy爬虫框架结构"><a href="#Scrapy爬虫框架结构" class="headerlink" title="Scrapy爬虫框架结构"></a>Scrapy爬虫框架结构</h1><p><strong>“5+2”结构</strong>：</p>
<ul>
<li>Engine模块</li>
<li>Spider模块</li>
<li>Downloader模块</li>
<li>ItemPipelines模块</li>
<li>Scheduler模块</li>
</ul>
<p>下图为嵩天教授讲授爬虫课程时的Scrapy框架图截图：</p>
<p><img src="/posts/Spider/Scrapy_Engine.jpg" alt></p>
<p>另外在Engine和Spider模块之间，以及Engine和Downloader模块之间包含了两个MiddleWare模块这个结构就称为Scrapy爬虫框架。在Scrapy框架中，数据包括用户提交的网络爬虫请求，以及从网络上提取地相关内容，在这些结构之间进行流动，形成数据流。</p>
<p>Scrapy框架主要包含<strong>三条主要的数据流路径</strong>：</p>
<ul>
<li><p>第一条从Spiders经过Engine到达Scheduler，Engine从Spiders处获得用户的请求（Requests），可以简单地认为是一个URL，请求到达Engine后，Engine将其分配给Scheduler模块，而Scheduler模块负责对爬取请求进行调度。</p>
</li>
<li><p>Scrapy的第二条数据流路径是从Scheduler模块通过Engine模块到达Downloader模块，并且最终返回Spider模块。首先Engine模块从Scheduler模块获取下一个要爬取的网络请求，这一个网络请求是真实的要到网络上进行爬取的请求，Engine获得这个请求后通过中间键发送给Downloader模块，Downloader模块和获取请求之后真实地连接互联网并且爬取相关网页，爬取到网页后Downloader模块形成响应（Response），将所有内容封装为Response后打包通过Engine中间键发送给Spiders。在这条路径中一个真实地爬取URL的请求经过Scheduler,Downloader最终返回了相关内容返回Spiders。</p>
</li>
<li><p>第三条数据流路径是从 Spiders模块经过Engine模块到达ItemPipelines模块以及Scheduler模块。首先Spiders处理从Downloader获得的响应（从网络中爬取的相关内容），得到两个数据类型，一个数据类型叫爬取项（Scrapy Item），另一个数据类型是新的爬取请求。也就是我们从网络上获得一个网页之后，如果网页中有我们感兴趣的链接，我们可以在Spiders中增加相关的功能，对新的链接发起再次的爬取。Spiders生成了这两个数据类型之后，将它们发送给Engine模块，Engine随后将Item发送给ItemPipelines，将Requests发送给Requests进行调度，从而为后期的数据处理以及再获取网络爬虫请求提供了相应的数据来源。</p>
</li>
</ul>
<p>在这条路径中，Engine控制着各个模块的数据流，并且它不断地从Scheduler获取真正要爬取的请求并发送给Downloader。这个框架的入口是Spiders，出口是ItemPipelines。在这个“5+2”结构中，Engine，Scheduler和Downloader都已有实现，用户不需要去编写他们，他们会按照既定的功能 完成相关的任务。用户需要编写的是Spiders模块和Item Pipelines模块，其中Spiders模块向整个Scrapy模块提供访问URL链接，同时解析从网络上获得的页面内容，ItemPipeline模块负责对提取的信息进行后处理。由于在这个框架下，用户只需要编写部分代码，因此这个过程也被称为配置，用户只需要在这个框架下进行简单的配置即可完成爬取需求。</p>
<p>Downloader Middleware目的是实施Engine、Scheduler、Downloader之间进行用户可配置的控制，功能是修改、丢弃、新增请求或响应。Spider Middleware目的是对请求和爬取项进行再处理，功能是修改、丢弃、新增请求或爬取项。</p>
<h1 id="Requests库和Scrapy爬虫的比较"><a href="#Requests库和Scrapy爬虫的比较" class="headerlink" title="Requests库和Scrapy爬虫的比较"></a>Requests库和Scrapy爬虫的比较</h1><p><strong>相同点</strong>：</p>
<ol>
<li>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线</li>
<li>两者可用性都好，文档丰富，入门简单</li>
<li>两者都没有处理js、提交表单、用对验证码功能（可扩展）</li>
</ol>
<p><strong>不同点</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>requests</th>
<th>Scrapy</th>
</tr>
</thead>
<tbody>
<tr>
<td>页面级爬虫</td>
<td>网站级爬虫</td>
</tr>
<tr>
<td>功能库</td>
<td>框架</td>
</tr>
<tr>
<td>并发性考虑不足，性能较差</td>
<td>并发性好，性能较高</td>
</tr>
<tr>
<td>重点在于页面下载</td>
<td>重点在于爬虫结构</td>
</tr>
<tr>
<td>定制灵活</td>
<td>一般定制灵活，深度定制困难</td>
</tr>
<tr>
<td>上手简单</td>
<td>入门稍难</td>
</tr>
</tbody>
</table>
</div>
<p>具体选择：</p>
<ul>
<li>非常小的请求：Requests库</li>
<li>不太小的请求：Scrapy框架</li>
<li>定制成都很高的请求，自搭框架：Requests库</li>
</ul>
<h1 id="Scarpy爬虫的常用命令"><a href="#Scarpy爬虫的常用命令" class="headerlink" title="Scarpy爬虫的常用命令"></a>Scarpy爬虫的常用命令</h1><ul>
<li>Scrapy命令行的启用——直接在命令行输入：scrapy -h</li>
<li>命令格式：scarpy &lt;command&gt; [options] [args]，Scrapy的命令在command体现</li>
</ul>
<p>常用命令如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>格式</th>
</tr>
</thead>
<tbody>
<tr>
<td>startproject</td>
<td>创建一个工程</td>
<td>scrapy startproject &lt;name&gt;[dir]</td>
</tr>
<tr>
<td>genspider</td>
<td>创建一个爬虫</td>
<td>scrapy genspider [options] &lt;name&gt;&lt;domain&gt;</td>
</tr>
<tr>
<td>settings</td>
<td>获得爬虫配置信息</td>
<td>scrapy settings [options]</td>
</tr>
<tr>
<td>crawl</td>
<td>运行一个爬虫</td>
<td>scrapy crawl &lt;spider&gt;</td>
</tr>
<tr>
<td>list</td>
<td>列出工程中所有爬虫</td>
<td>scrapy list</td>
</tr>
<tr>
<td>shell</td>
<td>启动URL调试命令行</td>
<td>scrapy shell [url]</td>
</tr>
</tbody>
</table>
</div>
<p><strong>为什么Scrapy采用命令行创建和运行爬虫？</strong></p>
<ul>
<li>命令行（不是图形界面）更容易自动化，适合脚本控制（只有用户才会关注图形界面）</li>
</ul>
<h1 id="Scrapy爬虫第一个实例"><a href="#Scrapy爬虫第一个实例" class="headerlink" title="Scrapy爬虫第一个实例"></a>Scrapy爬虫第一个实例</h1><p>首先我们有Scrapy爬虫的产生步骤：</p>
<ol>
<li>建立一个Scrapy爬虫工程</li>
<li>在工程中产生一个Scrapy爬虫</li>
<li>配置产生的spider爬虫</li>
<li>运行爬虫，获取网页</li>
</ol>
<h2 id="建立一个Scrapy爬虫工程"><a href="#建立一个Scrapy爬虫工程" class="headerlink" title="建立一个Scrapy爬虫工程"></a>建立一个Scrapy爬虫工程</h2><p>演示HTML页面地址：<a href="http://python123.io/ws/demo.html" target="_blank" rel="noopener">http://python123.io/ws/demo.html</a></p>
<p>文件名称：demo.html</p>
<p>我们建立工程的所有的操作如下：</p>
<p><img src="/posts/Spider/Scrapy_Start_1.png" alt></p>
<p><img src="/posts/Spider/Scrapy_Start_2.png" alt></p>
<p>我们可以看到整个文件的架构：</p>
<p><img src="/posts/Spider/Scrapy_Start.jpg" alt></p>
<p>在生成的工程目录spider/下，含有Spiders代码模板目录（继承类），内部含有两个文件：</p>
<p><img src="/posts/Spider/Scrapy_Start_3.png" alt></p>
<p>其中 __init__.py是初始文件，无需修改；而__pycache__是我们熟悉的缓存目录，也无需修改</p>
<h2 id="在工程中产生一个Scrapy爬虫"><a href="#在工程中产生一个Scrapy爬虫" class="headerlink" title="在工程中产生一个Scrapy爬虫"></a>在工程中产生一个Scrapy爬虫</h2><p>我们在工作目录下先输入scrapy genspider demo python123.io命令，然后将会在python123demo文件夹下的spiders文件夹中产生一个新的文件：demo.py，具体操作如下：</p>
<p><img src="/posts/Spider/Scrapy_Start_4.png" alt></p>
<p>这条命令的作用仅是生成demo.py，demo.py文件内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span></span><br><span class="line">    allowed_domains = [<span class="string">'python123.io'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>这个类函数DemoSpider必须是继承scrapy.Spider，name指的是这个爬虫的名字，allowed_domains是用户提交给命令行的域名，也就是爬虫在爬取网站时，只能爬取该域名下的链接，start_urls就是scrapy爬取框架爬取的初始页面。parse函数是解析页面的空的类的方法，用于处理响应，可以解析从网络中爬取内容，形成字典类型，同时从网页中发现新的要爬取的内容，生成url。</p>
<h2 id="配置产生的spider爬虫"><a href="#配置产生的spider爬虫" class="headerlink" title="配置产生的spider爬虫"></a>配置产生的spider爬虫</h2><p>我们对parse的要求是将返回的html存成文件，下面直接看这部分代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['python123.io']</span></span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/ws/demo.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 从响应的url中提取名字作为本地文件名</span></span><br><span class="line">        fname = response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 将返回内容保存为文件</span></span><br><span class="line">        <span class="keyword">with</span> open(fname, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">"Saved file %s."</span> %name)</span><br></pre></td></tr></table></figure>
<h2 id="运行爬虫，获取网页"><a href="#运行爬虫，获取网页" class="headerlink" title="运行爬虫，获取网页"></a>运行爬虫，获取网页</h2><p>我们在命令行下执行crawl命令获取网页：</p>
<p><img src="/posts/Spider/Scrapy_Start_5.png" alt></p>
<p>捕获的文件存在python123demo路径下的demo.html文件中</p>
<p>事实上，官方给出的更标准的写法是下面这个：</p>
<p><img src="/posts/Spider/Scrapy_Start_6.png" alt></p>
<p>这里主要使用了yield，这个函数当返回的URL列表很大时，能够极大地节省存储空间，若有需要进一步了解yield的用法可自行看文档。</p>
<h1 id="Scrapy爬虫的使用步骤"><a href="#Scrapy爬虫的使用步骤" class="headerlink" title="Scrapy爬虫的使用步骤"></a>Scrapy爬虫的使用步骤</h1><p>下面我们对爬虫使用步骤做个总结：</p>
<ol>
<li>创建一个工程和Spider模板</li>
<li>编写Item Pipeline</li>
<li>编写Spider</li>
<li>优化策略配置</li>
</ol>
<p>Scrapy爬虫的数据类型有：</p>
<ol>
<li>Requests类</li>
<li>Response类</li>
<li>Item类</li>
</ol>
<p><strong>Request对象</strong>表示一个HTTP请求，由Spider生成，由Downloader执行。具体来说Request类包含六个属性方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.url</td>
<td>Request对应的请求URL地址</td>
</tr>
<tr>
<td>.method</td>
<td>对应的请求方法，’GET’’POST’等</td>
</tr>
<tr>
<td>.headers</td>
<td>字典类型风格请求头</td>
</tr>
<tr>
<td>.body</td>
<td>请求内容主体，字符串类型</td>
</tr>
<tr>
<td>.meta</td>
<td>用户添加的扩展信息，在Scrapy内部模块间传递信息使用</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该请求</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Response对象</strong>表示一个HTTP响应，由Downloader生成，由Spider处理。包含了七个主要的属性和方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.url</td>
<td>Response对应的响应URL地址</td>
</tr>
<tr>
<td>.status</td>
<td>HTTP状态码，默认是200</td>
</tr>
<tr>
<td>.headers</td>
<td>Response对应的头部信息</td>
</tr>
<tr>
<td>.body</td>
<td>Response对应的内容信息，字符串类型</td>
</tr>
<tr>
<td>.flags</td>
<td>一组标记</td>
</tr>
<tr>
<td>.request</td>
<td>产生Response类型对应的Request对象</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该响应</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Item对象</strong>表示一个从HTML页面中提取的信息内容，由Spider生成，由Item Pipeline处理。Item类似字典类型，可以按照字典类型操作</p>
<p><strong>Scrapy爬虫提取信息的方法</strong><br>Scrapy支持多种HTML信息提取的方法：Beautiful Soup，lxml，re，XPATH Selector，CSS Selector等</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十三）—— 水木社区论坛爬取实战</title>
    <url>/posts/e97854a0.html</url>
    <content><![CDATA[<h1 id="实战阶段一：分析首页大板块-URL"><a href="#实战阶段一：分析首页大板块-URL" class="headerlink" title="实战阶段一：分析首页大板块 URL"></a>实战阶段一：分析首页大板块 URL</h1><p>我们首先打开待爬取页面 —— 水木社区的首页：<a href="http://www.newsmth.net/nForum/#!mainpage，进入后页面如下：" target="_blank" rel="noopener">http://www.newsmth.net/nForum/#!mainpage，进入后页面如下：</a><br><img src="https://img-blog.csdnimg.cn/20200428102419709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="水木社区板块">我们看到左边有很多讨论区板块，我们点进去试试：<br><img src="https://img-blog.csdnimg.cn/20200428102956475.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="休闲娱乐板块页面"><br>我们现在就已经点进去了休闲娱乐讨论区，在这个讨论区中我们可以看到最上面的链接：<a href="http://www.newsmth.net/nForum/#!section/2，那这个链接就很有灵魂了，我们很容易就能猜到只要我们修改不同的" target="_blank" rel="noopener">http://www.newsmth.net/nForum/#!section/2，那这个链接就很有灵魂了，我们很容易就能猜到只要我们修改不同的</a> section，我们就可以进入不同的讨论区，我们进入讨论区后可以看到有不同的板块：<br><img src="https://img-blog.csdnimg.cn/20200428110506257.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="实战阶段二：获取子板块-URL"><a href="#实战阶段二：获取子板块-URL" class="headerlink" title="实战阶段二：获取子板块 URL"></a>实战阶段二：获取子板块 URL</h1><p>我们希望获取到这些板块的链接和标题，但点进去之后发现这些板块并不像它的上一级打开一样有 section 给我们选择，它是一个个的板块名称的英文，有人或许会想我们可以直接把这些对应的名称输入字典翻译过来再输回来不就可以了吗，但事实上每一个英文单词可能有不同的表达，而且就算我们能够确定他们的表达，我们还得建一个字典爬虫，这多麻烦啊。<br>我们先试着查看一下这篇网页的 notwork，打开 network 再点击刷新，我们筛选 XML 类型的数据：<br><img src="https://img-blog.csdnimg.cn/20200428111143949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="XML数据">事实上 XML 的数据并不多，我们只需要都看一看他们的响应值就可以找到这样一个汇总了所有条目信息的数据，我们点开这条 XML 的消息头：<br><img src="https://img-blog.csdnimg.cn/20200428111322899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="消息头"><br>将对应的请求网址输入 POSTMAN 工具，点击 send 后得到对应的消息头：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428111530817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="POSTMAN"><br>点击最右侧的 code，此时它就会把我们所需的 headers 返回给我们：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428111648269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Code"><br>我们直接复制 headers 部分，然后就按套路就能得到响应了（如果连套路都不会的话大家赶紧复习一下前面的内容：<a href="https://blog.csdn.net/ChenKai_164/article/details/105617886" target="_blank" rel="noopener">requests 实战</a>，最好把前前后后的知识都补一遍！<br>下面是这个部分的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.newsmth.net/nForum/slist.json?uid=guest&amp;root=sec-9"</span></span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0'</span>,</span><br><span class="line">  <span class="string">'Host'</span>: <span class="string">'www.newsmth.net'</span>,</span><br><span class="line">  <span class="string">'Accept'</span>: <span class="string">'application/json, text/javascript, */*; q=0.01'</span>,</span><br><span class="line">  <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2'</span>,</span><br><span class="line">  <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</span><br><span class="line">  <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">  <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">  <span class="string">'Referer'</span>: <span class="string">'http://www.newsmth.net/nForum/'</span>,</span><br><span class="line">  <span class="string">'Cookie'</span>: <span class="string">'Hm_lvt_bbac0322e6ee13093f98d5c4b5a10912=1587005280; main[UTMPUSERID]=guest; main[UTMPKEY]=40324888; main[UTMPNUM]=37000; Hm_lpvt_bbac0322e6ee13093f98d5c4b5a10912=1587005521; main[XWJOKE]=hoho; __gads=ID=c8ebf8ba150e0059:T=1587005311:S=ALNI_MZ_fNqTOR1qs7ZaHX7DWAcYp5CD8A; left-index=00100000000'</span>,</span><br><span class="line">  <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">  <span class="string">'Cookie'</span>: <span class="string">'main[UTMPUSERID]=guest; main[UTMPKEY]=10851192; main[UTMPNUM]=83713'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = requests.get(url, headers=headers)</span><br><span class="line">        res.raise_for_status</span><br><span class="line">        res.encoding = res.apparent_encoding</span><br><span class="line">        text = res.text</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">text = get_text(url)</span><br><span class="line">text = text.replace(<span class="string">"["</span>, <span class="string">""</span>)</span><br><span class="line">text = text.replace(<span class="string">"]"</span>, <span class="string">""</span>)</span><br><span class="line">text_split = text.split(<span class="string">","</span>)</span><br><span class="line">href_list = []</span><br><span class="line">title_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(text_split)):</span><br><span class="line">    str_i = text_split[i]</span><br><span class="line">    pat1 = re.compile(<span class="string">r'a href=\\"(.*?)" title='</span>)</span><br><span class="line">    pat2 = re.compile(<span class="string">r'title=\\"(.*?)\\"&gt;'</span>)</span><br><span class="line">    href_ = re.findall(pat1, str_i)</span><br><span class="line">    title_ = re.findall(pat2, str_i)</span><br><span class="line">    <span class="keyword">if</span> (len(href_)) &gt; <span class="number">0</span> &amp; (len(title_) &gt; <span class="number">0</span>):</span><br><span class="line">        href_list.append(href_[<span class="number">0</span>])</span><br><span class="line">        title_list.append(title_[<span class="number">0</span>])</span><br><span class="line">print(href_list,<span class="string">"\n"</span>,title_list)</span><br><span class="line"></span><br><span class="line">href_list = [<span class="string">"http://www.newsmth.net"</span> + href.replace(<span class="string">"\\"</span>, <span class="string">""</span>) <span class="keyword">for</span> href <span class="keyword">in</span> href_list]</span><br><span class="line">href_list</span><br></pre></td></tr></table></figure>
<p>大家最好都手敲一遍，事实上这段代码大家完全不用看我的，自己都可以敲出来，这是最终得到的结果，URL 和标题信息都有了：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428112224201.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="URL和标题信息"><br>我们上面得到的标题，可以作为我们建立新文件夹的依据，我们得到的 URL，我们可以进一步分析得到文章的条件，然后按照套路抓取到所有的文章。接下来我们就看看怎么通过这些 URL 得到对应的文章：</p>
<h1 id="实战阶段三：获取子版块下所有文章"><a href="#实战阶段三：获取子版块下所有文章" class="headerlink" title="实战阶段三：获取子版块下所有文章"></a>实战阶段三：获取子版块下所有文章</h1><p>我们任意点进去一个子版块，以 AppleDev 子版块为例，依然是点开 network 刷新，得到一些 ajax 信息， ajax 在我们前面的 <a href="https://blog.csdn.net/ChenKai_164/article/details/105786852" target="_blank" rel="noopener">Ajax 详解</a><br>已经有讲过了，我们仍然按照套路抓取链接得到内容，然后由正则表达式匹配文章链接，文章链接的形式我们可以任意点开一篇文章观察到，或者也可以直接通过返回的信息得到，下面是抓取的代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res_ = requests.get(<span class="string">"http://www.newsmth.net/nForum/board/AppleDev?ajax"</span>, headers=headers)</span><br><span class="line">res_.encoding = res_.apparent_encoding</span><br><span class="line">text = res_.text</span><br><span class="line">pat3 = re.compile(<span class="string">r'(/nForum/article/AppleDev/\d+)'</span>)</span><br><span class="line">href_text = re.findall(pat3, text)</span><br><span class="line">total_href_text = [<span class="string">"http://www.newsmth.net"</span>+ href <span class="keyword">for</span> href <span class="keyword">in</span> href_text]</span><br><span class="line">print(total_href_text)</span><br></pre></td></tr></table></figure><br>我们将得到的链接内容进行截图：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428115615131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="链接内容"><br>不出意料，我们点进去任何一个链接看到的文章都正是我们想要得到的内容！</p>
<h1 id="实战阶段四：编写结构化代码"><a href="#实战阶段四：编写结构化代码" class="headerlink" title="实战阶段四：编写结构化代码"></a>实战阶段四：编写结构化代码</h1><p>我们现在直接开始编写结构化的代码：<br>首先导入我们所需要的包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br></pre></td></tr></table></figure>
<p>有人可能会好奇，为什么还要导入 selenium 包，事实上在爬取文章内容的时候，我试图分析了一波网页源代码和 Network 的 response，似乎并没有好的方法获得文章内容，于是我果断放弃，选择了相对低效的 selenium，至于问为什么低效，是因为我的代码中每一次都需要打开 passage 的网页才能抓取，当然现在有不需要打开网页的 selenium 方法，别问，问就是懒。<br>下面我们写一个很常用很老套的函数，从 URL 获得文章的 Text：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = requests.get(url)</span><br><span class="line">        res.raise_for_status</span><br><span class="line">        res.encoding = res.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> res.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br></pre></td></tr></table></figure>
<p>这几行代码如果大家看了前几期文章，现在倒着都能默出来了吧，如果没有的话赶紧去看看前面所有爬虫系列的文章噢~</p>
<p>然后我们定义一个函数，获取所有的 Section 的 URL：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSectionURL</span><span class="params">()</span>:</span></span><br><span class="line">    url_list = []</span><br><span class="line">    base_url = <span class="string">r"http://www.newsmth.net/nForum/#!section/"</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        url_list.append(base_url+str(i))</span><br><span class="line">    <span class="keyword">return</span> url_list</span><br></pre></td></tr></table></figure>
<p>下面这堆代码的用途是获得 Section 的名称，大家可以用这个名称在爬取完文章后建立文件夹保存文章：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSectionList</span><span class="params">()</span>:</span></span><br><span class="line">    sectionTitleURL = <span class="string">"http://www.newsmth.net/nForum/slist.json?uid=guest&amp;root=list-section"</span></span><br><span class="line">    text = getText(sectionTitleURL)</span><br><span class="line">    textList = text.split(<span class="string">","</span>)</span><br><span class="line">    textList = [i <span class="keyword">for</span> i <span class="keyword">in</span> textList <span class="keyword">if</span> i.endswith(<span class="string">'&lt;/a&gt;"'</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(textList)):</span><br><span class="line">        textList[i] = re.findall(<span class="string">r'\\"&gt;(.*?)&lt;/a&gt;'</span>, textList[i])</span><br><span class="line">    textList = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> textList <span class="keyword">if</span> len(i)&gt;<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> textList</span><br></pre></td></tr></table></figure>
<p>下面这段代码用于爬取大板块下面的所有子版块的链接和标题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSubSectionList</span><span class="params">(section_num)</span>:</span></span><br><span class="line">    url = <span class="string">"http://www.newsmth.net/nForum/section/"</span> + str(section_num) + <span class="string">"?ajax"</span></span><br><span class="line">    text = getText(url)</span><br><span class="line">    soup = BeautifulSoup(text, <span class="string">"html"</span>)</span><br><span class="line">    hrefList = soup.select(<span class="string">"td a"</span>)</span><br><span class="line">    hrefList = [str(href) <span class="keyword">for</span> href <span class="keyword">in</span> hrefList]</span><br><span class="line">    hrefList = [href <span class="keyword">for</span> href <span class="keyword">in</span> hrefList <span class="keyword">if</span> href.startswith(<span class="string">r'&lt;a href="/nForum/board'</span>)]</span><br><span class="line">    pat1 = re.compile(<span class="string">r'&lt;a href="(.*?)"&gt;'</span>)</span><br><span class="line">    pat2 = re.compile(<span class="string">r'"&gt;(.*?)&lt;/a&gt;'</span>)</span><br><span class="line">    HrefList = []</span><br><span class="line">    TitleList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(hrefList)):</span><br><span class="line">        href = re.findall(pat1, hrefList[i])</span><br><span class="line">        href = <span class="string">"http://www.newsmth.net"</span> + href[<span class="number">0</span>]</span><br><span class="line">        title = re.findall(pat2, hrefList[i])</span><br><span class="line">        HrefList.append(href)</span><br><span class="line">        TitleList.append(title[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> HrefList, TitleList</span><br></pre></td></tr></table></figure>
<p>下面这行代码用于从上面获得的 SubSectionURL 中获得所有文章的 URL 链接：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPassageURL</span><span class="params">(SubSectionURL)</span>:</span></span><br><span class="line">    TextURL = SubSectionURL + <span class="string">"?ajax"</span></span><br><span class="line">    text = getText(TextURL)</span><br><span class="line">    title = re.findall(<span class="string">r"nForum/board/(.*)\?ajax"</span>, TextURL)[<span class="number">0</span>]</span><br><span class="line">    pat = re.compile(<span class="string">r'href="/nForum/article/'</span> + re.escape(title) + <span class="string">r'/(\d&#123;2,10&#125;)'</span>)</span><br><span class="line">    article_num = re.findall(pat, text)</span><br><span class="line">    href_list = [<span class="string">r"http://www.newsmth.net/nForum/#!article/"</span> + title + <span class="string">"/"</span> + href <span class="keyword">for</span> href <span class="keyword">in</span> article_num]</span><br><span class="line">    <span class="keyword">return</span> href_list</span><br></pre></td></tr></table></figure>
<p>下面这行代码用于从上面的文章 URL 中解析出文章内容，评论区内容被截掉了，大家可根据需要修改代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPassageFromURL</span><span class="params">(PassageURL_List)</span>:</span></span><br><span class="line">    text_List = []</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> PassageURL_List:</span><br><span class="line">        driver = webdriver.Firefox()</span><br><span class="line">        driver.get(url)</span><br><span class="line">        page = driver.page_source</span><br><span class="line">        soup = BeautifulSoup(page)</span><br><span class="line">        content = soup.select(<span class="string">"td[class~=a-content]"</span>)</span><br><span class="line">        passage = re.findall(<span class="string">r'&lt;td class="a-content"&gt;(.*?)&lt;/td&gt;'</span>, str(content[<span class="number">0</span>]))</span><br><span class="line">        print(passage)</span><br><span class="line">        text_List.append(passage)</span><br><span class="line">        driver.close()</span><br><span class="line">    <span class="keyword">return</span> text_List</span><br></pre></td></tr></table></figure>
<p>大功告成！大家试着运行一下代码看看能不能跑通吧~</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十二）—— 新闻消息爬取实战</title>
    <url>/posts/ac35ade0.html</url>
    <content><![CDATA[<h1 id="链接介绍"><a href="#链接介绍" class="headerlink" title="链接介绍"></a>链接介绍</h1><p>我们需要爬取的链接是：<a href="https://news.qq.com/" target="_blank" rel="noopener">https://news.qq.com/</a><br>我们最终的爬取目标是将所有标题及其内容罗列出来存储为表格文档</p>
<h1 id="爬取过程"><a href="#爬取过程" class="headerlink" title="爬取过程"></a>爬取过程</h1><p>其实这个爬虫的代码特别简单，就是使用我们前面学的 <a href="https://blog.csdn.net/Mikow/article/details/105787541" target="_blank" rel="noopener">Selenium</a><br> 教程，下面拆分代码进行讲解：</p>
<p>首先自然就是导入所需的所有库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver  </span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys  </span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> csv</span><br></pre></td></tr></table></figure>
<p>然后通过 driver 打开上面给出的链接：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"https://news.qq.com/"</span>)</span><br></pre></td></tr></table></figure>
<p>然后由于新闻页面需要滑动才会显示下面的内容，我们设置一个控制屏幕滑动的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">100</span>):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    driver.execute_script(<span class="string">"window.scrollTo(window.scrollX, %d);"</span>%(i*<span class="number">150</span>))</span><br></pre></td></tr></table></figure>
<p>我们知道爬取这些由 JS 控制的页面时的困难主要在于我们得到的页面源代码和我们点击 F12 查看控制台看到的东西不是一回事，而 Selenium 的一大好处就是我们可以模拟浏览器查看时的状态，也就是我们可以像平时一样查看 Element 等属性，我们只需要定位到相应的标题行，得到对应的内容格式：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427221951190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Element1"></p>
<p>以及打开每一个 li 后得到的内容：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427221752383.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Element">我们可以写出代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">html=driver.page_source</span><br><span class="line">soup=BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">jx_tit=soup.find_all(<span class="string">"div"</span>,&#123;<span class="string">"class"</span>:<span class="string">"jx-tit"</span>&#125;)[<span class="number">0</span>].find_next_sibling().find_all(<span class="string">"li"</span>)</span><br></pre></td></tr></table></figure>
<p>然后根据上面第二张 Element 图中解析出的格式保存相关信息即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = open(<span class="string">'news.csv'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">csv_writer = csv.writer(f)</span><br><span class="line">csv_writer.writerow([<span class="string">"index"</span>,<span class="string">","</span>,<span class="string">"title"</span>,<span class="string">","</span>,<span class="string">"url"</span>])</span><br><span class="line"><span class="keyword">for</span> i,jxtit <span class="keyword">in</span> enumerate(jx_tit):    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        text=jxtit.find_all(<span class="string">"img"</span>)[<span class="number">0</span>][<span class="string">"alt"</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        text=jxtit.find_all(<span class="string">"div"</span>,&#123;<span class="string">"class"</span>:<span class="string">"lazyload-placeholder"</span>&#125;)[<span class="number">0</span>].text</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        url=jxtit.find_all(<span class="string">"a"</span>)[<span class="number">0</span>][<span class="string">"href"</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(jxtit)</span><br><span class="line">    csv_writer.writerow([i+<span class="number">1</span>,text,url]) </span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十一）—— Selenium 详解</title>
    <url>/posts/26182e00.html</url>
    <content><![CDATA[<h1 id="Selenium-库的安装"><a href="#Selenium-库的安装" class="headerlink" title="Selenium 库的安装"></a>Selenium 库的安装</h1><p>Selenium 的安装比起其他 python 库的安装稍显复杂，下面对此做简要介绍：<br>首先自然是 pip install selenium，然后我们需要安装对应的浏览器 driver，driver是什么呢，在介绍怎么安装之前，我们先看下面这两行代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Firefox()</span><br></pre></td></tr></table></figure>
<p>也就是说我们 selenium 是需要启用 Firefox 等浏览器才能执行某些操作的，因此我们需要让 selenium 可以通过某种方式连接到我们的浏览器。</p>
<h4 id="Firefox-driver-的安装"><a href="#Firefox-driver-的安装" class="headerlink" title="Firefox driver 的安装"></a>Firefox driver 的安装</h4><p>直接戳 <a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver/releases</a> ，在下面的这些地址中找到适合自己系统的安装包进行安装：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427201928244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="安装包">相应的，Chrome浏览器需要安装chromedriver，IE浏览器要安装IEdriver。</p>
<p>driver的路径可以直接放在python路径下，我这里把driver放在了python的Scripts路径下，同时把script路径加入环境变量。也可以选择将相应的 driver 安装到其他文件夹中，但要记得将这个文件夹添加到环境变量 PATH 中。</p>
<h4 id="Chrome-driver"><a href="#Chrome-driver" class="headerlink" title="Chrome driver"></a>Chrome driver</h4><p>Chrome driver 有两个下载地址：<br>1、<a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">http://chromedriver.storage.googleapis.com/index.html</a><br>2、<a href="https://npm.taobao.org/mirrors/chromedriver/" target="_blank" rel="noopener">https://npm.taobao.org/mirrors/chromedriver/</a></p>
<p>只需要记得一定要下载对应的版本即可！</p>
<p>同时 selenium 还支持 IE 浏览器，因为想来也没什么人会用，有需要的就自行安装吧。</p>
<h1 id="Selenium-入门"><a href="#Selenium-入门" class="headerlink" title="Selenium 入门"></a>Selenium 入门</h1><p>这里直接通过代码方式给大家演示 Selenium 的使用，大家并不需要了解每个语句的具体运作，只需要了解每条代码的大体意思即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Firefox()</span><br></pre></td></tr></table></figure>
<p>这部分代码是用来导入 selenium 库，此时会直接启动一个 Firefox 界面，它是由 selenium 直接控制的，我打开的是一个空白标签页：<br><img src="https://img-blog.csdnimg.cn/20200427205654733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="空白标签页"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).clear()</span><br><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).send_keys(<span class="string">"P2P"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"su"</span>).click()</span><br></pre></td></tr></table></figure>
<p>这三行代码会打开搜索框，清空搜索框内容，输入 P2P 字样，点击搜索，其中的 id 内容正是指向相应位置（点击按钮或输入框）的定位。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e_items = driver.find_elements_by_xpath(<span class="string">"//*[@class='result c-container ']"</span>)</span><br><span class="line">print(<span class="string">"\n"</span>.join([e.find_element_by_tag_name(<span class="string">"a"</span>).text <span class="keyword">for</span> e <span class="keyword">in</span> e_items]))</span><br><span class="line"><span class="comment"># P2P种子搜索器_p2psearcher官方下载【种子搜索神器】-华军软件园</span></span><br><span class="line"><span class="comment"># P2P金融_百度百科</span></span><br><span class="line"><span class="comment"># P2P_最有钛度的P2P资讯-钛媒体官方网站</span></span><br><span class="line"><span class="comment"># 种子搜索神器_p2p种子搜索器下载[p2psearcher]下载之家</span></span><br><span class="line"><span class="comment"># 网贷之家-中国普惠金融投资理财行业门户_P2P网贷银行存款贷款保险...</span></span><br><span class="line"><span class="comment"># 富金利_P2P平台_专业安全的P2P网贷投融资平台【唯一官网】</span></span><br><span class="line"><span class="comment"># P2P概念板块_股票行情-手机金融界</span></span><br></pre></td></tr></table></figure>
<p>这两行代码的作用是找到搜索条目中的标题并返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'\n'</span>.join([e.find_element_by_tag_name(<span class="string">"a"</span>).get_attribute(<span class="string">"href"</span>) <span class="keyword">for</span> e <span class="keyword">in</span> e_items]))</span><br></pre></td></tr></table></figure>
<p>上面这行代码是找到相应的标题链接，我们可以发现，其实这些函数名都和 js 的函数名几乎相同，下面这行代码是点击网页最下方的下一页：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_xpath(<span class="string">"//div[@id='page']/a/span[2]"</span>).click()</span><br></pre></td></tr></table></figure>
<p>我们可以综合上述代码，实现整个爬取标题过程的自动化，实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC </span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait </span><br><span class="line"></span><br><span class="line">driver.get(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">WebDriverWait(driver, <span class="number">10</span>).until(EC.presence_of_element_located((By.ID, <span class="string">"kw"</span>)))</span><br><span class="line"></span><br><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).clear()</span><br><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).send_keys(<span class="string">"P2P"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"su"</span>).click()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">5</span>,<span class="number">1</span>):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    e_items = driver.find_elements_by_xpath(<span class="string">"//*[@class='result c-container ']"</span>)</span><br><span class="line">    print(<span class="string">"page "</span>,i<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>.join([e.find_element_by_tag_name(<span class="string">"a"</span>).text <span class="keyword">for</span> e <span class="keyword">in</span> e_items]))</span><br><span class="line">    driver.find_element_by_xpath(<span class="string">f"//div[@id='page']/a/span[<span class="subst">&#123;i&#125;</span>]"</span>).click()</span><br></pre></td></tr></table></figure>
<p>事实上，我们并不需要自己写这些代码的函数，我们可以直接在 Firefox 安装 Katalon 插件，点击运行，然后在浏览器上执行我们需要执行的操作即可导出代码。</p>
<p>下面是 Katalon 扩展程序页面，点击 Record，然后在 Firefox 像往常操作一样输入内容，检索内容，翻页：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427211523448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="Katalon"></p>
<p>执行内容时，Katalon 会弹出一些提示窗，说明已经执行了指令并转化为了代码：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427211807844.png#pic_center" alt="Katalon 提示"><br>执行完毕后点击 Stop（在刚刚点击 Record 的位置）即可，然后点击 Export，就可以发现我们可以导出任何形式我们需要的代码，甚至可以自己定制：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427211940518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="导出代码"></p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（十）—— Session 和 Cookie 详解</title>
    <url>/posts/5cc32bbc.html</url>
    <content><![CDATA[<h1 id="Session-和-Cookie"><a href="#Session-和-Cookie" class="headerlink" title="Session 和 Cookie"></a>Session 和 Cookie</h1><p>我们先介绍 Session 和 Cookie 的区别：</p>
<h4 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h4><p>在网站中，http 请求是无状态的。也就是说即使第一次和服务器连接后并且登录成功后，第二次请求服务器依然不能知道当前请求是哪个用户。cookie 的出现就是为了解决这个问题，第一次登录后服务器返回一些数据（cookie）给浏览器，然后浏览器保存在本地，当该用户发送第二次请求的时候，就会自动的把上次请求存储的 cookie 数据自动的携带给服务器，服务器通过浏览器携带的数据就能判断当前用户是哪个了。cookie 存储的数据量有限，不同的浏览器有不同的存储大小，但一般不超过4KB。因此使用 cookie 只能存储一些小量的数据。<br>可以简单理解为 Cookies 中保存了登录凭证，我们只要持有这个凭证，就可以在服务端保持一个登录状态。</p>
<h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>session和cookie的作用有点类似，都是为了存储用户相关的信息。不同的是，cookie是存储在本地浏览器，而session存储在服务器。存储在服务器的数据会更加的安全，不容易被窃取。但存储在服务器也有一定的弊端，就是会占用服务器的资源，但现在服务器已经发展至今，一些session信息还是绰绰有余的。</p>
<h4 id="两者的联系"><a href="#两者的联系" class="headerlink" title="两者的联系"></a>两者的联系</h4><p>我们举个例子，在我们执行登录操作时，当我们输入好用户名和密码后，客户端会将这个 Cookies 放在请求头一起发送给服务端，这时，服务端就知道是谁在进行登录操作，并且可以判断这个人输入的用户名和密码对不对，如果输入正确，则在服务端的 Session 记录一下这个人已经登录成功了，下次再请求的时候这个人就是登录状态了。</p>
<p>如果客户端传给服务端的 Cookies 是无效的，或者这个 Cookies 根本不是由这个服务端下发的，或者这个 Cookies 已经过期了，那么接下里的请求将不再能访问需要登录后才能访问的页面。</p>
<p>所以， Session 和 Cookies 之间是需要相互配合的，一个在服务端，一个在客户端。</p>
<p>事实上，在如今的市场或者企业里，一般有两种存储方式：<br>1、存储在服务端：通过cookie存储一个session_id，然后具体的数据则是保存在session中。如果用户已经登录，则服务器会在cookie中保存一个session_id，下次再次请求的时候，会把该session_id携带上来，服务器根据session_id在session库中获取用户的session数据。就能知道该用户到底是谁，以及之前保存的一些状态信息。这种专业术语叫做server side session。<br>2、将session数据加密，然后存储在cookie中。这种专业术语叫做client side session。flask采用的就是这种方式，但是也可以替换成其他形式。</p>
<h4 id="查看-Cookie"><a href="#查看-Cookie" class="headerlink" title="查看 Cookie"></a>查看 Cookie</h4><p>我们打开京东的网站，在 Chrome 中按 F12 打开开发者工具，选择 Application 标签，点开 Cookies 这一栏，会出现如下页面：<br><img src="https://img-blog.csdnimg.cn/20200427153645515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="京东 Cookie"></p>
<p>我们解析一下这些参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>内容</th>
<th>解析</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>这个是 Cookie 的名字。一旦创建，该名称便不可更改</td>
</tr>
<tr>
<td>Value</td>
<td>这个是 Cookie 的值</td>
</tr>
<tr>
<td>Domain</td>
<td>这个是可以访问该 Cookie 的域名。例如，如果设置为 .jd.com ，则所有以 jd.com ，结尾的域名都可以访问该Cookie</td>
</tr>
<tr>
<td>Max Age</td>
<td>Cookie 失效的时间，单位为秒，也常和 Expires 一起使用。 Max Age 如果为正数，则在 Max Age 秒之后失效，如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会保存该 Cookie </td>
</tr>
<tr>
<td>Path</td>
<td>Cookie 的使用路径。如果设置为 /path/ ，则只有路径为 /path/ 的页面可以访问该 Cookie 。如果设置为 / ，则本域名下的所有页面都可以访问该 Cookie</td>
</tr>
<tr>
<td>Size</td>
<td>Cookie 的大小</td>
</tr>
<tr>
<td>HTTPOnly</td>
<td>如果此项打勾，那么通过 JS 脚本将无法读取到 Cookie 信息，这样能有效的防止 XSS 攻击，窃取 Cookie 内容，可以增加 Cookie 的安全性</td>
</tr>
<tr>
<td>Secure</td>
<td>如果此项打勾，那么这个 Cookie 只能用 HTTPS 协议发送给服务器，用 HTTP 协议是不发送的</td>
</tr>
</tbody>
</table>
</div>
<h4 id="退出操作"><a href="#退出操作" class="headerlink" title="退出操作"></a>退出操作</h4><p>当我们关闭浏览器的时候会自动销毁服务端的会话，这个是错误的，因为在关闭浏览器的时候，浏览器并不会额外的通知服务端说，我要关闭了，你把和我的会话销毁掉吧。<br>因为服务端的会话是保存在内存中的，虽然一个会话不会很大，但是架不住会话多啊，硬件毕竟是会有限制的，不能无限扩充下去的，所以在服务端设置会话的过期时间就非常有必要。<br>当然，有没有方式能让浏览器在关闭的时候同步的关闭服务端的会话，当然是可以的，我们可以通过脚本语言 JS 来监听浏览器关闭的动作，当浏览器触发关闭动作的时候，由 JS 像服务端发起一个请求来通知服务端销毁会话。<br>由于不同的浏览器对 JS 事件的实现机制不一致，不一定保证 JS 能监听到浏览器关闭的动作，所以现在常用的方式还是在服务端自己设置会话的过期时间</p>
<h1 id="模拟登录163"><a href="#模拟登录163" class="headerlink" title="模拟登录163"></a>模拟登录163</h1><p>在这里我们提前使用了 Selenium 库，该库的具体使用方法可以见下期，下面简单展示相关代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"></span><br><span class="line">name = <span class="string">'*'</span></span><br><span class="line">passwd = <span class="string">'*'</span></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">'https://mail.163.com/'</span>)</span><br><span class="line"><span class="comment"># 将窗口调整最大</span></span><br><span class="line">driver.maximize_window()</span><br><span class="line"><span class="comment"># 休息5s</span></span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">current_window_1 = driver.current_window_handle</span><br><span class="line">print(current_window_1)</span><br><span class="line"><span class="comment"># CDwindow-28F02680782B96D54B997F9A8E8334DD</span></span><br><span class="line"></span><br><span class="line">button = driver.find_element_by_id(<span class="string">'lbNormal'</span>)</span><br><span class="line">button.click()</span><br><span class="line">driver.switch_to.frame(driver.find_element_by_xpath(<span class="string">"//iframe[starts-with(@id, 'x-URS-iframe')]"</span>))</span><br><span class="line"></span><br><span class="line">email = driver.find_element_by_name(<span class="string">'email'</span>)</span><br><span class="line"><span class="comment">#email = driver.find_element_by_xpath('//input[@name="email"]')</span></span><br><span class="line">email.send_keys(name)</span><br><span class="line">password = driver.find_element_by_name(<span class="string">'password'</span>)</span><br><span class="line"><span class="comment">#password = driver.find_element_by_xpath("//input[@name='password']")</span></span><br><span class="line">password.send_keys(passwd)</span><br><span class="line">submit = driver.find_element_by_id(<span class="string">"dologin"</span>)</span><br><span class="line">time.sleep(<span class="number">15</span>)</span><br><span class="line">submit.click()</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">print(driver.page_source)</span><br><span class="line"><span class="comment"># 返回页面源代码</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（九）—— ajax 详解</title>
    <url>/posts/1dee9666.html</url>
    <content><![CDATA[<h1 id="ajax-介绍"><a href="#ajax-介绍" class="headerlink" title="ajax 介绍"></a>ajax 介绍</h1><p>我们看到 ajax 这个词，大多数人都会觉得这个词和以前的不太一样，似乎听的更少了更陌生了，我们要怎么理解 ajax 呢？我们先看看它的英文全称：AJAX = Asynchronous JavaScript and XML，翻译成中文就是异步的 JavaScript 和 XML，异步也就是说，它可以在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。<br>举个例子：你打开一个股票实时数据网站，假如这个网站不是使用 ajax 的，那么你每次想要得到这个网站的最新信息都需要刷新一下这个网站，而若采用了 ajax，你可以实时获得股票的信息更新，而无需刷新网站。因此简单来说，ajax 是一种用于创建快速动态网页的技术，它可以实现在不重新加载整个网页的情况下，对网页的某部分进行更新。<br>下面是 AJAX 工作原理图：<br><img src="https://img-blog.csdnimg.cn/2020042711365962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="AJAX工作原理"></p>
<p>这张图其实已经很清晰的反映了我们对 ajax 信息进行爬取的路线 —— 根据需要的内容，模拟相应的请求，发送至服务器，得到需要的内容。可能术语说的尚不标准，但大概就是这个意思。</p>
<h1 id="分析网页的-Ajax-请求"><a href="#分析网页的-Ajax-请求" class="headerlink" title="分析网页的 Ajax 请求"></a>分析网页的 Ajax 请求</h1><p>我们打开下面网址：<a href="https://unsplash.com/" target="_blank" rel="noopener">https://unsplash.com/</a> 这是一个图片网站，页面如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427120716268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="unsplash 页面"><br>我们希望获取到该页面所有图片，先点开控制台的 Network，最开始可能什么都没有，这时候我们刷新页面，勾选XML，就得到了这么多返回数据：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427120911945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="返回数据"><br>我们随便点开一个 “photo?” 的内容，点开 preview：</p>
<p><img src="https://img-blog.csdnimg.cn/20200427121204639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="preview"><br>找到 urls 的 full，复制链接到浏览器打开，得到的正是我们需要的图片。我们分析一下这个 Ajax 请求，找到 Header 部分：<br><img src="https://img-blog.csdnimg.cn/20200427121538482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="header"><br> 于是我们的思路就是根据这个 Headers 模拟 Ajax 请求，从网页的回复中解析出图片的 URL 列表，然后遍历图片的 URL 列表得到图片，存储下来即可。这个思路不正和上面的 Ajax 工作流程图吻合吗？</p>
<h1 id="代码测试"><a href="#代码测试" class="headerlink" title="代码测试"></a>代码测试</h1><p>下面我们测试上面的想法的可行性：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> time</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>header = &#123;<span class="string">"referer"</span>:<span class="string">"https://unsplash.com/"</span>, <span class="string">"user-agent"</span>:<span class="string">"Mozilla/5.0"</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ajax_url = <span class="string">"https://unsplash.com/napi/photos?page=3&amp;per_page=12"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = requests.get(ajax_url, headers=header)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.encoding = res.apparent_encoding</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = res.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>js = json.loads(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(js)</span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>js[<span class="number">1</span>][<span class="string">"urls"</span>]</span><br><span class="line">&#123;<span class="string">'raw'</span>: <span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span>, <span class="string">'full'</span>: <span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;q=85&amp;fm=jpg&amp;crop=entropy&amp;cs=srgb&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span>, <span class="string">'regular'</span>: <span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span>, <span class="string">'small'</span>: <span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=400&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span>, <span class="string">'thumb'</span>: <span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=200&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>js[<span class="number">1</span>][<span class="string">"urls"</span>][<span class="string">"raw"</span>]</span><br><span class="line"><span class="string">'https://images.unsplash.com/photo-1587825159137-0b606e25fe80?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9'</span></span><br></pre></td></tr></table></figure>
<p>我们将最后得到的这段 URL 输入网址查询，发现确实是我们需要找的图片内容，我们倒数第二段代码可以发现，这些返回的内容正是我们之前看到的那些内容，于是我们可以着手编写完整代码。</p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">header = &#123;<span class="string">"referer"</span>:<span class="string">"https://unsplash.com/"</span>, <span class="string">"user-agent"</span>:<span class="string">"Mozilla/5.0"</span>&#125;</span><br><span class="line">url_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    ajax_url = <span class="string">"https://unsplash.com/napi/photos?page="</span> + str(i) + <span class="string">"&amp;per_page=12"</span></span><br><span class="line">    url_list.append(ajax_url)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_json</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = requests.get(url, headers=header)</span><br><span class="line">        res.raise_for_status</span><br><span class="line">        res.encoding = res.apparent_encoding</span><br><span class="line">        text = res.text</span><br><span class="line">        js = json.loads(text)</span><br><span class="line">        <span class="keyword">return</span> js</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"get_json ERROR"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ParseFromJSON</span><span class="params">(list_, json)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(json)):</span><br><span class="line">        url_i = &#123;&#125;</span><br><span class="line">        url_i[<span class="string">"url"</span>] = js[i][<span class="string">"urls"</span>][<span class="string">"raw"</span>]</span><br><span class="line">        url_i[<span class="string">"id"</span>] = js[i][<span class="string">"id"</span>]</span><br><span class="line">        list_.append(url_i)</span><br><span class="line">    <span class="keyword">return</span> list_</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPic</span><span class="params">(pic_list)</span>:</span></span><br><span class="line">    path = <span class="string">"UnsplashPic"</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">    <span class="keyword">for</span> pic <span class="keyword">in</span> pic_list:</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(path + <span class="string">"/"</span> + pic[<span class="string">"id"</span>] + <span class="string">".jpg"</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = requests.get(pic[<span class="string">"url"</span>], headers=header)</span><br><span class="line">            <span class="keyword">if</span> res.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="keyword">with</span> open(path + <span class="string">"/"</span> + pic[<span class="string">"id"</span>] + <span class="string">".jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(res.content)</span><br><span class="line">                    print(<span class="string">"PIC"</span>+pic[<span class="string">"id"</span>]+<span class="string">"is saved"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"STATUS_CODE ERROR"</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"PIC"</span>+pic[<span class="string">"id"</span>]+<span class="string">" save FAIL"</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    pic_list = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        js = get_json(url)</span><br><span class="line">        ParseFromJSON(pic_list, js)</span><br><span class="line">    getPic(pic_list)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（八）—— lxml 详解及代理IP爬取</title>
    <url>/posts/ea515678.html</url>
    <content><![CDATA[<p>前一篇文章的末尾我们提到，可以使用 lxml + xpath 提取文章内容，在这篇文章中，我们将对 lxml 与 xpath 进行详细阐述，在这之前我们使用 pip install lxml 安装 lxml 库。</p>
<h1 id="Xpath-语法"><a href="#Xpath-语法" class="headerlink" title="Xpath 语法"></a>Xpath 语法</h1><p>XPath 即为 XML 路径语言（XML Path Language），它是一种用来确定 XML 文档中某部分位置的语言。而 XML 我们在<a href="https://blog.csdn.net/ChenKai_164/article/details/105635357" target="_blank" rel="noopener">这篇文章</a>中已经提到了，它的语法与 HTML 基本一致，可以说是通过 HTML发展而来的通用表达形式。</p>
<h3 id="路径表达式"><a href="#路径表达式" class="headerlink" title="路径表达式"></a>路径表达式</h3><p>XPath 使用路径表达式在 XML 文档中选取节点，节点是通过沿着路径选取的。下面列出了最常用的路径表达式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>nodename</td>
<td>选取此节点的所有节点</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点</td>
</tr>
<tr>
<td>@</td>
<td>选取属性</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Predicates"><a href="#Predicates" class="headerlink" title="Predicates"></a>Predicates</h3><div class="table-container">
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
<th>实例</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>[n]</td>
<td>选择某节点的第n(n&gt;=1)个子节点</td>
<td>xpath(“//h//a[1]”)</td>
<td>选择h节点下第1个a节点</td>
</tr>
<tr>
<td>[last()]</td>
<td>选择某节点的最后一个子节点</td>
<td>xpath(“//h//a[last()]”)</td>
<td>选择h节点下最后一个a节点</td>
</tr>
<tr>
<td>[@attribute]</td>
<td>选择节点带有attribute属性的节点</td>
<td>xpath(“//img[@src]”)</td>
<td>选择带有src属性的img节点</td>
</tr>
<tr>
<td>[@attribute=value]</td>
<td>选择带有attribute属性值的节点</td>
<td>xpath(“//a[@href=”aaa.jpg”]”)</td>
<td>选择href属性值为aaa.jpg的a节点</td>
</tr>
<tr>
<td>*</td>
<td>任意匹配元素或者属性</td>
<td>xpath(“//a/“),xpath(“//a[@]”)</td>
<td>选择a节点下的所有子节点,选择带有属性的a节点</td>
</tr>
</tbody>
</table>
</div>
<h3 id="模糊搜索与匹配"><a href="#模糊搜索与匹配" class="headerlink" title="模糊搜索与匹配"></a>模糊搜索与匹配</h3><div class="table-container">
<table>
<thead>
<tr>
<th>函数</th>
<th>用法</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>starts-with</td>
<td>xpath(“//div[starts-with(@id,’user’)]”)</td>
<td>选择id值以user开头的div节点</td>
</tr>
<tr>
<td>contains</td>
<td>xpath(“//div[contains(@id,’user’)]”)</td>
<td>选择id值包含user的div节点</td>
</tr>
<tr>
<td>and</td>
<td>xpath(“//div[starts-with(@class,”login”) and contains(@id,’user’)]”)</td>
<td>选择class值以login开头和id值包括user的div节点</td>
</tr>
<tr>
<td>text()</td>
<td>xpath(“//div[starts-with(text(),”mytest”)]”)</td>
<td>选取节点文本包含myest的div节点</td>
</tr>
</tbody>
</table>
</div>
<h3 id="xpath轴"><a href="#xpath轴" class="headerlink" title="xpath轴"></a>xpath轴</h3><div class="table-container">
<table>
<thead>
<tr>
<th>轴名称</th>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ancestor</td>
<td>xpath(“./ancestor:: *”)</td>
<td>选取当前节点的所有父辈节点</td>
</tr>
<tr>
<td>ancestor-or.self</td>
<td>xpath(“./ancestor-or-self:: *”)</td>
<td>选取当前节点的父辈节点和节点自身</td>
</tr>
<tr>
<td>child</td>
<td>xpath(“./child:: *”)</td>
<td>选择当前节点的所有子节点</td>
</tr>
<tr>
<td>descendant</td>
<td>xpath(“./descendant:: *”)</td>
<td>选择当前节点的所有后代节点(子节点、孙节点等)</td>
</tr>
<tr>
<td>follow</td>
<td>xpath(“./following:: *”)</td>
<td>选取当前节点结束标签后的所有节点</td>
</tr>
<tr>
<td>follow-sibling</td>
<td>xpath(“./follow-sibling:: *”)</td>
<td>选取当前节点之后的兄弟节点</td>
</tr>
<tr>
<td>preceding</td>
<td>xpath(“./preceding:: *”)</td>
<td>选取当前开始标签前的所有节点</td>
</tr>
</tbody>
</table>
</div>
<h1 id="lxml-使用"><a href="#lxml-使用" class="headerlink" title="lxml 使用"></a>lxml 使用</h1><p>首先需要导入库：from lxml import etree，下面这行代码 lxml 会将 html 文本转成 xml 对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tree = etree.HTML(html)</span><br></pre></td></tr></table></figure>
<p>我们要使用 lxml + Xpath 从源代码直接获取信息时，会使用 tree.xpath() 语句，括号内的是所需获取内容的一些形式，具体形式在上面已经列出了，比如我们要获取<a href="http://www.dxy.cn/bbs/topic/43201486" target="_blank" rel="noopener">丁香园</a>页面的用户名称信息，首先查看该页面源代码：<br><img src="https://img-blog.csdnimg.cn/20200425135042154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70" alt="用户信息">我们可以使用：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tree.xpath(<span class="string">'//div[@class=“auth”]/a/text()'</span>)</span><br></pre></td></tr></table></figure><br>我们想要获取评论信息：<br><img src="https://img-blog.csdnimg.cn/20200425135042227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70" alt="评论信息"><br>我们可以使用<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tree.xpath(<span class="string">'//td[@class=“postbody”]'</span>)</span><br></pre></td></tr></table></figure><br>我们需要注意的是，当我们将文本对象转换为了 lxml 对象后，实际获取内容的方式也发生了改变，参看下面代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"><span class="comment"># 返回 &lt;Element html at 0x233468e5788&gt;</span></span><br><span class="line">result = etree.tostring(html)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回 text 中信息</span></span><br></pre></td></tr></table></figure></p>
<h1 id="代理-IP-爬取"><a href="#代理-IP-爬取" class="headerlink" title="代理 IP 爬取"></a>代理 IP 爬取</h1><p>介绍完 xpath 的基本内容，我们下面来实战一个非常有用的爬虫 —— 爬取代理IP，其具体页面内容如下：<br><img src="https://img-blog.csdnimg.cn/20200425133902765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="西刺代理IP主页"><br>我们点击查看源代码，可以发现相关的IP地址直接出现在页面中，因此我们能够通过现有知识进行爬取：</p>
<p><img src="https://img-blog.csdnimg.cn/20200425134041233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="西刺代理IP主页源代码"><br>下面是测试代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">"https://www.xicidaili.com/"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>headers = &#123;<span class="string">"user-agent"</span>:<span class="string">"Mozilla/5.0"</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = requests.get(url, headers=headers)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.encoding = res.apparent_encoding</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = res.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>html = etree.HTML(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>html.xpath(<span class="string">'/html/body/div[1]/div[2]/div[1]/div[1]/table/tbody/tr[3]/td[2]'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ip_ = html.xpath(<span class="string">'/html/body/div[1]/div[2]/div[1]/div[1]/table//tr[3]/td[2]'</span>)</span><br><span class="line">[&lt;Element td at <span class="number">0x233473b8d08</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = etree.tostring(ip_[<span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result</span><br><span class="line"><span class="string">b'&lt;td&gt;119.4.13.26&lt;/td&gt;\n</span></span><br></pre></td></tr></table></figure></p>
<p>事实上，我们在实际使用时，想要获得 Xpath 不一定会直接一行行代码看过去，更可能直接复制 Xpath：</p>
<p><img src="https://img-blog.csdnimg.cn/20200425140302427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="查看器获取 Xpath"></p>
<p>我们右键移至复制，会出现一个名为“复制Xpath”的选项，点击复制Xpath 后即可得到相应的 Xpath，但在这里有一点需要注意的是，Xpath 会给页面 HTML 树进行自动补齐，也就是说会出现一些原来没有的东西，就会出现上面代码中倒数第六行出现空返回的问题，这时候就需要我们去仔细检查哪里的信息可能是多余的。</p>
<p>下面我们展示完整代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle<span class="comment">#我们封装数据要用到的库</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment">#请求头</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_url</span><span class="params">()</span>:</span><span class="comment">#老样子，先构建url列表，我这就爬一页</span></span><br><span class="line">    urls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">1</span>,<span class="number">2</span>):</span><br><span class="line">        url = <span class="string">'https://www.xicidaili.com/nn/'</span>+str(i)</span><br><span class="line">        urls.append(url)</span><br><span class="line">    <span class="keyword">return</span> urls</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(url)</span>:</span><span class="comment">#从数据中解析出ip数据，并验证ip是否可用</span></span><br><span class="line">    res = requests.get(url,headers=headers)</span><br><span class="line">    html = etree.HTML(res.text,etree.HTMLParser())</span><br><span class="line">    <span class="comment">#把返回的数据解析成etree._Element对象，这样才可以使用Xpath语句提取数据</span></span><br><span class="line">    ip = html.xpath(<span class="string">'//*[@id="ip_list"]/tr/td[2]/text()'</span>)<span class="comment">#ip</span></span><br><span class="line">    ip_port = html.xpath(<span class="string">'//*[@id="ip_list"]/tr/td[3]/text()'</span>)<span class="comment">#端口号</span></span><br><span class="line">    ip_type = html.xpath(<span class="string">'//*[@id="ip_list"]/tr/td[6]/text()'</span>)<span class="comment">#网络类型</span></span><br><span class="line"></span><br><span class="line">    lis = []</span><br><span class="line">    <span class="keyword">for</span> i,j,k <span class="keyword">in</span> zip(ip,ip_port,ip_type):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            proxies = &#123;</span><br><span class="line">                    k.lower():i+<span class="string">':'</span>+j<span class="comment">#注意到爬下来的是HTTP或者是HTTPS，我们要将它们变成http或https</span></span><br><span class="line">                    &#125;</span><br><span class="line">            print(proxies)<span class="comment">#把IP输出来看看</span></span><br><span class="line">            requests.get(<span class="string">'https://www.baidu.com/'</span>,headers=headers,timeout=<span class="number">1</span>,proxies=proxies)</span><br><span class="line">            <span class="comment">#用这个IP发一个请求给百度，测试一下这个IP能不能用</span></span><br><span class="line">            lis.append(proxies)<span class="comment">#如果没有超时，说明能用，存进列表里</span></span><br><span class="line">            print(<span class="string">'http://'</span>+i+<span class="string">':'</span>+j+<span class="string">" is fine"</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">'http://'</span>+i+<span class="string">':'</span>+j+<span class="string">" is timeout"</span>)<span class="comment">#超时抛出错误，输出哪个IP超时</span></span><br><span class="line">    <span class="keyword">return</span> lis<span class="comment">#返回可用IP列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    urls = construct_url()</span><br><span class="line">    lis = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        rlist = get_proxy(url)</span><br><span class="line">        lis += rlist</span><br><span class="line">    pickle.dump(lis,open(<span class="string">'ip.pkl'</span>,<span class="string">'wb'</span>))<span class="comment">#将可用的IP进行存储</span></span><br><span class="line">    print(<span class="string">'----可用IP---'</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> lis:</span><br><span class="line">        print(item)</span><br><span class="line">    print(<span class="string">'---  END  ---'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（七）—— 丁香园评论留言板爬取</title>
    <url>/posts/4905f1d8.html</url>
    <content><![CDATA[<h1 id="观察待爬取页面，判断爬取可行性"><a href="#观察待爬取页面，判断爬取可行性" class="headerlink" title="观察待爬取页面，判断爬取可行性"></a>观察待爬取页面，判断爬取可行性</h1><p>我们首先查看待爬取页面：<a href="http://www.dxy.cn/bbs/thread/626626#626626" target="_blank" rel="noopener">http://www.dxy.cn/bbs/thread/626626#626626</a> ，具体形式为下图：<br><img src="https://img-blog.csdnimg.cn/20200425085817786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70" alt="丁香园待爬取页面">我们查看源代码信息：<br><img src="https://img-blog.csdnimg.cn/20200425085925913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="页面源代码">可以发现留言板块的内容在页面源代码全部直接显示了出来，那么我们可以尝试直接进行爬取，我们采取的思路首先是 <a href="https://blog.csdn.net/ChenKai_164/article/details/105610353" target="_blank" rel="noopener">requests 库</a> + <a href="https://blog.csdn.net/ChenKai_164/article/details/105625042" target="_blank" rel="noopener">bs4 库</a>，这两者的具体用法在前面的文章均已涉及。</p>
<h1 id="检测爬取链接，初步尝试爬取"><a href="#检测爬取链接，初步尝试爬取" class="headerlink" title="检测爬取链接，初步尝试爬取"></a>检测爬取链接，初步尝试爬取</h1><p>首先我们可以在IDLE上检查爬取链接是否正常，直接展示代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">"http://www.dxy.cn/bbs/thread/626626#626626"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>headers = &#123;<span class="string">"user-agent"</span>:<span class="string">"Mozilla/5.0"</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = requests.get(url, headers=headers, stream=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = res.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text[<span class="number">1000</span>:<span class="number">1300</span>]</span><br><span class="line"><span class="string">'og:description" content="我遇到一个“怪”病人，向大家请教。她，42岁。反复惊吓后晕厥30余年。每次受响声惊吓后发生跌倒，短暂意识丧失。无逆行性遗忘，无抽搐，无口吐白沫，无大小便失禁。多次跌倒致外伤。婴儿时有惊厥史。入院查体无殊。ECG、24小时动态心电图无殊；头颅MRI示小软化灶；脑电图无殊。入院后有数次类似发作。请问该患者该做何诊断，还需做什么检查，治疗方案怎样？"/&gt;\n        &lt;meta property="og:author" content="楼医生"/&gt;\n        &lt;meta property="og:release_date" content'</span></span><br></pre></td></tr></table></figure>
<p>我们想要抓取链接，先在上面的源代码中对标题层级结构进行分析:<br><img src="https://img-blog.csdnimg.cn/20200425091636816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="标题代码"></p>
<p>然后直接写出相关代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>title = soup(<span class="string">"div"</span>, id=<span class="string">"postview"</span>)[<span class="number">0</span>].tr.th.h1.contents[<span class="number">0</span>]</span><br><span class="line">[<span class="string">'\n                    晕厥待查——请教各位同仁                 '</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>title = title.replace(<span class="string">" "</span>, <span class="string">""</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>title = title.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">&gt;&gt;&gt;&gt; title</span><br><span class="line"><span class="string">'晕厥待查——请教各位同仁'</span></span><br></pre></td></tr></table></figure>
<p>下面同理可以查看其他信息，下面我们为了便于操作直接采用 CSS 类名查找：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>context = soup.select(<span class="string">"[class~=postbody]"</span>)</span><br><span class="line"><span class="comment"># 返回所有内容信息</span></span><br></pre></td></tr></table></figure>
<p>返回的信息内容如下：<br><img src="https://img-blog.csdnimg.cn/20200425093131992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="爬取返回内容">对内容进行清理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cont = context[<span class="number">1</span>].contents[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cont = cont.replace(<span class="string">" "</span>, <span class="string">""</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cont = cont.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cont</span><br><span class="line"><span class="string">'从发作的症状上比较符合血管迷走神经性晕厥，直立倾斜试验能协助诊断。在行直立倾斜实验前应该做常规的体格检查、ECG、UCG、holter和X-ray胸片除外器质性心脏病。'</span></span><br></pre></td></tr></table></figure>
<p>测试代码写完后，我们写出完整代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTML</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;<span class="string">"user-agent"</span>:<span class="string">"Mozilla/5.0"</span>&#125;</span><br><span class="line">        res = requests.get(url, headers=headers, stream=<span class="literal">True</span>)</span><br><span class="line">        res.raise_for_status</span><br><span class="line">        res.encoding = res.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> res.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ParseFromPage</span><span class="params">(html)</span>:</span></span><br><span class="line">    commentList = []</span><br><span class="line">    soup = BeautifulSoup(html)</span><br><span class="line">    title = soup(<span class="string">"div"</span>, id=<span class="string">"postview"</span>)[<span class="number">0</span>].tr.th.h1.contents[<span class="number">0</span>]</span><br><span class="line">    title = title.replace(<span class="string">" "</span>, <span class="string">""</span>)</span><br><span class="line">    title = title.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">    context = soup.select(<span class="string">"[class~=postbody]"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(context)):</span><br><span class="line">        cont = context[i].contents[<span class="number">0</span>]</span><br><span class="line">        cont = cont.replace(<span class="string">" "</span>, <span class="string">""</span>)</span><br><span class="line">        commentList.append(cont)</span><br><span class="line">    <span class="keyword">return</span> title, commentList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">"http://www.dxy.cn/bbs/thread/626626#626626"</span></span><br><span class="line">    html = getHTML(base_url)</span><br><span class="line">    title, cList = ParseFromPage(html)</span><br><span class="line">    print(title, cList)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h1 id="拓展一"><a href="#拓展一" class="headerlink" title="拓展一"></a>拓展一</h1><p>我们爬取完单一页面后，我们可以考虑，是否可以爬取更多的页面。我们的思考出发点是丁香园每个页面的链接下面都会存在一个引向其他同类型页面的链接，比如：<br><img src="https://img-blog.csdnimg.cn/2020042510254560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="链接抓取">我们可以看到，下面的 “骨折手术后一周突发心跳骤停，抢救无效死亡，什么原因？”这段话不就是我们要找的链接，点开后也确实如我们所料，页面结构的解析和本页面的解析一样，我们完全可以复用这段代码，只需要改变一下 url 即可。下面我们尝试通过 re 库获取此 url 链接：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>href = re.findall(<span class="string">r'&amp;#149; &lt;a href="(.*?)" target'</span>, text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>href</span><br><span class="line">[<span class="string">'http://job.dxy.cn/bbs/topic/43200651'</span>, <span class="string">'http://www.dxy.cn/bbs/topic/43177945'</span>, <span class="string">'http://Radiology.dxy.cn/bbs/topic/43179123'</span>, <span class="string">'http://www.dxy.cn/bbs/topic/43172871'</span>]</span><br></pre></td></tr></table></figure>
<p>这里我们看到有四个链接，但并不是每个链接都是我们需要的，通过观察后我们发现，只有以 “<a href="http://www.dxy.cn/bbs/topic" target="_blank" rel="noopener">http://www.dxy.cn/bbs/topic</a>“ 开头的链接才是我们需要的，我们可以将查找链接的方式改为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.findall(<span class="string">r'&lt;a href="(http://www.dxy.cn/bbs/topic/.*?) target="_blank"'</span>,text)</span><br><span class="line">[<span class="string">'http://www.dxy.cn/bbs/topic/43177945"'</span>, <span class="string">'http://www.dxy.cn/bbs/topic/43172871"'</span>]</span><br></pre></td></tr></table></figure>
<p>我们看到返回了两个正确结果，为了不增大网站压力，我们修改主函数，仅仅爬取十条链接的内容，并存储到 dxy.txt 文件中，主函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">"http://www.dxy.cn/bbs/thread/626626#626626"</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        html = getHTML(base_url)</span><br><span class="line">        title, cList = ParseFromPage(html)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"dxy.txt"</span>, <span class="string">"a"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(title)</span><br><span class="line">            f.write(<span class="string">"\n"</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(cList)):</span><br><span class="line">                f.write(cList[i])</span><br><span class="line">                f.write(<span class="string">"\n"</span>)</span><br><span class="line">            f.write(<span class="string">"\n"</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url_list = re.findall(<span class="string">r'&lt;a href="(http://www.dxy.cn/bbs/topic/.*?) target="_blank"'</span>,html)</span><br><span class="line">            base_url = url_list[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p>此代码应能正确爬取链接，代码规范有待网友指正。</p>
<h1 id="拓展二"><a href="#拓展二" class="headerlink" title="拓展二"></a>拓展二</h1><p>我们还可以使用 lxml 爬取内容，事实上此代码会更加简洁, lxml 的具体用法将在下期内容展开，以下直接简单使用之：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html, etree</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree1 = html.tostring(tree.xpath(<span class="string">'//td[@class="postbody"]'</span>)[<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>txt = HTMLParser().unescape(tree1.decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>txt.replace(<span class="string">" "</span>,<span class="string">""</span>)</span><br><span class="line"><span class="string">'&lt;tdclass="postbody"&gt;\n\n从发作的症状上比较符合血管迷走神经性晕厥，直立倾斜试验能协助诊断。在行直立倾斜实验前应该做常规的体格检查、ECG、UCG、holter和X-ray胸片除外器质性心脏病。&lt;br&gt;&lt;br&gt;贴一篇“口服氨酰心安和依那普利治疗血管迷走性晕厥的疗效观察”&lt;br&gt;作者：林文华任自文丁燕生&lt;br&gt;&lt;br&gt;&lt;ahref="http://www.ccheart.com.cn/ccheart_site/Templates/jieru/200011/1-1.htm"target="_blank"class="ilink"rel="nofollow"&gt;http://www.ccheart.com.cn/ccheart_site/Templates/jieru/200011/1-1.htm&lt;/a&gt;\n\t&lt;/td&gt;\n'</span>                           <span class="string">'</span></span><br></pre></td></tr></table></figure>
<p>其他信息的抽取以及代码的结构化同上即可。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（六）—— 正则表达式库入门</title>
    <url>/posts/617da02a.html</url>
    <content><![CDATA[<p>正则表达式，即 RE，是 regular expression 的简称，是用来简洁表达一组字符串的表达式。</p>
<h1 id="正则表达式的语法"><a href="#正则表达式的语法" class="headerlink" title="正则表达式的语法"></a>正则表达式的语法</h1><div class="table-container">
<table>
<thead>
<tr>
<th>操 作 符</th>
<th>说 明</th>
<th>正 则 表 达 式 样 例</th>
</tr>
</thead>
<tbody>
<tr>
<td> .</td>
<td>匹配任何字符（换行符除外）</td>
<td>b.b</td>
</tr>
<tr>
<td>[…]</td>
<td>匹配字符组里出现的任意一个字符</td>
<td>[abcd]</td>
</tr>
<tr>
<td>*</td>
<td>匹配前面出现的正则表达式零次或多次</td>
<td>abc*</td>
</tr>
<tr>
<td>+</td>
<td>匹配前面出现的正则表达式一次或多次</td>
<td>abc+</td>
</tr>
<tr>
<td>？</td>
<td>匹配前面出现的正则表达式零次或一次</td>
<td>abc?</td>
</tr>
<tr>
<td>\</td>
<td></td>
<td>匹配左或右任意一个正则表达式</td>
<td>re1</td>
<td>re2</td>
</tr>
<tr>
<td>{A}</td>
<td>匹配前面出现的正则表达式A次</td>
<td>[0-9]{5}</td>
</tr>
<tr>
<td>{A, B}</td>
<td>匹配前面出现的正则表达式A-B次（含B）</td>
<td>[0-9]{1, 5}</td>
</tr>
<tr>
<td>^</td>
<td>匹配字符串的开始</td>
<td>^abc</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串的结束</td>
<td>abc$</td>
</tr>
<tr>
<td>[…a-b…]</td>
<td>匹配从字符a-b中的任意一个字符</td>
<td>[0-9],[A-Za-z]</td>
</tr>
<tr>
<td><sup><a href="#fn_..." id="reffn_...">...</a></sup></td>
<td>不匹配此字符集中出现的任何一个字符， 包括某一范围的字符</td>
<td><sup><a href="#fn_abc" id="reffn_abc">abc</a></sup>, <sup><a href="#fn_a-z" id="reffn_a-z">a-z</a></sup></td>
</tr>
<tr>
<td>(…)</td>
<td>匹配封闭括号中的正则表达式，并保存为子组</td>
<td>（[1-3]{2})</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任何数字</td>
<td>\d.txt</td>
</tr>
<tr>
<td>\w</td>
<td>匹配任何数字字母字符（包括_),\W与\w作用相反</td>
<td>\w? </td>
</tr>
<tr>
<td>\s</td>
<td>匹配任何空白符，等价于[\n\s\r\v\f]，\S与\s作用相反</td>
<td>\s?</td>
</tr>
<tr>
<td>\b</td>
<td>匹配单词边界，\B与\b作用相反</td>
<td>\bMonkey\b</td>
</tr>
<tr>
<td>\nn</td>
<td>匹配已保存的子组      </td>
</tr>
<tr>
<td>\c</td>
<td>逐一匹配特殊字符c      </td>
</tr>
<tr>
<td>\A(\Z)</td>
<td>匹配字符串的起始（结束）</td>
<td>\ATest</td>
</tr>
</tbody>
</table>
</div>
<p>我们可以举一些其他常用例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">^[A - Z a - z]+$ 表示由26个字母组成的字符串</span><br><span class="line">^[A - Z a - z 0 - 9]+$ 表示由26个字母和数字组成的字符串</span><br><span class="line">^-?\d+$ 表示整数形式的字符串（有正负）</span><br><span class="line">[1-9]\d&#123;5&#125; 表示中国境内邮政编码，6位</span><br><span class="line">[\u4e00 - \u9fa5] 匹配中文字符</span><br><span class="line">\d&#123;3&#125; - \d&#123;8&#125; | \d&#123;4&#125; - \d&#123;7&#125; 匹配国外电话号码</span><br></pre></td></tr></table></figure>
<p>我们思考一下，我们应该怎么匹配 IP 地址呢？<br>我们知道，IP 地址分为4段，每段取值范围是 0-255，那么我们会考虑到根据这256个数字的特殊性进行划分，比如：<br>1、当取值为 0-99 时，我们可以记为 [1-9] ? \d<br>2、当取值为 100-199 时，我们可以记为 1\d{2}<br>3、当取值为 200-249 时，我们可以记为 2[0-4]\d<br>4、当取值为 250-255 时，我们可以记为 25[0-5]<br>我们只需要将每个区间按 | 预算符进行划分即可获得每段 IP 地址的正确表达式。</p>
<h1 id="Re-库的基本使用"><a href="#Re-库的基本使用" class="headerlink" title="Re 库的基本使用"></a>Re 库的基本使用</h1><p>我们先介绍 Re 库的主要功能函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>re.search()</td>
<td>在一个字符串中搜索匹配正则表达式的第一个位置，返回 match 对象</td>
</tr>
<tr>
<td>re.match()</td>
<td>从一个字符串的开始位置起匹配正则表达式，返回 match 对象</td>
</tr>
<tr>
<td>re.findall()</td>
<td>搜索字符串，以列表类型返回全部能匹配的子串</td>
</tr>
<tr>
<td>re.split()</td>
<td>将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td>
</tr>
<tr>
<td>re.finditer()</td>
<td>搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是 match 对象</td>
</tr>
<tr>
<td>re.sub()</td>
<td>在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td>
</tr>
</tbody>
</table>
</div>
<p>下面一一介绍这六个函数：</p>
<h3 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern, string, flags=0)"></a>re.search(pattern, string, flags=0)</h3><p>其中 pattern 是正则表达式的字符串或原生字符串表示，string 是待匹配字符串，flags 是正则表达式使用时的控制标记。<br>flags 的控制常用标记如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>常用标记</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>re.I re.IGNORECASE</td>
<td>忽略正则表达式的大小写，[A-Z] 能够匹配小写字符</td>
</tr>
<tr>
<td>re.M re.MULTILINE</td>
<td>正则表达式中的 ^ 操作符能够将给定字符串的每行当作匹配开始</td>
</tr>
<tr>
<td>re.S re.DOTAIL</td>
<td>正则表达式中的 . 操作能够匹配所有字符，默认匹配除换行外的所有字符</td>
</tr>
</tbody>
</table>
</div>
<p>下面是一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line">		print(match.group(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="number">100081</span></span><br></pre></td></tr></table></figure>
<h3 id="re-match-pattern-string-flags-0"><a href="#re-match-pattern-string-flags-0" class="headerlink" title="re.match(pattern, string, flags=0)"></a>re.match(pattern, string, flags=0)</h3><p>match 对象的三个参数和标记都与 search 对象相同，我们直接看一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.match(<span class="string">r'[0-9]\d&#123;5&#125;'</span>, <span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line">	match.group(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 无结果</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match.group(<span class="number">0</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;pyshell#15&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    match.group(<span class="number">0</span>)</span><br><span class="line">AttributeError: <span class="string">'NoneType'</span> object has no attribute <span class="string">'group'</span></span><br></pre></td></tr></table></figure>
<p>我们发现上述代码的 match 对象并没有任何结果返回，也就是说这个对象是空的，因为我们知道 match 是从头开始匹配，而字符串的头为 ‘BIT’，自然匹配错误。我们换一下字符串的表示，看看是否能匹配出相关结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.match(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'100081 BIT'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line">	match.group(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'100081'</span></span><br></pre></td></tr></table></figure>
<p>这印证了我们上面的讨论</p>
<h3 id="re-findall-pattern-string-flags-0"><a href="#re-findall-pattern-string-flags-0" class="headerlink" title="re.findall(pattern, string, flags=0)"></a>re.findall(pattern, string, flags=0)</h3><p>我们直接看例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls = re.findall(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls</span><br><span class="line">[<span class="string">'100081'</span>, <span class="string">'100084'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="re-split-pattern-string-maxsplit-0-flags-0"><a href="#re-split-pattern-string-maxsplit-0-flags-0" class="headerlink" title="re.split(pattern, string, maxsplit=0, flags=0)"></a>re.split(pattern, string, maxsplit=0, flags=0)</h3><p>split 函数的三个参数我们都已经熟知，中间新增加的参数 maxsplit 表示最大分割数，剩余部分作为最后一个元素输出，下面看看例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls = re.findall(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls</span><br><span class="line">[<span class="string">'100081'</span>, <span class="string">'100084'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line">[<span class="string">'BIT'</span>, <span class="string">' TSU'</span>, <span class="string">''</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>, maxsplit=<span class="number">1</span>)</span><br><span class="line">[<span class="string">'BIT'</span>, <span class="string">' TSU100084'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="re-finditer-pattern-string-flags-0"><a href="#re-finditer-pattern-string-flags-0" class="headerlink" title="re.finditer(pattern, string, flags=0)"></a>re.finditer(pattern, string, flags=0)</h3><p>我们直接看代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>):</span><br><span class="line">	<span class="keyword">if</span> m:</span><br><span class="line">		print(m.group(<span class="number">0</span>))</span><br><span class="line"><span class="number">100081</span></span><br><span class="line"><span class="number">100084</span></span><br></pre></td></tr></table></figure>
<h3 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern, repl, string, count=0, flags=0)"></a>re.sub(pattern, repl, string, count=0, flags=0)</h3><p>我们可以看到所有参数中新增了两个参数，repl 是指替换匹配字符串的字符串，而 count 是匹配的最大替换次数，下面我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.sub(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'zipcode'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line"><span class="string">'BITzipcode TSUzipcode'</span></span><br></pre></td></tr></table></figure>
<h3 id="Re-库的另一种等价用法"><a href="#Re-库的另一种等价用法" class="headerlink" title="Re 库的另一种等价用法"></a>Re 库的另一种等价用法</h3><p>我们上面给出的函数例子都是函数使用法，也就是仅支持一次性操作的用法，这是什么意思呢，我们对比一下下面的面向对象用法就知道了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pat = re.compile(<span class="string">r'[1-9]\d&#123;5&#125;'</span>)</span><br><span class="line">rst = pat.search(<span class="string">'BIT 100081'</span>)</span><br></pre></td></tr></table></figure>
<p>re.compile 函数的完整形式是：re.compile(pattern, flags=0)，该函数将正则表达式的字符串形式编译成正则表达式对象。编译后才是一个正则表达式，表示一组字符串。</p>
<h1 id="Re-库的-Match-对象"><a href="#Re-库的-Match-对象" class="headerlink" title="Re 库的 Match 对象"></a>Re 库的 Match 对象</h1><p>Match 就是一次匹配的结果，它返回了匹配的相关信息，我们直接看前面的一个代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.match(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'100081 BIT'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> match:</span><br><span class="line">	match.group(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'100081'</span></span><br></pre></td></tr></table></figure>
<p>在这里我们想看一下 match 的类型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(match)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">re</span>.<span class="title">Match</span>'&gt;</span></span><br></pre></td></tr></table></figure>
<p>Match 对象有很多属性，下面我们重点介绍4个属性：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.string</td>
<td>待匹配的文本</td>
</tr>
<tr>
<td>.re</td>
<td>匹配时使用的 pattern 对象（正则表达式）</td>
</tr>
<tr>
<td>.pos</td>
<td>正则表达式搜索文本的开始位置</td>
</tr>
<tr>
<td>.endpos</td>
<td>正则表达式搜索文本的结束位置</td>
</tr>
</tbody>
</table>
</div>
<p>Match 对象有很多方法，下面列举4个常用对象：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.group(0)</td>
<td>获得匹配后的字符串</td>
</tr>
<tr>
<td>.start()</td>
<td>匹配字符串在原始字符串的开始位置</td>
</tr>
<tr>
<td>.end()</td>
<td>匹配字符串在原始字符串的结束位置</td>
</tr>
<tr>
<td>.span()</td>
<td>返回 (.start(), .end())</td>
</tr>
</tbody>
</table>
</div>
<p>我们看一下相关例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.string</span><br><span class="line"><span class="string">'BIT100081 TSU100084'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.re</span><br><span class="line">re.compile(<span class="string">'[1-9]\\d&#123;5&#125;'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.pos</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.endpos</span><br><span class="line"><span class="number">19</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'100081'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.start()</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.end()</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span()</span><br><span class="line">(<span class="number">3</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Re-库的贪婪匹配和最小匹配"><a href="#Re-库的贪婪匹配和最小匹配" class="headerlink" title="Re 库的贪婪匹配和最小匹配"></a>Re 库的贪婪匹配和最小匹配</h1><p>首先我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'PY.*N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'PYANBNCNDN'</span></span><br></pre></td></tr></table></figure>
<p>其中的 ‘.*‘ 表示匹配任意字符串，这里我们应该留意到， Re 库默认采用贪婪匹配，即输出匹配最长的子串。那么我们应该如何实现输出最短的子串呢？<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>match = re.search(<span class="string">r'PY.*?N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>match.group(<span class="number">0</span>)</span><br><span class="line"><span class="string">'PYAN'</span></span><br></pre></td></tr></table></figure></p>
<p>最小匹配操作符有下面四种：<br>操作符 | 说明<br>———|———<br>*? | 前一个字符0次或无限次扩展<br>+? | 前一个字符1次或无限次扩展<br>?? | 前一个字符0次或1次扩展<br>{m,n}? | 扩展前一个字符 m 至 n 次（含 n）</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（五）—— 中国大学排名定向爬虫</title>
    <url>/posts/f995f5ca.html</url>
    <content><![CDATA[<h1 id="实例介绍"><a href="#实例介绍" class="headerlink" title="实例介绍"></a>实例介绍</h1><p>我们准备从上海交大设计的最好大学网获得大学的排名，由下面链接打开就能直接看到中国最好大学的基本信息：<br><a href="http://www.zuihaodaxue.com/zuihaodaxuepaiming2016.html" target="_blank" rel="noopener">http://www.zuihaodaxue.com/zuihaodaxuepaiming2016.html</a></p>
<p>我们要写一段程序，从网上获得大学的排名，然后以此输出，具体的功能描述就是：</p>
<blockquote>
<p>输入：大学排名 URL 链接<br>输出： 大学排名信息的屏幕输出（排名， 大学名称， 总分）<br>技术路线： requests-bs4<br>定向爬虫： 仅对输入 URL 进行爬取，不扩展爬取</p>
</blockquote>
<p>我们在该网站首页观察到的是下图：<br><img src="https://img-blog.csdnimg.cn/2020042018273636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="中国大学排名">我们首先看看根据我们目前掌握的知识能否进行爬取，右键点开源代码，发现相应内容可以在源代码中找到，说明这些内容不是由一个动态脚本控制 的，这个定向爬虫是我们可以实现的：<br><img src="https://img-blog.csdnimg.cn/20200420182907319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="中国大学排名网站源代码">此外，我们还需看一下这个网站是否提供了 robots 协议的约定，我们直接打开 www.zuihaodaxue.cn/robots.cn ，我们发现网页不存在，因此我们是可以对该网站进行爬取的。<br>验证可行性之后，我们需要首先对爬虫做一个初步的设计，获取大学排名并且输出大学排名信息：</p>
<blockquote>
<p>步骤一：从网络上获取大学排名网页内容<br>步骤二：提取网页内容中信息到合适的数据结构<br>步骤三：利用数据结构输出其中的信息并且获得我们需要的结果</p>
</blockquote>
<p>对应上述步骤，我们可以提取出具体的程序结构设计：</p>
<blockquote>
<p>步骤一：getHTMLText()<br>步骤二：fillUnivList()<br>步骤三：printUnivList()</p>
</blockquote>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p>下面我们直接展示代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">"tbody"</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">"td"</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">2</span>].string])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span> </span><br><span class="line">    print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(<span class="string">"排名"</span>,<span class="string">"学校"</span>,<span class="string">"总分"</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(u[<span class="number">0</span>],u[<span class="number">1</span>],u[<span class="number">2</span>]))</span><br><span class="line">    print(<span class="string">"Suc"</span> + str(num)) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    uinfo = []</span><br><span class="line">    url = <span class="string">"http://www.zuihaodaxue.com/zuihaodaxuepaiming2016.html"</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(uinfo, html)</span><br><span class="line">    printUnivList(uinfo, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<p>最后输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">排名    	  学校  	    总分</span><br><span class="line">    1     	 清华大学 	   北京市</span><br><span class="line">    2     	 北京大学 	   北京市</span><br><span class="line">    3     	 浙江大学 	   浙江省</span><br><span class="line">    4     	上海交通大学	   上海市</span><br><span class="line">    5     	 复旦大学 	   上海市</span><br><span class="line">    6     	 南京大学 	   江苏省</span><br><span class="line">    7     	中国科学技术大学	   安徽省</span><br><span class="line">    8     	哈尔滨工业大学	   黑龙江省</span><br><span class="line">    9     	华中科技大学	   湖北省</span><br><span class="line">    10    	 中山大学 	   广东省</span><br><span class="line">    11    	 东南大学 	   江苏省</span><br><span class="line">    12    	 天津大学 	   天津市</span><br><span class="line">    13    	 同济大学 	   上海市</span><br><span class="line">    14    	北京航空航天大学	   北京市</span><br><span class="line">    15    	 四川大学 	   四川省</span><br><span class="line">    16    	 武汉大学 	   湖北省</span><br><span class="line">    17    	西安交通大学	   陕西省</span><br><span class="line">    18    	 南开大学 	   天津市</span><br><span class="line">    19    	大连理工大学	   辽宁省</span><br><span class="line">    20    	 山东大学 	   山东省</span><br><span class="line">Suc20</span><br></pre></td></tr></table></figure>
<p>接下来我们将对上述显示结果进行优化。</p>
<h1 id="实例优化"><a href="#实例优化" class="headerlink" title="实例优化"></a>实例优化</h1><p>我们发现上述显示结果并不美观，没有按我们想象中的居中对齐，这是由于中西文的空格填充方式不同，要怎么解决这个问题呢？我们可以采用中文字符的空格填充，即 chr(12288)，我们将上述代码中的 printUnivList 函数进行如下修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span> </span><br><span class="line">    tplt = <span class="string">"&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">"排名"</span>,<span class="string">"学校"</span>,<span class="string">"总分"</span>,chr(<span class="number">12288</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(tplt.format(u[<span class="number">0</span>],u[<span class="number">1</span>],u[<span class="number">2</span>], chr(<span class="number">12288</span>)))</span><br><span class="line">    print(<span class="string">"Suc"</span> + str(num))</span><br></pre></td></tr></table></figure>
<p>输出结果为：<br><img src="https://img-blog.csdnimg.cn/20200420212749712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="输出结果"></p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（四）—— 信息组织与提取方法</title>
    <url>/posts/85092151.html</url>
    <content><![CDATA[<h1 id="信息标记的三种形式"><a href="#信息标记的三种形式" class="headerlink" title="信息标记的三种形式"></a>信息标记的三种形式</h1><blockquote>
<p>信息的标记：<br>标记后的信息可形成信息组织结构，增加信息维度<br>标记后的信息可用于通信、存储或展示<br>标记的结构和信息一样具有重要价值<br>标记后的信息更利于程序理解和运用</p>
</blockquote>
<p>国际公认的信息标记的三种形式分别是 XML、JSON、YAML，下面分别介绍这三者：<br><strong>XML 即 eXtensible Markup Language</strong>，采用了以标签为主来构建信息和表达信息的方式，比如：<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"china.jpg size="</span><span class="attr">10</span>"&gt;</span> ... <span class="tag">&lt;/<span class="name">img</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"china.jpg size="</span><span class="attr">10</span>" /&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- This is a comment --&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>我们不难发现，XML 的形式与 HTML 几乎完全一致，可以说 XML 是通过 HTML发展而来的通用表达形式。<br>另一种信息标记形式是 <strong>JSON (JavaScript Object Notation)</strong>，它是 JavaScript 语言中对面向对象的信息的一种表达形式，简单来讲，JSON 是由有类型的键值对构建的信息表达方式，具体形式如下：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"key1":"value1"</span><br><span class="line">"key2":["value2", "value3"]</span><br><span class="line">"key3":&#123;"subkey":"subvalue"&#125;</span><br></pre></td></tr></table></figure>
<p><strong>YAML(YAML Ain’t Markup Language)</strong> 是一种递归的定义，采用的是五类型的键值对来组织信息，和 Python 语言类似，YAML 通过缩进的方式表达所属关系比如：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span>	<span class="comment"># 用 tab 表示所属关系</span></span><br><span class="line">	<span class="attr">newName :</span> <span class="string">abc</span></span><br><span class="line">	<span class="attr">oldName :</span> <span class="string">ABC</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span>	<span class="comment"># 用-表示并列关系</span></span><br><span class="line">	<span class="string">-abc</span></span><br><span class="line">	<span class="string">-ABC</span></span><br><span class="line"></span><br><span class="line"><span class="attr">text:</span> <span class="string">|</span>	<span class="comment"># 用 | 表示整块数据</span></span><br><span class="line">	<span class="string">简介：汉武帝刘彻（前156年7月7日—前87年3月29日），</span></span><br><span class="line">	<span class="string">西汉第七位皇帝（含前后少帝），政治家、文学家。</span></span><br><span class="line">	<span class="string">汉武帝在位期间（前141年—前87年），在政治上，创设中外朝制、刺史制、察举制，</span></span><br><span class="line">	<span class="string">颁行推恩令，加强君主专制与中央集权。</span></span><br></pre></td></tr></table></figure>
<h1 id="三种信息标记形式的比较"><a href="#三种信息标记形式的比较" class="headerlink" title="三种信息标记形式的比较"></a>三种信息标记形式的比较</h1><div class="table-container">
<table>
<thead>
<tr>
<th>信息标记形式</th>
<th>特点</th>
<th>应用情况</th>
</tr>
</thead>
<tbody>
<tr>
<td>XML</td>
<td>是最早的通用信息标记语言，可扩展性好，但较为繁琐</td>
<td>Internet 上的信息交互与传递</td>
</tr>
<tr>
<td>JSON</td>
<td>信息有类型，适合程序处理，比 XML 更简洁</td>
<td>移动应用云端和节点的信息通信，无注释</td>
</tr>
<tr>
<td>YAML</td>
<td>信息无类型，文本信息比例最高，可读性好</td>
<td>各类系统的配置文件，有注释易读，应用相对较广泛</td>
</tr>
</tbody>
</table>
</div>
<h1 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h1><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>例子</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>完整解析信息标记形式，再提取关键信息，需要标记解析器</td>
<td>bs4 库的标签树遍历</td>
<td>信息解析准确</td>
<td>提取过程繁琐、速度慢</td>
</tr>
<tr>
<td>无视标记形式，直接搜索关键信息</td>
<td>Word 对信息的文本查找函数</td>
<td>提取过程简介，速度较快</td>
<td>提取结果准确性与信息内容相关</td>
</tr>
</tbody>
</table>
</div>
<p>实际上更常用的是两者结合的融合方法：即结合形式解析与搜索方法，提取关键信息，这需要标记解析器及文本查找函数。下面我们考虑实例：提取 HTML 中所有的 URL 链接，具体思路是：</p>
<blockquote>
<p>  1） 搜索所有 \<a> 标签<br>  2） 解析 \<a> 标签格式，提取 href 后的链接内容</a></a></p>
</blockquote>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">"http://python123.io/ws/demo.html"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = requests.get(url)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo = res.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo</span><br><span class="line"><span class="string">'&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;\r\n&lt;body&gt;\r\n&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;\r\n&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n&lt;a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;\r\n&lt;/body&gt;&lt;/html&gt;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(<span class="string">"a"</span>):</span><br><span class="line">	print(link.get(<span class="string">"href"</span>))</span><br><span class="line"></span><br><span class="line">http://www.icourse163.org/course/BIT<span class="number">-268001</span></span><br><span class="line">http://www.icourse163.org/course/BIT<span class="number">-1001870001</span></span><br></pre></td></tr></table></figure>
<h1 id="基于-BeautifulSoup-库的内容查找方法"><a href="#基于-BeautifulSoup-库的内容查找方法" class="headerlink" title="基于 BeautifulSoup 库的内容查找方法"></a>基于 BeautifulSoup 库的内容查找方法</h1><p>由前例我们可以看见， find_all 方法可以返回一个列表类型，存储查找的结果，find_all 的具体使用方法为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;&gt;.find_all(name, attrs, recursive, string, **kwargs)</span><br></pre></td></tr></table></figure>
<p>其中，name 为对标签的检索字符串，如果我们希望查找两个标签，可以以列表形式传入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">"a"</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all([<span class="string">"a"</span>,<span class="string">"b"</span>])</span><br><span class="line">[&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;, &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(true):</span><br><span class="line">		print(tag.name)</span><br><span class="line">	</span><br><span class="line">html</span><br><span class="line">head</span><br><span class="line">title</span><br><span class="line">body</span><br><span class="line">p</span><br><span class="line">b</span><br><span class="line">p</span><br><span class="line">a</span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<p>我们看到，若 find_all(true) ，则会返回所有标签。若希望只显示所有以 b 开头的标签，包括 b 和 body 标签，我们应该怎么办呢？这时我们应该使用正则表达式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">'b'</span>)):</span><br><span class="line">	print(tag.name)</span><br><span class="line">	</span><br><span class="line">body</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<p>find_all 的第二个参数 attrs 是对标签属性值的检索字符串，可标注属性检索。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">"p"</span>, <span class="string">"course"</span>)</span><br><span class="line">[&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line">&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=<span class="string">"link1"</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=<span class="string">"link"</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(id=re.compile(<span class="string">"link"</span>))</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br></pre></td></tr></table></figure>
<p>find_all 的第三个参数是 recursive，即是否对子孙进行全部检索，默认为 True。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">"a"</span>)</span><br><span class="line">[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">"a"</span>,recursive=<span class="literal">False</span>)</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<p>上述代码说明，soup 的根节点下是没有 a 标签的， a 标签在其子孙节点中。<br>find_all 的第四个参数是 string，是对 <>...</> 中字符串区域的检索字符串，下面看一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(string=<span class="string">"Basic Python"</span>)</span><br><span class="line">[<span class="string">'Basic Python'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(string=<span class="string">"Python"</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(string=re.compile(<span class="string">"Python"</span>))</span><br><span class="line">[<span class="string">'Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n'</span>, <span class="string">'Basic Python'</span>, <span class="string">'Advanced Python'</span>]</span><br></pre></td></tr></table></figure>
<p>事实上，我们有一个简写的形式，即：</p>
<blockquote>
<p>\<tag>(…)  等价于 \<tag>.find_all(…)<br>soup(…) 等价于 soup.find_all(…)</tag></tag></p>
</blockquote>
<p>BeautifulSoup 库有八个常用方法，都是顾名思义的，就不在此做详细介绍了，方法列举为：find_all, find, find_parent, find_parents, find_next_siblings, find_next_sibling, find_previous_sibling, find_previous_siblings </p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（三）—— Beautiful Soup 库入门</title>
    <url>/posts/caae3505.html</url>
    <content><![CDATA[<h1 id="BeautifulSoup-库的安装"><a href="#BeautifulSoup-库的安装" class="headerlink" title="BeautifulSoup 库的安装"></a>BeautifulSoup 库的安装</h1><p>安装beautiful soup 库可以直接使用命令 pip install beautifulsoup4，安装完成之后可以通过演示 HTML 页面地址：<a href="http://python123.io/ws/demo.html" target="_blank" rel="noopener">http://python123.io/ws/demo.html</a> 进行测试。我们打开这个网址，查询源代码，得到下面的结果：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a python demo page<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span><span class="tag">&lt;<span class="name">b</span>&gt;</span>The demo python introduces several python courses.<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"course"</span>&gt;</span>Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-268001"</span> <span class="attr">class</span>=<span class="string">"py1"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span>Basic Python<span class="tag">&lt;/<span class="name">a</span>&gt;</span> and <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-1001870001"</span> <span class="attr">class</span>=<span class="string">"py2"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span>Advanced Python<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>我们刚刚查看源代码是直接右键查看源代码进行拷贝的，除此之外，我们还可以使用 requests 库自动获得该链接的源代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>)</span><br><span class="line">demo = r.text</span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line">print(soup.prettify())</span><br></pre></td></tr></table></figure>
<p>这是上述代码的返回结果：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br><span class="line">   This is a python demo page</span><br><span class="line">  <span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">b</span>&gt;</span></span><br><span class="line">    The demo python introduces several python courses.</span><br><span class="line">   <span class="tag">&lt;/<span class="name">b</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"course"</span>&gt;</span></span><br><span class="line">   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"py1"</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-268001"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span></span><br><span class="line">    Basic Python</span><br><span class="line">   <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   and</span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"py2"</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-1001870001"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span></span><br><span class="line">    Advanced Python</span><br><span class="line">   <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>看得出来，这个效果还是蛮好看的。实际上，Beautiful Soup 库是解析、遍历和维护“标签树”的功能库，可以理解为：BeautifulSoup 对应一个 HTML/XML 文档的全部内容。</p>
<h1 id="BeautifulSoup-库详解"><a href="#BeautifulSoup-库详解" class="headerlink" title="BeautifulSoup 库详解"></a>BeautifulSoup 库详解</h1><p>BeautifulSoup 库的解析器一共有四种，分别列举如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>条件</th>
</tr>
</thead>
<tbody>
<tr>
<td>bs4 的 HTML 解析器</td>
<td>BeautifulSoup(mk, “html.parser”)</td>
<td>pip install bs4</td>
</tr>
</tbody>
</table>
</div>
<p>lxml 的 HTML 解析器|BeautifulSoup(mk, “lxml”)|pip install lxml|<br>|lxml 的 XML 解析器|BeautifulSoup(mk, “xml”)|pip install lxml|<br>|html5lib 的解析器|BeautifulSoup(mk, “html5lib”)|pip install html5lib|</p>
<p>BeautifulSoup 类的基本元素列举如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基本元素</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tag</td>
<td>标签，最基本的信息组织单元，分别用 <> 和 </>标明开头和结尾</td>
</tr>
<tr>
<td>Name</td>
<td>标签的名字，获取格式：\<tag>.name</tag></td>
</tr>
<tr>
<td>Attributes</td>
<td>标签的属性，获取格式：\<tag>.attrs</tag></td>
</tr>
<tr>
<td>NavigableString</td>
<td>标签内非属性字符串，获取格式：\<tag>.string</tag></td>
</tr>
<tr>
<td>Comment</td>
<td>标签内字符串的注释部分，一种特殊的Comment类型</td>
</tr>
</tbody>
</table>
</div>
<p>我们可以对前面获得的 soup 进行 尝试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.title)</span><br><span class="line"><span class="comment"># 返回 &lt;title&gt;This is a python demo page&lt;/title&gt;</span></span><br><span class="line">tag = soup.a</span><br><span class="line">print(tag)</span><br><span class="line"><span class="comment"># 返回 &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; </span></span><br><span class="line">print(soup.a.parent.name)	<span class="comment"># 返回 'p'</span></span><br><span class="line">print(soup.a.parent.parent.name)	<span class="comment"># 返回 'body'</span></span><br><span class="line">print(tag.attrs)</span><br><span class="line"><span class="comment"># 返回 &#123;'href': 'http://www.icourse163.org/course/BIT-268001', 'class': ['py1'], 'id': 'link1'&#125;</span></span><br><span class="line">prin(soup.a.string)	<span class="comment"># 返回 Basic Python</span></span><br></pre></td></tr></table></figure>
<p>我们需要注意的是，当我们的 HTML 存在多个相同的标签时，只会返回第一个。我们再来看一下 Comment 元素的使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">newsoup = Beautiful(<span class="string">"&lt;b&gt;&lt;!--This is a comment--&gt;&lt;/b&gt;&lt;p&gt;This is not a comment&lt;/p&gt;"</span>, <span class="string">"html.parser"</span>)</span><br><span class="line">print(newsoup.b.string) <span class="comment"># 返回 'This is a comment'</span></span><br><span class="line">print(newsoup.p.string)	<span class="comment"># 返回 'This is not a comment'</span></span><br></pre></td></tr></table></figure>
<p>我们总结一下，对于 &lt; p class=”title”&gt; … \&lt;/p&gt;，若要得到 p ，我们使用： <strong>.name</strong>，若要得到 class=”title”，我们使用：<strong>属性.attrs</strong>，若要得到 title，我们使用<strong>标签.\<tag> </tag></strong>，若要得到…(文本)，我们使用：<strong>非属性字符串/注释.string</strong></p>
<h1 id="基于-bs4-库的-HTML-内容遍历方法"><a href="#基于-bs4-库的-HTML-内容遍历方法" class="headerlink" title="基于 bs4 库的 HTML 内容遍历方法"></a>基于 bs4 库的 HTML 内容遍历方法</h1><p>在 HTML 中我们有三种遍历方式 —— 上行遍历、下行遍历和平行遍历，下面分别介绍这几种遍历的直接方法。</p>
<h3 id="标签树的下行遍历"><a href="#标签树的下行遍历" class="headerlink" title="标签树的下行遍历"></a>标签树的下行遍历</h3><div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.contents</td>
<td>子节点的列表，将 \<tag> 所有儿子节点存入列表</tag></td>
</tr>
<tr>
<td>.children</td>
<td>子节点的迭代类型，与 .contents 类似，用于循环遍历儿子节点</td>
</tr>
<tr>
<td>.descendants</td>
<td>子孙节点的迭代类型，包括所有子孙节点，用于循环遍历</td>
</tr>
</tbody>
</table>
</div>
<p>例如：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line">&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.head</span><br><span class="line">&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;soup.head.contents</span><br><span class="line">[&lt;title&gt;This is a python demo page&lt;/title&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.body.contents</span><br><span class="line">['\n', &lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;, '\n', &lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line"></span><br><span class="line">&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;, '\n']</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(soup.body.contents)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.body.contents[<span class="number">1</span>]</span><br><span class="line">&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>
<h3 id="标签树的上行遍历"><a href="#标签树的上行遍历" class="headerlink" title="标签树的上行遍历"></a>标签树的上行遍历</h3><div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.parent</td>
<td>节点的父亲标签</td>
</tr>
<tr>
<td>.parents</td>
<td>节点父辈标签的迭代类型，用于循环遍历先辈节点</td>
</tr>
</tbody>
</table>
</div>
<p>下面是上行遍历的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">		<span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">			print(parent)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			print(parent.name)</span><br><span class="line">			</span><br><span class="line">p</span><br><span class="line">body</span><br><span class="line">html</span><br><span class="line">[document]</span><br></pre></td></tr></table></figure>
<h3 id="标签树的平行遍历"><a href="#标签树的平行遍历" class="headerlink" title="标签树的平行遍历"></a>标签树的平行遍历</h3><div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>.next_sibling</td>
<td>返回按照 HTML 文本顺序的下一个平行节点标签</td>
</tr>
<tr>
<td>.previous_sibling</td>
<td>返回按照 HTML 文本顺序的上一个平行节点标签</td>
</tr>
<tr>
<td>.next_siblings</td>
<td>返回按照 HTML 文本顺序的后续所有平行节点标签</td>
</tr>
<tr>
<td>.previous_siblings</td>
<td>返回按照 HTML 文本顺序的前续所有平行节点标签</td>
</tr>
</tbody>
</table>
</div>
<p>需要注意的是：平行遍历发生在同一个父节点下的各节点间。<br>还记得我们前面的那个标签树吗？我们截取一部分来回顾一下：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"course"</span>&gt;</span></span><br><span class="line">   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:</span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"py1"</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-268001"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span></span><br><span class="line">    Basic Python</span><br><span class="line">   <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   and</span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"py2"</span> <span class="attr">href</span>=<span class="string">"http://www.icourse163.org/course/BIT-1001870001"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>我们接下来对 a 标签进行简单的平行遍历尝试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.a.next_sibling</span><br><span class="line"><span class="string">' and '</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.a.next_sibling.next_sibling</span><br><span class="line">&lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.a.previous_sibling</span><br><span class="line"><span class="string">'Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n'</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过以下代码做标签树的平行遍历：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_siblings:</span><br><span class="line">	print(sibling)</span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_siblings:</span><br><span class="line">	print(sibling)</span><br></pre></td></tr></table></figure>
<h1 id="基于-bs4-库的-HTML-格式输出"><a href="#基于-bs4-库的-HTML-格式输出" class="headerlink" title="基于 bs4 库的 HTML 格式输出"></a>基于 bs4 库的 HTML 格式输出</h1><p>我们应该如何让 \<html> 内容更加友好地输出呢？就是我们前面使用的 BeautifulSoup.prettify() 函数来实现。</html></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(<span class="string">"&lt;p&gt; 中文 china &lt;/p&gt;"</span>,<span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.prettify()</span><br><span class="line"><span class="string">'&lt;p&gt;\n 中文 china\n&lt;/p&gt;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(soup.prettify())</span><br><span class="line">&lt;p&gt;</span><br><span class="line"> 中文 china</span><br><span class="line">&lt;/p&gt;</span><br></pre></td></tr></table></figure>
<p>下面附上一张总结（截）图：<br><img src="https://img-blog.csdnimg.cn/20200420105036148.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="总结图"></p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（二）—— Requests 网络爬虫实战</title>
    <url>/posts/e8ecbe5c.html</url>
    <content><![CDATA[<p>前面我们讲了网络爬虫常用库——Requests，下面我们直接通过几个实例实现网络爬虫：</p>
<h1 id="实例一：京东商品页面的爬取"><a href="#实例一：京东商品页面的爬取" class="headerlink" title="实例一：京东商品页面的爬取"></a>实例一：京东商品页面的爬取</h1><p>首先我们打开京东页面选择商品：<br><a href="https://item.jd.com/100008348530.html" target="_blank" rel="noopener">https://item.jd.com/100008348530.html</a> ，我们要做的事情是通过网络爬虫获取该商品的有关信息，该页面内容如下：<br><img src="https://img-blog.csdnimg.cn/20200419165244644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="京东商品 Apple iphone 11"> 下面我们对网页进行简单爬取测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">"https://item.jd.com/100008348530.html"</span>)</span><br><span class="line">print(r.statue_code) 		<span class="comment"># 返回200</span></span><br><span class="line">print(r.encoding)     		<span class="comment"># 返回'gbk'</span></span><br><span class="line">r.text[:<span class="number">1000</span>]				<span class="comment"># 返回了正确内容</span></span><br></pre></td></tr></table></figure>
<p>测试基本正常，我们按照前面的文章所说的爬虫通用框架对网页进行爬取：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">"https://item.jd.com/100008348530.html"</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    r = requests.get(url)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding = r.apparent_encoding</span><br><span class="line">    print(r.text)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="实例二：亚马逊商品页面的爬取"><a href="#实例二：亚马逊商品页面的爬取" class="headerlink" title="实例二：亚马逊商品页面的爬取"></a>实例二：亚马逊商品页面的爬取</h1><p>我们首先访问页面 <a href="https://www.amazon.cn/dp/B0785D5L1H/ref=sr_1_1" target="_blank" rel="noopener">https://www.amazon.cn/dp/B0785D5L1H/ref=sr_1_1</a> ，下面直接通过代码实现对商品信息的爬取：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    r = requests.get(<span class="string">"https://www.amazon.cn/dp/B0785D5L1H/ref=sr_1_1"</span>)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding = apparent_encoding</span><br><span class="line">    print(r.text[:<span class="number">100</span>])</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取错误"</span>)</span><br><span class="line">    print(r.status_code)</span><br><span class="line"><span class="comment"># 返回 503</span></span><br></pre></td></tr></table></figure>
<p>由此例我们可见，亚马逊对我们的爬虫有限制，我们输入以下命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r.request.headers</span><br><span class="line"><span class="comment"># &#123;'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'&#125;</span></span><br></pre></td></tr></table></figure>
<p>我们发现在 ‘User-Agent’ 字段下，我们的爬虫忠实地告诉了亚马逊地服务器，这次访问是由 python 的 requests 库发起的，因此我们需要模拟浏览器的请求：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">"https://www.amazon.cn/dp/B0785D5L1H/ref=sr_1_1"</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	kv = &#123;<span class="string">'user-agent'</span>:<span class="string">'Mozilla/5.0'</span>&#125;</span><br><span class="line">	r = requests.get(url, headers=kv)</span><br><span class="line">	r.raise_for_status()</span><br><span class="line">	r.encoding = r.apparent_encoding</span><br><span class="line">	print(r.text[<span class="number">1000</span>:<span class="number">2000</span>])</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>
<p>此时已成功爬取到数据</p>
<h1 id="实例三：百度-360搜索关键词提交"><a href="#实例三：百度-360搜索关键词提交" class="headerlink" title="实例三：百度/360搜索关键词提交"></a>实例三：百度/360搜索关键词提交</h1><p>首先我们观察一下百度和360的搜索接口：</p>
<blockquote>
<p>百度的关键词接口： <a href="http://www.baidu.com/s?wd=keyword" target="_blank" rel="noopener">http://www.baidu.com/s?wd=keyword</a><br>360的关键词接口：<a href="http://www.so.com/s?q=keyword" target="_blank" rel="noopener">http://www.so.com/s?q=keyword</a></p>
</blockquote>
<p>也就是在这两个接口中，我们只需要将关键词代入 keyword，即可实现提交关键词了，下面我们用 requests 实现相关代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">kv = &#123;<span class="string">"wd"</span>:<span class="string">"python"</span>&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://www.baidu.com/s"</span>,params=kv)</span><br><span class="line">print(r.status_code)</span><br><span class="line">print(r.request.url)</span><br></pre></td></tr></table></figure></p>
<p>若无意外状态码返回值为200，而后面的 url 返回的是一个百度安全验证的东西，360搜索的返回值则正常。</p>
<h1 id="实例四：网络图片的爬取和存储"><a href="#实例四：网络图片的爬取和存储" class="headerlink" title="实例四：网络图片的爬取和存储"></a>实例四：网络图片的爬取和存储</h1><p>首先我们看一下网络图片链接的格式：</p>
<blockquote>
<p><a href="http://www.example.com/picture.jpg" target="_blank" rel="noopener">http://www.example.com/picture.jpg</a></p>
</blockquote>
<p>那么我们应该怎样进行图片存储呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">root = <span class="string">"D://Temp//Python//Spider//Spider_lecture//Real_test//"</span></span><br><span class="line">url = <span class="string">"http://img0.dili360.com/pic/2019/05/13/5cd93370871d19s75711409_t.jpg"</span></span><br><span class="line">path = root + url.split(<span class="string">"/"</span>)[<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(root):</span><br><span class="line">        os.mkdir(root)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(r.content)</span><br><span class="line">            f.close()</span><br><span class="line">            print(<span class="string">"文件保存成功 "</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"文件已存在"</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="实例五：IP地址归属地的自动查询"><a href="#实例五：IP地址归属地的自动查询" class="headerlink" title="实例五：IP地址归属地的自动查询"></a>实例五：IP地址归属地的自动查询</h1><p>当给出一个 IP 地址，我们要怎么判断这个地址是在北京呢，还是在广州呢？事实上，我们有一个网站可以查询相关数据：www.ip138.com ，下图即为该网站的界面：<br><img src="https://img-blog.csdnimg.cn/20200419224808694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoZW5LYWlfMTY0,size_16,color_FFFFFF,t_70#pic_center" alt="IP地址查询网站界面">那么我们应该怎样通过爬虫进行自动实现呢？事实上，我们通过输入不同的 URL 链接可以发现这样一个规律，即该网站的 URL 组成为：</p>
<blockquote>
<p><a href="http://www.ip138.com/iplookup.asp?ip=**ipaddress**&amp;action=2" target="_blank" rel="noopener">http://www.ip138.com/iplookup.asp?ip=**ipaddress**&amp;action=2</a></p>
</blockquote>
<p>其中 ipaddress 即为用户输入的 IP 地址，我们通过以下代码尝试爬取成功：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">"https://www.ip138.com/iplookup.asp?ip="</span></span><br><span class="line">header=&#123;<span class="string">'user-agent'</span>:<span class="string">'Mozilla/5.0'</span>&#125;</span><br><span class="line">url_back = <span class="string">"&amp;action=2"</span></span><br><span class="line">r = requests.get(url+<span class="string">"202.204.80.112"</span>+url_back, headers = header,timeout=<span class="number">7</span>)</span><br><span class="line">print(r.status_code)</span><br><span class="line">r.encoding = r.apparent_encoding</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫初级（一）—— Requests 库入门</title>
    <url>/posts/a8332004.html</url>
    <content><![CDATA[<h1 id="requests-模块的导入"><a href="#requests-模块的导入" class="headerlink" title="requests 模块的导入"></a>requests 模块的导入</h1><p>request 函数的导入可以直接使用 import requests 来实现，当然，若事先没有安装可以直接在命令行输入 pip install reqeusts 来进行安装。<br>requests 模块中包含了七个主要的方法，下面将进行一一解析和尝试调用。</p>
<h1 id="requests-get-函数"><a href="#requests-get-函数" class="headerlink" title="requests.get() 函数"></a>requests.get() 函数</h1><p>requests.get() 函数是一个用于向服务器构造请求资源的 Requests 对象，具体实例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = www.baidu.com</span><br><span class="line">r = requests.get(url)</span><br><span class="line">print(r.status_code)	<span class="comment"># 返回200表示爬取成功</span></span><br><span class="line">r.encoding = <span class="string">"utf-8"</span>	<span class="comment"># 转换编码</span></span><br><span class="line">print(r.text)			<span class="comment"># 打印对应的HTML文本</span></span><br><span class="line">print(type(r))			<span class="comment"># 应返回 &lt;class 'requests.model.Response'&gt;</span></span><br><span class="line">print(r.headers)		<span class="comment"># 应返回&#123;'Cache-Control': 'private （省略）</span></span><br></pre></td></tr></table></figure>
<p>其中 r 是一个 response 对象，即存储了服务器返回本地的数据，我们对数据的操作即对 r 的操作，response 对象有五个属性，这五个属性是爬虫处理数据的重中之重，其具体含义如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>r.status_code</td>
<td>HTTP请求的返回状态，202表示连接成功，404表示失败</td>
</tr>
<tr>
<td>r.text</td>
<td>HTTP相应内容的字符串形式，即 url 对应的页面内容</td>
</tr>
<tr>
<td>r.encoding</td>
<td>从HTTP header 中猜测的相应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的相应内容编码方式（备选编码方式）</td>
</tr>
<tr>
<td>r.content</td>
<td>HTTP相应内容的二进制形式</td>
</tr>
</tbody>
</table>
</div>
<p>在我们使用 get 方法从网上获取资源时有基本流程如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A((r.status_code)) -- 返回值为 200 --&gt; B[可以使用 r.text 和 r.encoding 等方法]</span><br><span class="line">A -- 返回值为 404 或其他--&gt; C[某些原因出错将产生异常]</span><br></pre></td></tr></table></figure>
<p>我们要怎么区分 response 的两种编码方式呢？我们来看一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">print(r.status_code)			<span class="comment"># 返回200表示正常运行,可以继续</span></span><br><span class="line">print(r.text)					<span class="comment"># 显示的内容很多是乱码看不清编码是什么</span></span><br><span class="line">print(r.encoding)				<span class="comment"># 返回 'ISO-8859-1'</span></span><br><span class="line">print(r.apparent_encoding)		<span class="comment"># 返回 'utf-8'</span></span><br><span class="line">r.encoding = <span class="string">'utf-8'</span></span><br><span class="line">print(r.text)					<span class="comment">#返回了正确信息</span></span><br></pre></td></tr></table></figure>
<p>事实上，r.encoding 中的编码方式是从 HTTP 的 header 中的 charset 字段中获得，如果 HTTP 的 header 中有这个字段，说明我们访问的服务器对其资源的编码是有要求的，编码获得后存在 encoding 中，但若 HTTP 的 header中不含有此字段，将默认编码为 ‘ISO-8859-1’，但此编码并不能解析中文。而 apparent_encoding 则是从 HTTP 内容部分分析出可能的编码形式，原则来说此方法会更准确。</p>
<h1 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h1><p>在实际爬取过程中，requests.get(url) 并不是时时通用的，经常会遇到各种问题，Requests 库支持六种常用的连接异常：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>异常</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>requests.ConnectionError</td>
<td>网络连接错误异常，如 DNS 查询失败，拒绝连接等</td>
</tr>
<tr>
<td>requests.HTTPError</td>
<td>HTTP 错误异常</td>
</tr>
<tr>
<td>requests.URL.Required</td>
<td>URL 缺失异常</td>
</tr>
<tr>
<td>requests.TooManyRedirects</td>
<td>超过最大重定向次数，产生重定向异常</td>
</tr>
<tr>
<td>requests.ConnectTimeout</td>
<td>连接远程服务器超时异常</td>
</tr>
<tr>
<td>requests.Timeout</td>
<td>请求 URL 超时，产生超时异常）</td>
</tr>
</tbody>
</table>
</div>
<p>Response 的异常：<br>异常 | 说明<br>———— | ——-<br>r.raise_for_status() | 如果不是200，产生异常 requests.HTTPError<br>这个异常有什么用呢，我们来看一下爬取网页的通用代码框架就一目了然了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">geetHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">		r.raise_for_status() </span><br><span class="line">		r.encoding = r.apparent_encoding</span><br><span class="line">		<span class="keyword">return</span> r.text</span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">"产生异常"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">	url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">	print(geteHTMLText(url))</span><br></pre></td></tr></table></figure>
<h1 id="HTTP-协议与-Requests-库的主要方法"><a href="#HTTP-协议与-Requests-库的主要方法" class="headerlink" title="HTTP 协议与 Requests 库的主要方法"></a>HTTP 协议与 Requests 库的主要方法</h1><p>requests 库有七个主要方法，分别如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>requests.request()</td>
<td>构造一个请求，支撑以下各方法的基础方法</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取 HTML 网页的主要方法，对应于 HTTP 的 GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取 HTML网页头信息的方法，对应于 HTTP的 HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向 HTML 网页提交 POST 请求的方法，对应 HTTP 的 POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向 HTML 网页提交 PUT 请求的方法，对应于 HTTP 的 PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向 HTML 网页提交局部修改请求请求，对应 HTTP 的 PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向 HTML 网页提交删除请求，对应于 HTTP 的 DELETE</td>
</tr>
</tbody>
</table>
</div>
<p>为了理解以上方法，我们需要首先了解 HTTP 协议。</p>
<h2 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h2><p>HTTP 协议是超文本传输协议，是一种基于“请求与响应“模式的、无状态的应用层协议，其中无状态指的是第一次请求与第二次请求之间没有直接关联。HTTP 协议采用 URL 作为定位网络资源的标识，URL 格式为<br><a href="http://host[:port][path]，其中">http://host[:port][path]，其中</a> host 是一个合法的 Internet 主机域名或 IP 地址，port 为端口号，缺省端口为80，path 是请求资源的路径。HTTP协议对资源的操作方法如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>GET</td>
<td>请求获取 URL 位置的资源</td>
</tr>
<tr>
<td>HEAD</td>
<td>请求获取 URL 位置的资源的响应消息报告，即获得该资源的头部信息</td>
</tr>
<tr>
<td>POST</td>
<td>请求获取 URL 位置的资源后附加新的数据</td>
</tr>
<tr>
<td>PUT</td>
<td>请求获取 URL 位置存储一个资源，覆盖原 URL 位置的资源</td>
</tr>
<tr>
<td>PATCH</td>
<td>请求局部更新 URL 位置的资源，即改变该处资源的部分内容</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求删除 URL 位置存储的资源</td>
</tr>
</tbody>
</table>
</div>
<p>我们通过HTTP协议可以对资源进行上述操作，即：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR </span><br><span class="line">A[User] -- GET&#x2F;HEAD --&gt; B((cloud))</span><br><span class="line">B --PUT&#x2F;POST&#x2F;PATCH&#x2F;DELETE--&gt;A</span><br></pre></td></tr></table></figure>
<p>事实上，HTTP 协议通过 URL 做定位，通过上述六个方法对资源进行管理，每个操作都是无状态的。HTTP 协议与 Requests 库的方法是一一对应的，下面以 requests.post() 方法为例说明之：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">payload =  &#123;<span class="string">"key1"</span>:<span class="string">"value1"</span>, <span class="string">"key2"</span>:<span class="string">"value2"</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=payload)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">返回内容：</span><br><span class="line">&#123;...</span><br><span class="line"> &quot;form&quot; : &#123;</span><br><span class="line">	&quot;key2&quot;:&quot;value2&quot;,</span><br><span class="line">   &quot;key1&quot;:&quot;value1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这说明用 requests.post() 提交一个键值对时会将数据存储在表单（ form ) 下，若提交的是数据，则会存储在 data 下。</p>
<h1 id="Requests-库主要方法解析"><a href="#Requests-库主要方法解析" class="headerlink" title="Requests 库主要方法解析"></a>Requests 库主要方法解析</h1><h2 id="request-方法"><a href="#request-方法" class="headerlink" title="request 方法"></a>request 方法</h2><p>request 方法的使用规则是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.request(method, url, **kwargs)</span><br></pre></td></tr></table></figure>
<p>其中 method 表示请求方式，具体可填参数有 ‘GET’ , ‘HEAD’, ‘POST’, ‘PUT’, ‘PATCH’, ‘delete’, ‘OPTIONS’，**kwargs 为可选项，有13个参数，下面将进行部分举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># params 参数</span></span><br><span class="line">kv = &#123;<span class="string">"key1"</span>:<span class="string">"value1"</span>, <span class="string">"key2"</span>:<span class="string">"value2"</span>&#125;</span><br><span class="line">r = requests.request(<span class="string">'GET'</span>, <span class="string">'http://python123.io/ws'</span>, params=kv)</span><br><span class="line">print(r.url)</span><br><span class="line"><span class="comment"># http://python123.io/ws?key1=value1&amp;key2=value2</span></span><br></pre></td></tr></table></figure>
<p>其中”？”后面带的参数可供浏览器进行筛选，再将页面返回。除此之外更常用的是 header 参数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hd = &#123;<span class="string">'user=agent'</span>:<span class="string">'Chrome/10'</span>&#125;</span><br><span class="line">r = requests.request(<span class="string">'POST'</span>,<span class="string">'http://python123.io/ws'</span>, headers=hd)</span><br></pre></td></tr></table></figure></p>
<p>由上述代码可见，我们可以通过修改 headers 的 ‘user-agent’ 部分模拟不同的浏览器进行访问。另外我们也常常需要隐藏自己的 IP 地址来防止爬虫的逆追踪，我们会使用 proxies 可选项：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pxs = &#123;<span class="string">'http'</span> : <span class="string">'http://user:pass@10.10.10.1:1234'</span> </span><br><span class="line">	   <span class="string">'https'</span> : <span class="string">'https://10.10.10.1:4321'</span> &#125;</span><br><span class="line">r = requests.request(<span class="string">'GET'</span>, <span class="string">'http://www.baidu.com'</span>, proxies=pxs)</span><br></pre></td></tr></table></figure>
<p>其余的可选项还有 data, json, cookies, auth, files, timeout, allow_redirects, stream, verify, cert。</p>
<h1 id="get-方法"><a href="#get-方法" class="headerlink" title="get 方法"></a>get 方法</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.get(url, params=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>
<p>其中 params 为 url 中的额外参数，是字典或字节流格式，为可选项。**kwargs 有12个控制访问参数，即 request 方法中除了 params 的其他访问参数，就不一一介绍了。</p>
<h1 id="post-方法"><a href="#post-方法" class="headerlink" title="post 方法"></a>post 方法</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.post(url, data=<span class="literal">None</span>, json=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>
<p>其中 url 为拟更新页面的 url 链接， data 为字典、字节序列或文件，是 Requests 的内容， json 为 JSON 格式的数据， 是 Requests 的内容，**kwargs 为除了 data 与 json 外的其他控制访问参数。</p>
<h1 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h1><p>由于其他三个方法使用情况大同小异，直接在下面列出了调用格式：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.put(url, data=<span class="literal">None</span>, **kwargs)</span><br><span class="line">requests.patch(url, data=<span class="literal">None</span>, **kwargs)</span><br><span class="line">requests.delete(url, **kwargs)</span><br></pre></td></tr></table></figure></p>
<p>事实上我们可以发现，除了 request 方法，其他几个方法都不过是显式定义了 kwargs 中的部分参数。这样定义是因为这几个参数的使用频率更高，因此单独定义出来更方便。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
</search>
