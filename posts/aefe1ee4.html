<!DOCTYPE html>


<html lang="zh-CN">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    NLP项目Pipeline |  言念君子
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

</head>

</html>

<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-NLP项目Pipeline"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  NLP项目Pipeline
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/posts/aefe1ee4.html" class="article-date">
  <time datetime="2020-08-03T11:51:01.804Z" itemprop="datePublished">2020-08-03</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">4.1k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">23 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Traditional-NLP-Pipeline"><a href="#Traditional-NLP-Pipeline" class="headerlink" title="Traditional NLP Pipeline"></a>Traditional NLP Pipeline</h1><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>繁杂的数据预处理：分词形式、是否做字符级别的切分词等等</li>
<li>基于word embedding：太大，稀疏，线上部署需要保证内存等</li>
<li>模型能力不够，拟合能力有限，RNN、CNN等很难学习更多东西</li>
<li>训练数据：需要数据集大，人工标注成本大</li>
</ul>
<p>下面我们看看训练流程。</p>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="基于Pytorch"><a href="#基于Pytorch" class="headerlink" title="基于Pytorch"></a>基于Pytorch</h3><h4 id="下载并处理IMDB公开数据集"><a href="#下载并处理IMDB公开数据集" class="headerlink" title="下载并处理IMDB公开数据集"></a>下载并处理IMDB公开数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED) <span class="comment">#为CPU设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed(SEED)<span class="comment">#为GPU设置随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用来定义字段的处理方法（文本字段，标签字段）</span></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)<span class="comment">#torchtext.data.Field : </span></span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br></pre></td></tr></table></figure>
<h4 id="切分训练集、测试集和验证集"><a href="#切分训练集、测试集和验证集" class="headerlink" title="切分训练集、测试集和验证集"></a>切分训练集、测试集和验证集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>
<h4 id="读入glove词向量"><a href="#读入glove词向量" class="headerlink" title="读入glove词向量"></a>读入glove词向量</h4><p>目的是把训练集的Token转化为词向量传入模型中，若使用TEXT的api会自动下载词向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>,</span><br><span class="line">                 unk_init=torch.Tensor.normal_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建训练集字典</span></span><br><span class="line">LABEL.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader制作</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"><span class="comment"># 判断使用的是CPU还是GPU</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代器的形式会加速数据传输速度（异步读取）</span></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br></pre></td></tr></table></figure>
<h4 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [sent_len, batch _size, emb_size]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(TEXT.vocab) <span class="comment">#词个数</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span> <span class="comment">#词嵌入维度</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span> <span class="comment">#输出维度</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] <span class="comment">#pad索引</span></span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br><span class="line"></span><br><span class="line">拿到新的词向量，换掉原有的</span><br><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    model.train() <span class="comment">#model.train()代表了训练模式</span></span><br><span class="line">    <span class="comment">#这步一定要加，是为了区分model训练和测试的模式的。</span></span><br><span class="line">    <span class="comment">#有时候训练时会用到dropout、归一化等方法，但是测试的时候不能用dropout等方法。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator: <span class="comment">#iterator为train_iterator</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#加这步防止梯度叠加</span></span><br><span class="line"></span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#batch.text 就是上面forward函数的参数text</span></span><br><span class="line">        <span class="comment">#压缩维度，不然跟batch.label维度对不上</span></span><br><span class="line"></span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line"></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#梯度下降</span></span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#loss.item()已经本身除以了len(batch.label)</span></span><br><span class="line">        <span class="comment">#所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。</span></span><br><span class="line"></span><br><span class="line">        epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">        <span class="comment">#（acc.item()：一个batch的正确率） *batch数 = 正确数</span></span><br><span class="line">        <span class="comment">#train_iterator所有batch的正确数累加。</span></span><br><span class="line"></span><br><span class="line">        total_len += len(batch.label)</span><br><span class="line">        <span class="comment">#计算train_iterator所有样本的数量，不出意外应该是17500</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br><span class="line">    <span class="comment">#epoch_loss / total_len ：train_iterator所有batch的损失</span></span><br><span class="line">    <span class="comment">#epoch_acc / total_len ：train_iterator所有batch的正确率</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line"></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br><span class="line"></span><br><span class="line"><span class="comment">#import time</span></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>) <span class="comment">#无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#start_time = time.time()</span></span><br><span class="line"></span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#end_time = time.time()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss: <span class="comment">#只要模型效果变好，就存模型</span></span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(f'Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;s')</span></span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="基于Tensorflow2-0"><a href="#基于Tensorflow2-0" class="headerlink" title="基于Tensorflow2.0"></a>基于Tensorflow2.0</h3><h4 id="下载IMDB公开数据集，并处理"><a href="#下载IMDB公开数据集，并处理" class="headerlink" title="下载IMDB公开数据集，并处理"></a>下载IMDB公开数据集，并处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imdb_dir = <span class="string">"./aclImdb"</span></span><br><span class="line">train_dir = os.path.join(imdb_dir, <span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">"neg"</span>, <span class="string">"pos"</span>]:</span><br><span class="line">    dir_name = os.path.join(train_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(dir_name):</span><br><span class="line">        <span class="keyword">if</span> fname [<span class="number">-4</span>:] == <span class="string">".txt"</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">"neg"</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="制作训练集和验证集"><a href="#制作训练集和验证集" class="headerlink" title="制作训练集和验证集"></a>制作训练集和验证集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing. sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">maxlen = <span class="number">100</span> <span class="comment"># cuts off review after 100 words</span></span><br><span class="line">training_samples = <span class="number">200</span> <span class="comment"># Trains on 200 samples</span></span><br><span class="line">validation_samples = <span class="number">10000</span> <span class="comment"># Validates o 10000 samples</span></span><br><span class="line">max_words = <span class="number">10000</span> <span class="comment"># Considers only the top 10000 words in the dataset</span></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">word_index = tokenizer.word_index                   <span class="comment"># Length: 88582</span></span><br><span class="line">print(<span class="string">"Found %s unique tokens."</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">data = pad_sequences(sequences, maxlen=maxlen)</span><br><span class="line"></span><br><span class="line">labels = np.asarray(labels)</span><br><span class="line">print(<span class="string">"Shape of data tensor:"</span>, data.shape)</span><br><span class="line">print(<span class="string">"Shape of label tensor:"</span>, labels.shape)</span><br><span class="line"></span><br><span class="line">indices = np.arange(data.shape[<span class="number">0</span>]) <span class="comment"># Splits data into training and validation set, but shuffles is, since samples are ordered:</span></span><br><span class="line"><span class="comment"># all negatives first, then all positive</span></span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">data = data[indices]</span><br><span class="line">labels = labels[indices]</span><br><span class="line"></span><br><span class="line">x_train = data[:training_samples] <span class="comment"># (200, 100)</span></span><br><span class="line">y_train = labels[:training_samples] <span class="comment"># shape (200,)</span></span><br><span class="line">x_val = data[training_samples:training_samples+validation_samples] <span class="comment"># shape (10000, 100)</span></span><br><span class="line">y_val = labels[training_samples:training_samples+validation_samples] <span class="comment"># shape (10000,)</span></span><br></pre></td></tr></table></figure>
<h4 id="下载glove词向量，并读入"><a href="#下载glove词向量，并读入" class="headerlink" title="下载glove词向量，并读入"></a>下载glove词向量，并读入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">glove_dir = <span class="string">"./"</span></span><br><span class="line"></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line"></span><br><span class="line">f = open(os.path.join(glove_dir, <span class="string">"glove.6B.50d.txt"</span>), encoding=<span class="string">'utf-8'</span>) <span class="comment">#added , encoding='utf-8'</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">"float32"</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"found %s word vectors."</span> % len (embeddings_index))</span><br></pre></td></tr></table></figure>
<h4 id="读入词向量"><a href="#读入词向量" class="headerlink" title="读入词向量"></a>读入词向量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">embedding_dim = <span class="number">50</span> <span class="comment"># GloVe contains 50-dimensional embedding vectors for 400.000 words</span></span><br><span class="line"></span><br><span class="line">embedding_matrix = np.zeros((max_words, embedding_dim)) <span class="comment"># embedding_matrix.shape (10000, 50)</span></span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    <span class="keyword">if</span> i &lt; max_words:</span><br><span class="line">        embedding_vector = embeddings_index.get(word) <span class="comment"># embedding_vector.shape (100,)</span></span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector <span class="comment"># Words not found in the mebedding index will all be zeros</span></span><br></pre></td></tr></table></figure>
<h4 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential() </span><br><span class="line">model.add(keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen))</span><br><span class="line">model.add(keras.layers.Bidirectional(keras.layers.LSTM(<span class="number">64</span>, return_sequences = <span class="literal">True</span>)))</span><br><span class="line">model.add(keras.layers.GlobalMaxPool1D())</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">16</span>, activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation = <span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(loss = <span class="string">'binary_crossentropy'</span>, optimizer = <span class="string">'adam'</span>, metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h4 id="加载预训词向量"><a href="#加载预训词向量" class="headerlink" title="加载预训词向量"></a>加载预训词向量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.layers[<span class="number">0</span>].set_weights([embedding_matrix])</span><br><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = <span class="string">"rmsprop"</span>, </span><br><span class="line">              loss = <span class="string">"binary_crossentropy"</span>, <span class="comment"># in a multiclass problem categorical_crossentropy would be used</span></span><br><span class="line">              metrics = [<span class="string">"acc"</span>]) </span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                   epochs = <span class="number">10</span>,</span><br><span class="line">                   batch_size = <span class="number">32</span>,</span><br><span class="line">                   validation_data = (x_val, y_val))</span><br><span class="line">model.save_weights(<span class="string">"pre_trained_glove_model.h5"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="New-NLP-Pipeline"><a href="#New-NLP-Pipeline" class="headerlink" title="New NLP Pipeline"></a>New NLP Pipeline</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>先进的Tokenizer：WordPiece，将词拆成一个一个字，减少了vocab集的大小</li>
<li>模型结构：强大的特征提取器Transformer</li>
<li>预训练任务：解决了数据问题，不需要太多的数据清洗</li>
</ul>
<p>注：生成任务不适合使用Bert</p>
<h2 id="训练流程-1"><a href="#训练流程-1" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="基于Pytorch-1"><a href="#基于Pytorch-1" class="headerlink" title="基于Pytorch"></a>基于Pytorch</h3><h4 id="导入相关的包"><a href="#导入相关的包" class="headerlink" title="导入相关的包"></a>导入相关的包</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 使用transformers包</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<h4 id="种子和参数设定"><a href="#种子和参数设定" class="headerlink" title="种子和参数设定"></a>种子和参数设定</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数</span></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line">TRAIN = <span class="literal">False</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">TEXT = <span class="string">"I like you!"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定模型用种子，便于重复试验</span></span><br><span class="line">random.seed(SEED)</span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="处理Token"><a href="#处理Token" class="headerlink" title="处理Token"></a>处理Token</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用transformers中Tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">init_token_id = tokenizer.cls_token_id</span><br><span class="line">eos_token_id  = tokenizer.sep_token_id</span><br><span class="line">pad_token_id  = tokenizer.pad_token_id</span><br><span class="line">unk_token_id  = tokenizer.unk_token_id</span><br><span class="line"></span><br><span class="line">max_input_len = tokenizer.max_model_input_sizes[<span class="string">'bert-base-uncased'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子长度切割成510长，为了加上开头和最后一个token</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_crop</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    tokens = tokenizer.tokenize(sentence)</span><br><span class="line">    tokens = tokens[:max_input_len - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>
<h4 id="加载PyTorch提供的IMDB数据"><a href="#加载PyTorch提供的IMDB数据" class="headerlink" title="加载PyTorch提供的IMDB数据"></a>加载PyTorch提供的IMDB数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    text = data.Field(</span><br><span class="line">        batch_first=<span class="literal">True</span>,</span><br><span class="line">        use_vocab=<span class="literal">False</span>,</span><br><span class="line">        tokenize=tokenize_and_crop,</span><br><span class="line">        preprocessing=tokenizer.convert_tokens_to_ids,</span><br><span class="line">        init_token=init_token_id,</span><br><span class="line">        pad_token=pad_token_id,</span><br><span class="line">        unk_token=unk_token_id</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    label = data.LabelField(dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    train_data, test_data  = datasets.IMDB.splits(text, label)</span><br><span class="line">    train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"training examples count: <span class="subst">&#123;len(train_data)&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"test examples count: <span class="subst">&#123;len(test_data)&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"validation examples count: <span class="subst">&#123;len(valid_data)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    label.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = data.BucketIterator.splits(</span><br><span class="line">        (train_data, valid_data, test_data),</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        device=device</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, valid_iter, test_iter</span><br></pre></td></tr></table></figure>
<h4 id="引入Bert预处理模型"><a href="#引入Bert预处理模型" class="headerlink" title="引入Bert预处理模型"></a>引入Bert预处理模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看是否有GPU</span></span><br><span class="line">device = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过transformers包，建立BERT模型</span></span><br><span class="line">bert_model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="搭建模型-1"><a href="#搭建模型-1" class="headerlink" title="搭建模型"></a>搭建模型</h4><p>在这里开始做finetune处理了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处用BERT做为基础模型完成情感分析任务</span></span><br><span class="line"><span class="comment"># 在BERT之上加两层GRU</span></span><br><span class="line"><span class="comment"># 最后接一层线性层用于完成分类任务</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        bert,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">        bidirectional,</span></span></span><br><span class="line"><span class="function"><span class="params">        dropout</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">      </span><br><span class="line">        super(SentimentModel, self).__init__()</span><br><span class="line">    </span><br><span class="line">        self.bert = bert</span><br><span class="line">        embedding_dim = bert.config.to_dict()[<span class="string">'hidden_size'</span>]</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embedding_dim,</span><br><span class="line">            hidden_dim,</span><br><span class="line">            num_layers=n_layers,</span><br><span class="line">            bidirectional=bidirectional,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            dropout=<span class="number">0</span> <span class="keyword">if</span> n_layers &lt; <span class="number">2</span> <span class="keyword">else</span> dropout</span><br><span class="line">            )</span><br><span class="line">        self.out = nn.Linear(hidden_dim * <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> hidden_dim, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            embedded = self.bert(text)[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">        _, hidden = self.rnn(embedded)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> self.rnn.bidirectional:</span><br><span class="line">            hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = self.dropout(hidden[<span class="number">-1</span>,:,:])</span><br><span class="line">    </span><br><span class="line">        output = self.out(hidden)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">model = SentimentModel(</span><br><span class="line">  bert_model,</span><br><span class="line">  HIDDEN_DIM,</span><br><span class="line">  OUTPUT_DIM,!pip list</span><br><span class="line">  N_LAYERS,</span><br><span class="line">  BIDIRECTIONAL,</span><br><span class="line">  DROPOUT</span><br><span class="line">)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<h4 id="一个epoch需要多长时间"><a href="#一个epoch需要多长时间" class="headerlink" title="一个epoch需要多长时间"></a>一个epoch需要多长时间</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>
<h4 id="二分类问题的accuracy"><a href="#二分类问题的accuracy" class="headerlink" title="二分类问题的accuracy"></a>二分类问题的accuracy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float()</span><br><span class="line">    acc = correct.sum() / len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<h4 id="一个训练步"><a href="#一个训练步" class="headerlink" title="一个训练步"></a>一个训练步</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>
<h4 id="验证模型"><a href="#验证模型" class="headerlink" title="验证模型"></a>验证模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>
<h4 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(model, tokenizer, sentence)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    tokens = tokenizer.tokenize(sentence)</span><br><span class="line">    tokens = tokens[:max_input_len - <span class="number">2</span>]</span><br><span class="line">    indexed = [init_token_id] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_id]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br><span class="line"></span><br><span class="line">train_iter, valid_iter, test_iter = load_data()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss().to(device)</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">best_val_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="comment"># 训练一个epoch</span></span><br><span class="line">    train_loss, train_acc = train(model, train_iter, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'model.pt'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">if</span> TRAIN:</span><br><span class="line">        <span class="comment"># 读取数据</span></span><br><span class="line">        train_iter, valid_iter, test_iter = load_data()</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters())</span><br><span class="line">        criterion = nn.BCEWithLogitsLoss().to(device)</span><br><span class="line">        model = model.to(device)</span><br><span class="line"></span><br><span class="line">        best_val_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">            start_time = time.time()</span><br><span class="line">            <span class="comment"># 训练一个epoch</span></span><br><span class="line">            train_loss, train_acc = train(model, train_iter, optimizer, criterion)</span><br><span class="line">            valid_loss, valid_acc = evaluate(model, valid_iter, criterion)</span><br><span class="line"></span><br><span class="line">            end_time = time.time()</span><br><span class="line"></span><br><span class="line">            epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">                best_valid_loss = valid_loss</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">'model.pt'</span>)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">            print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">            print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">        <span class="comment"># 测试</span></span><br><span class="line">        model.load_state_dict(torch.load(<span class="string">'model.pt'</span>))</span><br><span class="line">        test_loss, test_acc = evaluate(model, test_iter, criterion)</span><br><span class="line">        print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 推理结果</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.load_state_dict(torch.load(<span class="string">'model.pt'</span>, map_location=device))</span><br><span class="line">        sentiment = predict_sentiment(model, tokenizer, TEXT)</span><br><span class="line">        print(sentiment)</span><br></pre></td></tr></table></figure>
<h3 id="基于Tensorflow2-0-1"><a href="#基于Tensorflow2-0-1" class="headerlink" title="基于Tensorflow2.0"></a>基于Tensorflow2.0</h3><p>This is a modification of <a href="https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" target="_blank" rel="noopener">https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb</a> using the Tensorflow 2.0 Keras implementation of BERT from <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">kpe/bert-for-tf2</a> with the original <a href="https://github.com/google-research/bert" target="_blank" rel="noopener">google-research/bert</a> weights.</p>
<p><strong>Predicting Movie Review Sentiment with <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">kpe/bert-for-tf2</a></strong></p>
<h4 id="First-install-some-prerequisites"><a href="#First-install-some-prerequisites" class="headerlink" title="First install some prerequisites:"></a>First install some prerequisites:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>In addition to the standard libraries we imported above, we’ll need to install the <a href="https://github.com/kpe/bert-for-tf2" target="_blank" rel="noopener">bert-for-tf2</a> python package, and do the imports required for loading the pre-trained weights and tokenizing the input text. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bert</span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> BertModelLayer</span><br><span class="line"><span class="keyword">from</span> bert.loader <span class="keyword">import</span> StockBertConfig, map_stock_config_to_params, load_stock_weights</span><br><span class="line"><span class="keyword">from</span> bert.tokenization.bert_tokenization <span class="keyword">import</span> FullTokenizer</span><br></pre></td></tr></table></figure>
<h4 id="Download-the-dataset"><a href="#Download-the-dataset" class="headerlink" title="Download the dataset"></a>Download the dataset</h4><p>First, let’s download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from <a href="https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub" target="_blank" rel="noopener">this Tensorflow tutorial</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all files from a directory in a DataFrame.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_directory_data</span><span class="params">(directory)</span>:</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    data[<span class="string">"sentence"</span>] = []</span><br><span class="line">    data[<span class="string">"sentiment"</span>] = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> tqdm(os.listdir(directory), desc=os.path.basename(directory)):</span><br><span class="line">        <span class="keyword">with</span> tf.io.gfile.GFile(os.path.join(directory, file_path), <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data[<span class="string">"sentence"</span>].append(f.read())</span><br><span class="line">            data[<span class="string">"sentiment"</span>].append(re.match(<span class="string">"\d+_(\d+)\.txt"</span>, file_path).group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame.from_dict(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge positive and negative examples, add a polarity column and shuffle.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(directory)</span>:</span></span><br><span class="line">    pos_df = load_directory_data(os.path.join(directory, <span class="string">"pos"</span>))</span><br><span class="line">    neg_df = load_directory_data(os.path.join(directory, <span class="string">"neg"</span>))</span><br><span class="line">    pos_df[<span class="string">"polarity"</span>] = <span class="number">1</span></span><br><span class="line">    neg_df[<span class="string">"polarity"</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> pd.concat([pos_df, neg_df]).sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download and process the dataset files.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_and_load_datasets</span><span class="params">(force_download=False)</span>:</span></span><br><span class="line"><span class="comment">#     dataset = tf.keras.utils.get_file(</span></span><br><span class="line"><span class="comment">#       fname="aclImdb.tar.gz", </span></span><br><span class="line"><span class="comment">#       origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", </span></span><br><span class="line"><span class="comment">#       extract=True)</span></span><br><span class="line">    </span><br><span class="line">    dataset = <span class="string">"./aclImdb"</span></span><br><span class="line">    train_df = load_dataset(os.path.join(os.path.dirname(dataset), </span><br><span class="line">                                       <span class="string">"aclImdb"</span>, <span class="string">"train"</span>))</span><br><span class="line">    test_df = load_dataset(os.path.join(os.path.dirname(dataset), </span><br><span class="line">                                      <span class="string">"aclImdb"</span>, <span class="string">"test"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br></pre></td></tr></table></figure>
<h4 id="Prepare-Encode"><a href="#Prepare-Encode" class="headerlink" title="Prepare/Encode"></a>Prepare/Encode</h4><p>Let’s use the <code>MovieReviewData</code> class below, to prepare/encode the data for feeding into our BERT model, by:</p>
<ul>
<li>tokenizing the text</li>
<li>trim or pad it to a <code>max_seq_len</code> length</li>
<li>append the special tokens <code>[CLS]</code> and <code>[SEP]</code></li>
<li>convert the string tokens to numerical <code>ID</code>s using the original model’s token encoding from <code>vocab.txt</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bert</span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> BertModelLayer</span><br><span class="line"><span class="keyword">from</span> bert.loader <span class="keyword">import</span> StockBertConfig, map_stock_config_to_params, load_stock_weights</span><br><span class="line"><span class="comment"># from bert.tokenization import FullTokenizer</span></span><br><span class="line"><span class="keyword">from</span> bert <span class="keyword">import</span> bert_tokenization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MovieReviewData</span>:</span></span><br><span class="line">    DATA_COLUMN = <span class="string">"sentence"</span></span><br><span class="line">    LABEL_COLUMN = <span class="string">"polarity"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokenizer: bert_tokenization.FullTokenizer, sample_size=None, max_seq_len=<span class="number">1024</span>)</span>:</span></span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.sample_size = sample_size</span><br><span class="line">        self.max_seq_len = <span class="number">0</span></span><br><span class="line">        train, test = download_and_load_datasets()</span><br><span class="line">        </span><br><span class="line">        train, test = map(<span class="keyword">lambda</span> df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), </span><br><span class="line">                          [train, test])</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">if</span> sample_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> sample_size % <span class="number">128</span> == <span class="number">0</span></span><br><span class="line">            train, test = train.head(sample_size), test.head(sample_size)</span><br><span class="line">            <span class="comment"># train, test = map(lambda df: df.sample(sample_size), [train, test])</span></span><br><span class="line">        </span><br><span class="line">        ((self.train_x, self.train_y),</span><br><span class="line">         (self.test_x, self.test_y)) = map(self._prepare, [train, test])</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"max seq_len"</span>, self.max_seq_len)</span><br><span class="line">        self.max_seq_len = min(self.max_seq_len, max_seq_len)</span><br><span class="line">        ((self.train_x, self.train_x_token_types),</span><br><span class="line">         (self.test_x, self.test_x_token_types)) = map(self._pad, </span><br><span class="line">                                                       [self.train_x, self.test_x])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        x, y = [], []</span><br><span class="line">        <span class="keyword">with</span> tqdm(total=df.shape[<span class="number">0</span>], unit_scale=<span class="literal">True</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">            <span class="keyword">for</span> ndx, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]</span><br><span class="line">                tokens = self.tokenizer.tokenize(text)</span><br><span class="line">                tokens = [<span class="string">"[CLS]"</span>] + tokens + [<span class="string">"[SEP]"</span>]</span><br><span class="line">                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">                self.max_seq_len = max(self.max_seq_len, len(token_ids))</span><br><span class="line">                x.append(token_ids)</span><br><span class="line">                y.append(int(label))</span><br><span class="line">                pbar.update()</span><br><span class="line">        <span class="keyword">return</span> np.array(x), np.array(y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pad</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        x, t = [], []</span><br><span class="line">        token_type_ids = [<span class="number">0</span>] * self.max_seq_len</span><br><span class="line">        <span class="keyword">for</span> input_ids <span class="keyword">in</span> ids:</span><br><span class="line">            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - <span class="number">2</span>)]</span><br><span class="line">            input_ids = input_ids + [<span class="number">0</span>] * (self.max_seq_len - len(input_ids))</span><br><span class="line">            x.append(np.array(input_ids))</span><br><span class="line">            t.append(token_type_ids)</span><br><span class="line">        <span class="keyword">return</span> np.array(x), np.array(t)</span><br></pre></td></tr></table></figure>
<h4 id="A-tweak"><a href="#A-tweak" class="headerlink" title="A tweak"></a>A tweak</h4><p>Because of a <code>tf.train.load_checkpoint</code> limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">bert_ckpt_dir=<span class="string">"./uncased_L-12_H-768_A-12/"</span></span><br><span class="line">bert_ckpt_file = bert_ckpt_dir + <span class="string">"bert_model.ckpt"</span></span><br><span class="line">bert_config_file = bert_ckpt_dir + <span class="string">"bert_config.json"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%time</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert_model_dir="2018_10_18"</span></span><br><span class="line"><span class="comment"># bert_model_name="uncased_L-12_H-768_A-12"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># !mkdir -p .model .model/$bert_model_name</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for fname in ["bert_config.json", "vocab.txt", "bert_model.ckpt.meta", "bert_model.ckpt.index", "bert_model.ckpt.data-00000-of-00001"]:</span></span><br><span class="line"><span class="comment">#   cmd = f"gsutil cp gs://bert_models/&#123;bert_model_dir&#125;/&#123;bert_model_name&#125;/&#123;fname&#125; .model/&#123;bert_model_name&#125;"</span></span><br><span class="line"><span class="comment">#   !$cmd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># !ls -la .model .model/$bert_model_name</span></span><br><span class="line"></span><br><span class="line">bert_ckpt_dir    = os.path.join(<span class="string">".model/"</span>,bert_model_name)</span><br><span class="line">bert_ckpt_file   = os.path.join(bert_ckpt_dir, <span class="string">"bert_model.ckpt"</span>)</span><br><span class="line">bert_config_file = os.path.join(bert_ckpt_dir, <span class="string">"bert_config.json"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h4><p>Now let’s fetch and prepare the data by taking the first <code>max_seq_len</code> tokenens after tokenizing with the BERT tokenizer, und use <code>sample_size</code> examples for both training and testing.</p>
<p>To keep training fast, we’ll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use <code>max_seq_len=512</code>, but on a GPU this would be too slow, and you will have to use a very small <code>batch_size</code>s to fit the model into the GPU memory).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, <span class="string">"vocab.txt"</span>))</span><br><span class="line">data = MovieReviewData(tokenizer, </span><br><span class="line">                       sample_size=<span class="number">10</span>*<span class="number">128</span>*<span class="number">2</span>,<span class="comment">#5000, </span></span><br><span class="line">                       max_seq_len=<span class="number">128</span>)</span><br><span class="line">print(<span class="string">"            train_x"</span>, data.train_x.shape)</span><br><span class="line">print(<span class="string">"train_x_token_types"</span>, data.train_x_token_types.shape)</span><br><span class="line">print(<span class="string">"            train_y"</span>, data.train_y.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"             test_x"</span>, data.test_x.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"        max_seq_len"</span>, data.max_seq_len)</span><br></pre></td></tr></table></figure>
<h4 id="Adapter-BERT"><a href="#Adapter-BERT" class="headerlink" title="Adapter BERT"></a>Adapter BERT</h4><p>If we decide to use <a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener">adapter-BERT</a> we need some helpers for freezing the original BERT layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_layers</span><span class="params">(root_layer)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(root_layer, keras.layers.Layer):</span><br><span class="line">        <span class="keyword">yield</span> root_layer</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> root_layer._layers:</span><br><span class="line">        <span class="keyword">for</span> sub_layer <span class="keyword">in</span> flatten_layers(layer):</span><br><span class="line">            <span class="keyword">yield</span> sub_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">freeze_bert_layers</span><span class="params">(l_bert)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> flatten_layers(l_bert):</span><br><span class="line">        <span class="keyword">if</span> layer.name <span class="keyword">in</span> [<span class="string">"LayerNorm"</span>, <span class="string">"adapter-down"</span>, <span class="string">"adapter-up"</span>]:</span><br><span class="line">            layer.trainable = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> len(layer._layers) == <span class="number">0</span>:</span><br><span class="line">            layer.trainable = <span class="literal">False</span></span><br><span class="line">        l_bert.embeddings_layer.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_learning_rate_scheduler</span><span class="params">(max_learn_rate=<span class="number">5e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   end_learn_rate=<span class="number">1e-7</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   warmup_epoch_count=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   total_epoch_count=<span class="number">90</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lr_scheduler</span><span class="params">(epoch)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> epoch &lt; warmup_epoch_count:</span><br><span class="line">            res = (max_learn_rate/warmup_epoch_count) * (epoch + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+<span class="number">1</span>)/(total_epoch_count-warmup_epoch_count+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> float(res)</span><br><span class="line">    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> learning_rate_scheduler</span><br></pre></td></tr></table></figure>
<h4 id="Creating-a-model"><a href="#Creating-a-model" class="headerlink" title="Creating a model"></a>Creating a model</h4><p>Now let’s create a classification model using <a href="https//arxiv.org/abs/1902.00751">adapter-BERT</a>, which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. <code>adapter_size</code> bellow) in every BERT layer.</p>
<p><strong>N.B.</strong> The commented out code below show how to feed a <code>token_type_ids</code>/<code>segment_ids</code> sequence (which is not needed in our case).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(max_seq_len, adapter_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Creates a classification model."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#adapter_size = 64  # see - arXiv:1902.00751</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create the bert layer</span></span><br><span class="line">    <span class="keyword">with</span> tf.io.gfile.GFile(bert_config_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">        bc = StockBertConfig.from_json_string(reader.read())</span><br><span class="line">        bert_params = map_stock_config_to_params(bc)</span><br><span class="line">        bert_params.adapter_size = adapter_size</span><br><span class="line">        bert = BertModelLayer.from_params(bert_params, name=<span class="string">"bert"</span>)</span><br><span class="line"></span><br><span class="line">    input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype=<span class="string">'int32'</span>, name=<span class="string">"input_ids"</span>)</span><br><span class="line">    <span class="comment"># token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name="token_type_ids")</span></span><br><span class="line">    <span class="comment"># output         = bert([input_ids, token_type_ids])</span></span><br><span class="line">    output         = bert(input_ids)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"bert shape"</span>, output.shape)</span><br><span class="line">    cls_out = keras.layers.Lambda(<span class="keyword">lambda</span> seq: seq[:, <span class="number">0</span>, :])(output)</span><br><span class="line">    cls_out = keras.layers.Dropout(<span class="number">0.5</span>)(cls_out)</span><br><span class="line">    logits = keras.layers.Dense(units=<span class="number">768</span>, activation=<span class="string">"tanh"</span>)(cls_out)</span><br><span class="line">    logits = keras.layers.Dropout(<span class="number">0.5</span>)(logits)</span><br><span class="line">    logits = keras.layers.Dense(units=<span class="number">2</span>, activation=<span class="string">"softmax"</span>)(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)</span></span><br><span class="line">    <span class="comment"># model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])</span></span><br><span class="line">    model = keras.Model(inputs=input_ids, outputs=logits)</span><br><span class="line">    model.build(input_shape=(<span class="literal">None</span>, max_seq_len))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load the pre-trained model weights</span></span><br><span class="line">    load_stock_weights(bert, bert_ckpt_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freeze weights if adapter-BERT is used</span></span><br><span class="line">    <span class="keyword">if</span> adapter_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        freeze_bert_layers(bert)</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=keras.optimizers.Adam(),</span><br><span class="line">                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[keras.metrics.SparseCategoricalAccuracy(name=<span class="string">"acc"</span>)])</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">adapter_size = <span class="literal">None</span> <span class="comment"># use None to fine-tune all of BERT</span></span><br><span class="line">model = create_model(data.max_seq_len, adapter_size=adapter_size)</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line"></span><br><span class="line">log_dir = <span class="string">".log/movie_reviews/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%s"</span>)</span><br><span class="line">tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">total_epoch_count = <span class="number">50</span></span><br><span class="line"><span class="comment"># model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,</span></span><br><span class="line">model.fit(x=data.train_x, y=data.train_y,</span><br><span class="line">          validation_split=<span class="number">0.1</span>,</span><br><span class="line">          batch_size=<span class="number">48</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          epochs=total_epoch_count,</span><br><span class="line">          callbacks=[create_learning_rate_scheduler(max_learn_rate=<span class="number">1e-5</span>,</span><br><span class="line">                                                    end_learn_rate=<span class="number">1e-7</span>,</span><br><span class="line">                                                    warmup_epoch_count=<span class="number">20</span>,</span><br><span class="line">                                                    total_epoch_count=total_epoch_count),</span><br><span class="line">                     keras.callbacks.EarlyStopping(patience=<span class="number">20</span>, restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                     tensorboard_callback])</span><br><span class="line"></span><br><span class="line">model.save_weights(<span class="string">'./movie_reviews.h5'</span>, overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line"></span><br><span class="line">_, train_acc = model.evaluate(data.train_x, data.train_y)</span><br><span class="line">_, test_acc = model.evaluate(data.test_x, data.test_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train acc"</span>, train_acc)</span><br><span class="line">print(<span class="string">"test acc"</span>, test_acc)</span><br></pre></td></tr></table></figure>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>To evaluate the trained model, let’s load the saved weights in a new model instance, and evaluate.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%time </span><br><span class="line"></span><br><span class="line">model = create_model(data.max_seq_len, adapter_size=<span class="literal">None</span>)</span><br><span class="line">model.load_weights(<span class="string">"movie_reviews.h5"</span>)</span><br><span class="line"></span><br><span class="line">_, train_acc = model.evaluate(data.train_x, data.train_y)</span><br><span class="line">_, test_acc = model.evaluate(data.test_x, data.test_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train acc"</span>, train_acc)</span><br><span class="line">print(<span class="string">" test acc"</span>, test_acc)</span><br></pre></td></tr></table></figure>
<h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special <code>[CLS]</code> and <code>[SEP]</code> token at begin and end of the token sequence, and pad to match the model input shape.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pred_sentences = [</span><br><span class="line">  <span class="string">"That movie was absolutely awful"</span>,</span><br><span class="line">  <span class="string">"The acting was a bit lacking"</span>,</span><br><span class="line">  <span class="string">"The film was creative and surprising"</span>,</span><br><span class="line">  <span class="string">"Absolutely fantastic!"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, <span class="string">"vocab.txt"</span>))</span><br><span class="line">pred_tokens    = map(tokenizer.tokenize, pred_sentences)</span><br><span class="line">pred_tokens    = map(<span class="keyword">lambda</span> tok: [<span class="string">"[CLS]"</span>] + tok + [<span class="string">"[SEP]"</span>], pred_tokens)</span><br><span class="line">pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))</span><br><span class="line"></span><br><span class="line">pred_token_ids = map(<span class="keyword">lambda</span> tids: tids +[<span class="number">0</span>]*(data.max_seq_len-len(tids)),pred_token_ids)</span><br><span class="line">pred_token_ids = np.array(list(pred_token_ids))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'pred_token_ids'</span>, pred_token_ids.shape)</span><br><span class="line"></span><br><span class="line">res = model.predict(pred_token_ids).argmax(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text, sentiment <span class="keyword">in</span> zip(pred_sentences, res):</span><br><span class="line">  print(<span class="string">" text:"</span>, text)</span><br><span class="line">  print(<span class="string">"  res:"</span>, [<span class="string">"negative"</span>,<span class="string">"positive"</span>][sentiment])</span><br></pre></td></tr></table></figure>
<h3 id="一些Trick"><a href="#一些Trick" class="headerlink" title="一些Trick"></a>一些Trick</h3><ul>
<li>Multi-task学习</li>
<li>对抗训练</li>
<li>Domain data再训练</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://chenkai66.github.io/posts/aefe1ee4.html" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/posts/f4ba5069.html" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Opencv计算机视觉基础：（一）图像基本操作
          
        </div>
      </a>
    
    
      <a href="/posts/2b071c91.html" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">LBP特征描述算子-人脸检测</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "TQy5bHTePagP10u5BBsesx61-gzGzoHsz",
    app_key: "O6UyJYxBFgMKQMjktBh4KGad",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020
        <i class="ri-heart-fill heart_icon"></i> chenk
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 Myself 强力驱动
        
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="言念君子"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->


<script src="/js/clickBoom2.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


<script src="/js/dz.js"></script>



    
  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"live2d-widget-model-haruto"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>